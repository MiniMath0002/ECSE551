{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "# Group 13\n",
    "Mathieu Mailhot - Isabel Lougheed - Frank-Lucas Pantazis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- CKD: 28 numerical features, 1 target binary classification variable (\"Normal\" / \"battery\")\\n- Battery: 32 real-valued features, 2 classes (\"Normal\" / \"Defective\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "- CKD: 28 numerical features, 1 target binary classification variable (\"Normal\" / \"battery\")\n",
    "- Battery: 32 real-valued features, 2 classes (\"Normal\" / \"Defective\")\n",
    "\"\"\"\n",
    "\n",
    "# load data sets\n",
    "\n",
    "# Calculate cross entropy or/ Information Gain for all the data without the threshold\n",
    "\n",
    "# statistical analysis on the datasets\n",
    "\n",
    "# - normalize\n",
    "\n",
    "# models: all features, selective features based on statistical analysis (dropping features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\issyl\\AppData\\Local\\Temp\\ipykernel_30448\\3014589073.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_CKD[\"label\"] = df_CKD[\"label\"].replace({\"CKD\": 1, \"Normal\": 0})\n",
      "C:\\Users\\issyl\\AppData\\Local\\Temp\\ipykernel_30448\\3014589073.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_battery[\"label\"] = df_battery[\"label\"].replace({\"Defective\": 1, \"Normal\": 0})\n"
     ]
    }
   ],
   "source": [
    "# load data sets\n",
    "df_CKD = pd.read_csv(\"CKD.csv\")\n",
    "df_battery = pd.read_csv(\"Battery_Dataset.csv\")\n",
    "\n",
    "# Convert \"CKD\" to 1 and \"Normal\" to 0\n",
    "df_CKD[\"label\"] = df_CKD[\"label\"].replace({\"CKD\": 1, \"Normal\": 0})\n",
    "# Convert \"Defective\" to 1 and \"Normal\" to 0\n",
    "df_battery[\"label\"] = df_battery[\"label\"].replace({\"Defective\": 1, \"Normal\": 0})\n",
    "\n",
    "# Convert to a numpy array\n",
    "CKD_data = df_CKD.to_numpy()\n",
    "battery_data = df_battery.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Analysis Block\n",
    "\n",
    "# Class for the analysis\n",
    "class Stat_analysis:\n",
    "    def __init__(self, data, name, save_folder):\n",
    "        self.data = data\n",
    "        self.name = name\n",
    "        self.save_folder = save_folder\n",
    "        self.feature_distribution()\n",
    "        self.class_distrubution()\n",
    "\n",
    "    # Function to create a distribution for each feature\n",
    "    def feature_distribution(self):\n",
    "        for i in range(self.data.shape[1] - 2): # remove 1 and last column as we do not need them for the distribution of the features\n",
    "            feature_num = i + 1\n",
    "\n",
    "            plt.hist(self.data[:,feature_num], bins=20, edgecolor=\"black\")\n",
    "            plt.xlabel(\"Value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.title(f\"{self.name} Distribution of Feature {feature_num}\")\n",
    "\n",
    "            filename = os.path.join(self.save_folder, f\"{self.name}_feature{feature_num}_distribution.png\")\n",
    "            plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "    \n",
    "    # Function to create a distribution for the class\n",
    "    def class_distrubution(self):\n",
    "        my_bins = [-0.5, 0.5, 1.5]\n",
    "        class_0 = \"Normal\"\n",
    "        if self.name == \"CKD\":\n",
    "            class_1 = \"CKD\"\n",
    "        else:\n",
    "            class_1 = \"Defective\"\n",
    "\n",
    "        plt.hist(self.data[:,self.data.shape[1] - 1], bins=my_bins, edgecolor=\"black\", align=\"mid\", rwidth=0.6)\n",
    "        plt.xticks([0, 1], [class_0, class_1])\n",
    "        plt.xlabel(\"Category\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(f\"{self.name} Distribution of Class\")\n",
    "\n",
    "        filename = os.path.join(self.save_folder, f\"{self.name}_class_distribution.png\")\n",
    "        plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "# Perform the stastical analysis\n",
    "CKD_stat = Stat_analysis(CKD_data, \"CKD\", \"CKD_distribution\")\n",
    "battery_stat = Stat_analysis(battery_data, \"Battery\", \"Battery_distribution\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,  dataframe, data_array, description:str = None):\n",
    "        self.max_iteration = 10000\n",
    "        self.tolerance = 10**-9\n",
    "        self.step_size = 0.001\n",
    "\n",
    "        self.dataframe = dataframe\n",
    "        self.data_array = data_array\n",
    "        self.description = description\n",
    "        \n",
    "        self.accuracy_arr = 0\n",
    "        self.avg_accuracy = 0\n",
    "        \n",
    "        self.weigth_arr = 0\n",
    "        self.avg_weigth = 0\n",
    "    \n",
    "    def normalize(self, normalize_by_max:bool, standardize:bool): \n",
    "        # Normalize the dataset\n",
    "        # I think you should only normalize by max OR standardize, and I think standardizing would produce better results\n",
    "\n",
    "        # Separate features and target\n",
    "        df = self.dataframe # make sure data is a data frame\n",
    "        features = df.drop(['ID', 'label'], axis=1)\n",
    "        target = df['label']\n",
    "\n",
    "        df_norm = df # if normalize_by_max = false and standardize = false, will return original df\n",
    "\n",
    "        if normalize_by_max:\n",
    "            # normalizing by extremas, scales to [0,1]\n",
    "            # ensures data is well-conditioned\n",
    "            features_normalized = (features - features.min())/(features.max() - features.min())\n",
    "            df_norm = pd.concat([df[['ID']], features_normalized, df[['label']]], axis=1)\n",
    "\n",
    "        if standardize:\n",
    "            # z score normalization, good for gaussian distributions\n",
    "            # forces std 1 and mean 0\n",
    "            features_standardized = (features - features.mean())/features.std()\n",
    "            df_norm = pd.concat([df[['ID']], features_standardized, df[['label']]], axis=1)\n",
    "\n",
    "        # returns a pandas dataframe\n",
    "        return df_norm\n",
    "    \n",
    "    def crossValidation(self, folds:int): \n",
    "        # Split dataset into folds\n",
    "        # I think that self.data should only include non test data\n",
    "        data = self.data_array[:,1:] # removing first column (ID)\n",
    "        fold_size = len(data) // folds\n",
    "        validation_experiments = []\n",
    "        train_experiments = []\n",
    "\n",
    "        for i in range(folds):\n",
    "            if i==(folds-1):\n",
    "                # how should i deal with uneven split ??? is it okay for the last fold to be smaller?\n",
    "                validation_fold = data[(i*fold_size):]  #df.iloc[(i*fold_size):(len(df))]\n",
    "                train_fold = data[:(i*fold_size)] #pd.concat([df.iloc[:(i*fold_size)], df.iloc[(len(df)):]])\n",
    "\n",
    "            else:\n",
    "                validation_fold = data[(i*fold_size):(i*fold_size + fold_size)] #df.iloc[(i*fold_size):(i*fold_size + fold_size)]\n",
    "                train_fold = np.vstack([data[:(i*fold_size)], data[(i*fold_size + fold_size):]]) #pd.concat([df.iloc[:(i*fold_size)], df.iloc[(i*fold_size + fold_size):]])\n",
    "            validation_experiments.append(validation_fold)\n",
    "            train_experiments.append(train_fold)\n",
    "\n",
    "        # Train\n",
    "        avg_error1 = 0\n",
    "        avg_error2 = 0\n",
    "        for i in range(folds):\n",
    "            # train each training set with fit() to get weights\n",
    "            train_experiment = train_experiments[i]\n",
    "            w = self.fit(train_experiment)   \n",
    "\n",
    "            # get errors \n",
    "            validation_experiment = validation_experiments[i]\n",
    "            error1,error2 = self.Accu_eval(w, validation_experiment) \n",
    "            avg_error1 += error1\n",
    "            avg_error2 += error2\n",
    "\n",
    "        avg_error1 = avg_error1/folds\n",
    "        avg_error2 = avg_error2/folds\n",
    "        return (avg_error1,avg_error2)\n",
    "\n",
    "    def fit(self,train_data):\n",
    "        \n",
    "        # Trains using gradient descent: Lecture 5 slide 55-58\n",
    "\n",
    "        w_prev = np.ones(train_data.shape[1])*0.1 # (number of columns of train_data - 1 to remove label, + 1 to account for bias term)\n",
    "\n",
    "        for num_iter in range(1,self.max_iteration+1):\n",
    "            delta = np.zeros(len(w_prev))\n",
    "\n",
    "            for row_i in train_data:\n",
    "                y_i = row_i[-1] # extracting value\n",
    "                x_i = row_i.copy() # extracting features\n",
    "                x_i[-1] = 1 # because of bias term\n",
    "                delta += x_i*(y_i-self.predict(w_prev,x_i))\n",
    "\n",
    "            \n",
    "            w_new = w_prev + (self.step_size/num_iter)*delta # to be able to reach better precision need to devide step size by num iteration\n",
    "       \n",
    "            w_diff = np.linalg.norm(w_new - w_prev)\n",
    "            \n",
    "            w_prev = w_new\n",
    "            \n",
    "            if (w_diff**2<self.tolerance):\n",
    "                return w_new\n",
    "           \n",
    "            \n",
    "        # Means there was a problem\n",
    "        print(\"Not converged\", w_new , w_diff)\n",
    "        return w_new\n",
    "\n",
    "    \n",
    "    def predict(self,w,x):\n",
    "        # Predicts output: function at bottom Lecture 5 slide 44\n",
    "        a = w.T @ x\n",
    "        return 1/(1+np.exp(-a))\n",
    "\n",
    "    def Accu_eval(self,w,validate_data): # Used MSE\n",
    "        # Validation\n",
    "        error = 0\n",
    "        correct_prediction = 0\n",
    "        for row_i in validate_data:\n",
    "            y_i = row_i[-1] # extracting value\n",
    "            x_i = row_i.copy() # extracting features\n",
    "            x_i[-1] = 1\n",
    "            \n",
    "            error += (y_i-self.predict(w,x_i))**2 \n",
    "            if ((y_i-self.predict(w,x_i))<=0.5):\n",
    "                correct_prediction+=1\n",
    "            \n",
    "        return (error/len(validate_data), 1-correct_prediction/len(validate_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of original dataset :  1178.1268337574145\n",
      "Condition number of normalized dataset :  16.272627194785652\n",
      "Condition number of standardized dataset :  1.6816260377340273\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\issyl\\AppData\\Local\\Temp\\ipykernel_30448\\2782285286.py:115: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-a))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error after training original dataset :  (0.26649668292185524, 0.26060606060606056)\n",
      "Error after training normalized dataset :  (0.2465869549723802, 0.12121212121212119)\n",
      "Error after training standardized dataset :  (0.21807664123341292, 0.1727272727272727)\n"
     ]
    }
   ],
   "source": [
    "# testing whole model for CKD data with cross validation\n",
    "\n",
    "\n",
    "ckd_model = Model(df_CKD, CKD_data, \"This is the model for the CKD dataset\")\n",
    "features_ckd = df_CKD.drop(['ID', 'label'], axis=1)\n",
    "\n",
    "def condition_number(features):\n",
    "    A = features.to_numpy()\n",
    "    U, S, V = np.linalg.svd(A, full_matrices=False)\n",
    "    condition_num = np.max(S) / np.min(S[np.nonzero(S)])\n",
    "    return condition_num\n",
    "\n",
    "# normalize data\n",
    "df_norm = ckd_model.normalize(True, False)\n",
    "features_ckd_norm = df_norm.drop(['ID', 'label'], axis=1)\n",
    "ckd_model_norm = Model(df_norm, df_norm.to_numpy())\n",
    "\n",
    "# standardize data\n",
    "df_stand = ckd_model.normalize(False, True)\n",
    "features_ckd_stand = df_stand.drop(['ID', 'label'], axis=1)\n",
    "ckd_model_stand = Model(df_stand, df_stand.to_numpy())\n",
    "\n",
    "print(\"Condition number of original dataset : \", condition_number(features_ckd))\n",
    "print(\"Condition number of normalized dataset : \", condition_number(features_ckd_norm))\n",
    "print(\"Condition number of standardized dataset : \", condition_number(features_ckd_stand))\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"Error after training original dataset : \", ckd_model.crossValidation(10))\n",
    "print(\"Error after training normalized dataset : \", ckd_model_norm.crossValidation(10))\n",
    "print(\"Error after training standardized dataset : \", ckd_model_stand.crossValidation(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of original dataset :  1178.126833757448\n",
      "Condition number of normalized dataset :  16.272627194785652\n",
      "Condition number of standardized dataset :  1.6816260377340286\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_47332\\2782285286.py:115: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-a))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error after training original dataset :  (0.26649668292185524, 0.26060606060606056)\n"
     ]
    }
   ],
   "source": [
    "battery_model = Model(df_battery, battery_data, \"This is the model for the battery dataset\")\n",
    "features_battery = df_battery.drop(['ID', 'label'], axis=1)\n",
    "\n",
    "def condition_number(features):\n",
    "    A = features.to_numpy()\n",
    "    U, S, V = np.linalg.svd(A, full_matrices=False)\n",
    "    condition_num = np.max(S) / np.min(S[np.nonzero(S)])\n",
    "    return condition_num\n",
    "\n",
    "# normalize data\n",
    "df_norm = battery_model.normalize(True, False)\n",
    "features_battery_norm = df_norm.drop(['ID', 'label'], axis=1)\n",
    "battery_model_norm = Model(df_norm, df_norm.to_numpy())\n",
    "\n",
    "# standardize data\n",
    "df_stand = battery_model.normalize(False, True)\n",
    "features_battery_stand = df_stand.drop(['ID', 'label'], axis=1)\n",
    "battery_model_stand = Model(df_stand, df_stand.to_numpy())\n",
    "\n",
    "print(\"Condition number of original dataset : \", condition_number(features_battery))\n",
    "print(\"Condition number of normalized dataset : \", condition_number(features_battery_norm))\n",
    "print(\"Condition number of standardized dataset : \", condition_number(features_battery_stand))\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"Error after training original dataset : \", battery_model.crossValidation(10))\n",
    "print(\"Error after training normalized dataset : \", battery_model_norm.crossValidation(10))\n",
    "print(\"Error after training standardized dataset : \", battery_model_stand.crossValidation(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05558889  0.15980794  0.00202861  0.00334681 -0.00431049  0.00771148\n",
      " -0.00805591  0.00122004 -0.00968128 -0.00588205  0.0267138   0.02610267\n",
      "  0.00686789  0.00783587 -0.00879119  0.00966012  0.00234501  0.02384556\n",
      "  0.00453481  0.00237126  0.0209859  -0.0277358  -0.01637442  0.00510092\n",
      "  0.00684518  0.00283613 -0.00287397  0.01159189 -0.08387512]\n",
      "[1]\n",
      "Error for model with 1 quadratic features:  (0.24609957265917637, 0.1484848484848485)\n"
     ]
    }
   ],
   "source": [
    "# Adding quadratic features based on weights : features with higher weights will also have a x**2 term\n",
    "\n",
    "# normalize data\n",
    "df_ckd_norm = ckd_model.normalize(True, False)\n",
    "ckd_data_norm = df_ckd_norm.to_numpy()\n",
    "ckd_model_norm = Model(df_ckd_norm, ckd_data_norm)\n",
    "\n",
    "train_data = ckd_data_norm[:,1:] # removing first column (ID)\n",
    "weights = np.array(ckd_model_norm.fit(train_data))\n",
    "print(weights)\n",
    "\n",
    "num_quadratic_features = 3\n",
    "top_weighted_features = np.argsort(np.abs(weights[:-1]))[-num_quadratic_features:] # taking top num_quadratic_features in magnitude of weights, excluding w_0 weight\n",
    "print(top_weighted_features)\n",
    "\n",
    "# create new dataset\n",
    "df_ckd_quadratic = df_CKD.copy()\n",
    "data_ckd_quadratic = CKD_data.copy()\n",
    "\n",
    "for i in top_weighted_features:\n",
    "    df_ckd_quadratic[f'feature_{i}_squared'] = df_ckd_quadratic.iloc[:,i]**2 # adding a new quadratic feature to dataset\n",
    "\n",
    "# normalize new quadratic dataset\n",
    "ckd_model_quad = Model(df_ckd_quadratic, data_ckd_quadratic)\n",
    "df_ckd_quad_norm = ckd_model_quad.normalize(True, False)\n",
    "ckd_data_quad_norm = df_ckd_quad_norm.to_numpy()\n",
    "ckd_model_quad_norm = Model(df_ckd_quad_norm, ckd_data_quad_norm)\n",
    "\n",
    "print(f\"Error for model with {num_quadratic_features} quadratic features: \", ckd_model_quad_norm.crossValidation(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05558889  0.15980794  0.00202861  0.00334681 -0.00431049  0.00771148\n",
      " -0.00805591  0.00122004 -0.00968128 -0.00588205  0.0267138   0.02610267\n",
      "  0.00686789  0.00783587 -0.00879119  0.00966012  0.00234501  0.02384556\n",
      "  0.00453481  0.00237126  0.0209859  -0.0277358  -0.01637442  0.00510092\n",
      "  0.00684518  0.00283613 -0.00287397  0.01159189 -0.08387512]\n",
      "[10 21  0  1]\n",
      "Error for model with 4 cubic features:  (0.2458438001748032, 0.19090909090909086)\n"
     ]
    }
   ],
   "source": [
    "# Adding cubic features based on weights : features with higher weights will also have a x**3 and x**2 term\n",
    "\n",
    "# normalize data\n",
    "df_ckd_norm = ckd_model.normalize(True, False)\n",
    "ckd_data_norm = df_ckd_norm.to_numpy()\n",
    "ckd_model_norm = Model(df_ckd_norm, ckd_data_norm)\n",
    "\n",
    "train_data = ckd_data_norm[:,1:] # removing first column (ID)\n",
    "weights = np.array(ckd_model_norm.fit(train_data))\n",
    "print(weights)\n",
    "\n",
    "num_cubic_features = 3\n",
    "top_weighted_features = np.argsort(np.abs(weights[:-1]))[-num_cubic_features:] # taking top num_cubic_features in magnitude of weights, excluding w_0 weight\n",
    "print(top_weighted_features)\n",
    "\n",
    "# create new dataset\n",
    "df_ckd_cubic = df_CKD.copy()\n",
    "data_ckd_cubic = CKD_data.copy()\n",
    "\n",
    "for i in top_weighted_features:\n",
    "    df_ckd_cubic[f'feature_{i}_squared'] = df_ckd_cubic.iloc[:,i]**2 # adding a new quadratic feature to dataset\n",
    "    df_ckd_cubic[f'feature_{i}_cubed'] = df_ckd_cubic.iloc[:,i]**3 # adding a new quadratic feature to dataset\n",
    "\n",
    "# normalize new cubic dataset\n",
    "ckd_model_cube = Model(df_ckd_cubic, data_ckd_cubic)\n",
    "df_ckd_cube_norm = ckd_model_cube.normalize(True, False)\n",
    "ckd_data_cube_norm = df_ckd_cube_norm.to_numpy()\n",
    "ckd_model_cube_norm = Model(df_ckd_cube_norm, ckd_data_cube_norm)\n",
    "\n",
    "print(f\"Error for model with {num_cubic_features} cubic features: \", ckd_model_cube_norm.crossValidation(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
