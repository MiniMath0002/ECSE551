{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "# Group 13\n",
    "Mathieu Mailhot - Isabel Lougheed - Frank-Lucas Pantazis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- CKD: 28 numerical features, 1 target binary classification variable (\"Normal\" / \"CKD\")\\n- Battery: 32 real-valued features, 2 classes (\"Normal\" / \"Defective\")\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "- CKD: 28 numerical features, 1 target binary classification variable (\"Normal\" / \"battery\")\n",
    "- Battery: 32 real-valued features, 2 classes (\"Normal\" / \"Defective\")\n",
    "\"\"\"\n",
    "\n",
    "# load data sets\n",
    "\n",
    "# Calculate cross entropy or/ Information Gain for all the data without the threshold\n",
    "\n",
    "# statistical analysis on the datasets\n",
    "\n",
    "# - normalize\n",
    "\n",
    "# models: all features, selective features based on statistical analysis (dropping features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data sets\n",
    "df_CKD = pd.read_csv(\"CKD.csv\")\n",
    "df_battery = pd.read_csv(\"Battery_Dataset.csv\")\n",
    "\n",
    "# Convert \"CKD\" to 1 and \"Normal\" to 0\n",
    "df_CKD[\"label\"] = df_CKD[\"label\"].replace({\"CKD\": 1, \"Normal\": 0})\n",
    "# Convert \"Defective\" to 1 and \"Normal\" to 0\n",
    "df_battery[\"label\"] = df_battery[\"label\"].replace({\"Defective\": 1, \"Normal\": 0})\n",
    "\n",
    "# Convert to a numpy array\n",
    "CKD_data = df_CKD.to_numpy()\n",
    "battery_data = df_battery.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Analysis Block\n",
    "\n",
    "# Class for the analysis\n",
    "class Stat_analysis:\n",
    "    def __init__(self, data, name, save_folder):\n",
    "        self.data = data\n",
    "        self.name = name\n",
    "        self.save_folder = save_folder\n",
    "        self.feature_distribution()\n",
    "        self.class_distrubution()\n",
    "\n",
    "    # Function to create a distribution for each feature\n",
    "    def feature_distribution(self):\n",
    "        for i in range(self.data.shape[1] - 2): # remove 1 and last column as we do not need them for the distribution of the features\n",
    "            feature_num = i + 1\n",
    "\n",
    "            plt.hist(self.data[:,feature_num], bins=20, edgecolor=\"black\")\n",
    "            plt.xlabel(\"Value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.title(f\"{self.name} Distribution of Feature {feature_num}\")\n",
    "\n",
    "            filename = os.path.join(self.save_folder, f\"{self.name}_feature{feature_num}_distribution.png\")\n",
    "            plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "    \n",
    "    # Function to create a distribution for the class\n",
    "    def class_distrubution(self):\n",
    "        my_bins = [-0.5, 0.5, 1.5]\n",
    "        class_0 = \"Normal\"\n",
    "        if self.name == \"CKD\":\n",
    "            class_1 = \"CKD\"\n",
    "        else:\n",
    "            class_1 = \"Defective\"\n",
    "\n",
    "        plt.hist(self.data[:,self.data.shape[1] - 1], bins=my_bins, edgecolor=\"black\", align=\"mid\", rwidth=0.6)\n",
    "        plt.xticks([0, 1], [class_0, class_1])\n",
    "        plt.xlabel(\"Category\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(f\"{self.name} Distribution of Class\")\n",
    "\n",
    "        filename = os.path.join(self.save_folder, f\"{self.name}_class_distribution.png\")\n",
    "        plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "# Perform the stastical analysis\n",
    "CKD_stat = Stat_analysis(CKD_data, \"CKD\", \"CKD_distribution\")\n",
    "battery_stat = Stat_analysis(battery_data, \"Battery\", \"Battery_distribution\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,  dataframe, data_array, description:str = None):\n",
    "        \n",
    "        # Hyperparameter variables for Adam Gradient Descent Algorithm\n",
    "        self.max_iteration = 10000\n",
    "        self.tolerance = 10**-4\n",
    "        self.step_size = 0.001\n",
    "        self.b_1 = 0.99\n",
    "        \n",
    "        # Data variables\n",
    "        self.dataframe = dataframe\n",
    "        self.data_array = data_array\n",
    "        self.description = description\n",
    "        \n",
    "        # Training and Validation variables\n",
    "        self.accuracy_arr = []\n",
    "        self.avg_accuracy = 0\n",
    "        \n",
    "        self.weigth_arr = []\n",
    "        self.avg_weigth = 0\n",
    "    \n",
    "    def normalize(self, normalize_by_max:bool, standardize:bool): \n",
    "        # Normalize the dataset\n",
    "        # I think you should only normalize by max OR standardize, and I think standardizing would produce better results\n",
    "\n",
    "        # Separate features and target\n",
    "        df = self.dataframe # make sure data is a data frame\n",
    "        features = df.drop(['ID', 'label'], axis=1)\n",
    "        target = df['label']\n",
    "\n",
    "        df_norm = df # if normalize_by_max = false and standardize = false, will return original df\n",
    "\n",
    "        if normalize_by_max:\n",
    "            # normalizing by extremas, scales to [0,1]\n",
    "            # ensures data is well-conditioned\n",
    "            features_normalized = (features - features.min())/(features.max() - features.min())\n",
    "            df_norm = pd.concat([df[['ID']], features_normalized, df[['label']]], axis=1)\n",
    "\n",
    "        if standardize:\n",
    "            # z score normalization, good for gaussian distributions\n",
    "            # forces std 1 and mean 0\n",
    "            features_standardized = (features - features.mean())/features.std()\n",
    "            df_norm = pd.concat([df[['ID']], features_standardized, df[['label']]], axis=1)\n",
    "\n",
    "        # returns a pandas dataframe\n",
    "        return df_norm\n",
    "    \n",
    "    def crossValidation(self, folds:int): \n",
    "        # Split dataset into folds\n",
    "        # I think that self.data should only include non test data\n",
    "        data = self.data_array[:,1:] # removing first column (ID)\n",
    "        fold_size = len(data) // folds\n",
    "        validation_experiments = []\n",
    "        train_experiments = []\n",
    "\n",
    "        for i in range(folds):\n",
    "            if i==(folds-1):\n",
    "                # how should i deal with uneven split ??? is it okay for the last fold to be smaller?\n",
    "                validation_fold = data[(i*fold_size):]  #df.iloc[(i*fold_size):(len(df))]\n",
    "                train_fold = data[:(i*fold_size)] #pd.concat([df.iloc[:(i*fold_size)], df.iloc[(len(df)):]])\n",
    "\n",
    "            else:\n",
    "                validation_fold = data[(i*fold_size):(i*fold_size + fold_size)] #df.iloc[(i*fold_size):(i*fold_size + fold_size)]\n",
    "                train_fold = np.vstack([data[:(i*fold_size)], data[(i*fold_size + fold_size):]]) #pd.concat([df.iloc[:(i*fold_size)], df.iloc[(i*fold_size + fold_size):]])\n",
    "            validation_experiments.append(validation_fold)\n",
    "            train_experiments.append(train_fold)\n",
    "\n",
    "        # Train\n",
    "        avg_error1 = 0\n",
    "        avg_error2 = 0\n",
    "        np.array\n",
    "        for i in range(folds):\n",
    "            # train each training set with fit() to get weights\n",
    "            train_experiment = train_experiments[i]\n",
    "            w = self.fit(train_experiment)\n",
    "            self.weigth_arr.append(w.tolist())\n",
    "            # get errors \n",
    "            validation_experiment = validation_experiments[i]\n",
    "            error1,error2 = self.Accu_eval(w, validation_experiment) \n",
    "            \n",
    "            avg_error1 += error1\n",
    "            avg_error2 += error2\n",
    "        \n",
    "        self.avg_weigth = np.mean(np.array(self.weigth_arr),axis = 0)\n",
    "        \n",
    "  \n",
    "        avg_error1 = avg_error1/folds\n",
    "        avg_error2 = avg_error2/folds\n",
    "        return (avg_error1,avg_error2)\n",
    "\n",
    "    def fit(self,train_data):\n",
    "        \n",
    "        # Trains using gradient descent: Lecture 5 slide 55-58\n",
    "\n",
    "        w_prev = np.ones(train_data.shape[1])*0.1 # (number of columns of train_data - 1 to remove label, + 1 to account for bias term)\n",
    "        m = 0\n",
    "        for epoch in range(self.max_iteration):\n",
    "            delta = np.zeros(len(w_prev))\n",
    "\n",
    "            for row_i in train_data:\n",
    "                y_i = row_i[-1] # extracting value\n",
    "                x_i = row_i.copy() # extracting features\n",
    "                x_i[-1] = 1 # because of bias term\n",
    "                delta += x_i*(y_i-self.predict(w_prev,x_i))\n",
    "\n",
    "            m = self.b_1 * m + (1-self.b_1) * delta\n",
    "            #w_new = w_prev + self.step_size/(1+0.5*epoch) * delta\n",
    "            w_new = w_prev + self.step_size * m  # to be able to reach better precision need to devide step size by num iteration\n",
    "       \n",
    "            w_diff = np.linalg.norm(w_new - w_prev)\n",
    "            \n",
    "            w_prev = w_new\n",
    "            \n",
    "            if (w_diff**2<self.tolerance):\n",
    "                return w_new\n",
    "           \n",
    "            \n",
    "        # Means there was a problem\n",
    "        print(\"Not converged\", w_new , w_diff)\n",
    "        return w_new\n",
    "\n",
    "    \n",
    "    def predict(self,w,x):\n",
    "        # Predicts output: function at bottom Lecture 5 slide 44\n",
    "        a = w.T @ x\n",
    "        return 1/(1+np.exp(-np.clip(a, -500, 500)))\n",
    "\n",
    "    def Accu_eval(self,w,validate_data): # Used MSE\n",
    "        # Validation\n",
    "        error = 0\n",
    "        correct_prediction = 0\n",
    "        for row_i in validate_data:\n",
    "            y_i = row_i[-1] # extracting value\n",
    "            x_i = row_i.copy() # extracting features\n",
    "            x_i[-1] = 1\n",
    "            \n",
    "            error += (y_i-self.predict(w,x_i))**2 \n",
    "            if ((y_i-self.predict(w,x_i))<=0.5):\n",
    "                correct_prediction+=1\n",
    "            \n",
    "        return (error/len(validate_data), 1-correct_prediction/len(validate_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of normalized dataset :  16.272627194785652\n",
      "Condition number of standardized dataset :  1.6816260377340286\n",
      "---------------------------------------------------------\n",
      "Error after training normalized dataset :  (0.21288291093027514, 0.16666666666666666) [[-1.4184910875135412, 5.941666800891609, -0.7497868537664139, -0.3891041293756842, -0.20168489809480525, -0.09618833493293504, -0.22916765879222198, -1.3086308676863982, -0.9116986771652039, -0.3429581726790658, 0.25574888369639, 0.8108475023017744, -0.0316494839741422, -0.41351744818039926, -0.6456948109709769, -0.08787931528347197, 0.32234676544124596, 0.299845276026127, 0.5469430252355622, 0.4719236142378602, 0.23380917791387984, -1.118943148246078, -0.5111627979214928, 0.48197642163212057, -0.39961051953744564, 0.18682503386410923, 0.6030978220210198, 0.37598101179493065, -0.5566734155775546], [-1.5642248078796603, 5.230198869739613, -0.19805928497409928, -0.13156802285680028, -0.10866795420122584, -0.044671807821733175, -0.6208280216153876, -0.8065929763533302, -0.6700046943052821, -0.3409833845372513, 0.5615614317125572, 0.6808263272703188, -0.06031393255673458, -0.14305007276089102, -0.23859495092770075, -0.04969441872608994, 0.03798783948784247, 0.14464649691169387, 0.18113857366865346, 0.19789579145628328, 0.07513488666668754, -1.1940018118688378, -0.32684745815047084, 0.3174460406473745, -0.460838049932685, -0.567943929040778, 0.32869417825077957, 0.7005527722953474, -0.25160840043957555], [-1.7715831226940626, 5.344757124588506, -0.45114789430188706, -0.4075726361051606, -0.1810327744597022, -0.43652095278206815, -0.12868586418711472, -1.0978666404165103, -0.4651694525777529, -0.137619924435477, 0.37587167744620953, 0.6164556819466566, 0.06806138581737134, -0.360331552017278, -0.42786002924477545, 0.2656281940640517, 0.03505423524991668, -0.013996669115001944, 0.20202776396197433, 0.2037190631847644, 0.11052701738799628, -1.2400740135123123, -0.315617253287103, 0.3538416942471521, -0.09366739407811667, -0.2393790971178536, 0.06891891471819937, 0.5191063387217598, 0.0034228273508393166], [-1.7145315875206277, 5.393227683999511, -0.5146940598393318, -0.4830890144316951, 0.0049554523574062165, 0.18032613592015914, -0.24079940832930435, -1.1553043741829279, -0.5044604208787337, -0.003238547113049042, 0.3462506938263995, 0.2918714795383478, -0.3135800637496933, -0.49565684468459087, -0.30362186759515974, 0.06092028411436638, 0.0038667380835151373, 0.10731578375459416, 0.5848849411200119, 0.27184277266021434, 0.1900312791288866, -1.2739676584975832, -0.1925963447582884, 0.19922665724551228, -0.2813407236427953, 0.0005500505344118883, 0.4266066959066235, 0.5493759665111069, -0.4181498151652569], [-1.5395130053100956, 5.424414060950964, -0.19339352908068225, -0.8057747222420585, 0.015556961304846698, 0.0348607812536249, -0.3673217616038084, -1.3528841534901743, -0.5806911949790198, 0.0648387025737743, 0.26690043244316597, 0.48282055138699803, -0.15511056670078768, -0.10571733278409612, -0.49576199891302764, 0.02563084866938237, 0.1008569934321922, 0.24020749555520066, -0.001990332227295001, 0.09868602015628807, -0.17397258824670436, -0.9361357286470672, -0.5387683119761474, 0.31223925275438175, 0.04029982244873652, -0.5733771729588067, -0.08428126523898845, 1.0834277634889904, 0.03009722085010443], [-1.3534953714620526, 5.6253269549235965, -0.3646348792713459, -0.324993469840364, -0.290914703318212, -0.11968320481645575, -0.35978066614773724, -1.4512885068972547, -0.860766250379025, -0.01665129504438402, 0.8700110170498989, 0.599999966809808, -0.30275403795523764, -0.425838007046327, -0.6058421918015039, 0.34373844942426196, -0.03770212929831494, 0.0402110491250711, 0.29658086019448043, -0.003541231300502223, 0.11035283920539735, -0.9910027103301157, -0.39604049392370977, -0.18536081878268604, -0.08584172334909787, -0.23480851675389394, 0.13857853743548418, 0.41606016389562217, 0.089986236646595], [-1.5488222165756245, 5.500550245219914, -0.23910394193424353, -0.27109811284412716, -0.012066361709083461, 0.06964352142348523, -0.3830381778675933, -1.129281279765613, -0.753735116847861, -0.11965349837759903, 0.7542325118285299, 0.2703036187375277, -0.23968595072332274, -0.13256652490263235, -0.4867575227695441, -0.07688601942096943, -0.102181213995713, 0.021473711505522415, 0.12827439654633985, 0.2870218044206091, -0.11355088417496072, -0.8903898986209844, -0.42452974383829883, -0.06981714782398064, -0.0015726044073184151, 0.2785782568744062, 0.12245445059096843, 0.5951749391347919, -0.3795318580526213], [-1.500434901040651, 5.487134171383315, -0.4108720746444255, -0.41787205427898994, -0.19401990734273164, 0.15875726666839893, -0.4765286024607629, -1.3377186312305562, -0.6042644238863937, -0.252177526951401, 0.8495720597833825, 0.8424016203996293, 0.047777428708883816, -0.49137836739307056, -0.032202864830631235, 0.26856912300754027, -0.11780112554670756, 0.3265245603663914, -0.041137795618048575, 0.46612753695010534, 0.36187064021077636, -0.8789867676683453, -0.20479941645140778, -0.03181480051169474, -0.3892775744212556, -0.23932719407128056, 0.09434526317369535, 0.32543300960955296, -0.6195510232593407], [-1.514906626531185, 5.453276732086439, -0.32029011556712267, -0.38079995242782827, -0.43136471113093927, -0.12576382690266846, -0.2550901484201632, -1.3412806975997753, -0.8198101675926088, 0.07093113150259708, 0.503008164769368, 0.5666770619544557, -0.012638711949804112, 0.10090955023878563, -0.6079942419339642, 0.08961524525282263, 0.048661382223272896, 0.24754944265075002, 0.13631670171219523, 0.26480800825941453, 0.30755206900342774, -0.9156128547681359, -0.4711792858888079, -0.014133122026040413, -0.11978313883269265, -0.17929319033601482, -0.12346862794555455, 0.4813559194448539, -0.1931499324405406], [-1.729159062833264, 5.599049063143204, -0.22088127053672715, -0.33385990033416507, -0.27535490491466863, -0.15576071980894293, -0.24467179660267507, -1.6208460046683404, -1.006257482754853, -0.4224995751755489, 0.48172519298096345, 0.802889399474397, 0.1424255098849501, -0.23875654320985676, -0.1791670608662479, -0.1849571391656846, -0.03955383066699589, 0.21816515033523012, 0.37438256385925134, 0.08933305892486473, 0.25805404237378454, -0.9322559633605553, -0.8659397791775597, -0.12029786430113446, 0.026601553561606227, -0.08537281011807672, 0.43192237330979316, 1.2299497205482746, -0.32995942066186607]]\n",
      "Error after training standardized dataset :  (0.21674639064766038, 0.16969696969696968) [-0.45642166  1.52365407 -0.13877668 -0.12325337 -0.04172042 -0.03492365\n",
      " -0.10090794 -0.41386051 -0.21000226 -0.02744248  0.16433771  0.1769443\n",
      "  0.00171681 -0.09594292 -0.09601349  0.03231912  0.009121    0.06102037\n",
      "  0.09268106  0.0907736   0.03339252 -0.32265614 -0.13005664  0.05705509\n",
      " -0.06696556 -0.04638922  0.08516821  0.17200789  0.06050873]\n"
     ]
    }
   ],
   "source": [
    "# testing whole model for CKD data with cross validation\n",
    "\n",
    "\n",
    "ckd_model = Model(df_CKD, CKD_data, \"This is the model for the CKD dataset\")\n",
    "features_ckd = df_CKD.drop(['ID', 'label'], axis=1)\n",
    "\n",
    "def condition_number(features):\n",
    "    A = features.to_numpy()\n",
    "    U, S, V = np.linalg.svd(A, full_matrices=False)\n",
    "    condition_num = np.max(S) / np.min(S[np.nonzero(S)])\n",
    "    return condition_num\n",
    "\n",
    "# normalize data\n",
    "df_norm = ckd_model.normalize(True, False)\n",
    "features_ckd_norm = df_norm.drop(['ID', 'label'], axis=1)\n",
    "ckd_model_norm = Model(df_norm, df_norm.to_numpy())\n",
    "\n",
    "# standardize data\n",
    "df_stand = ckd_model.normalize(False, True)\n",
    "features_ckd_stand = df_stand.drop(['ID', 'label'], axis=1)\n",
    "ckd_model_stand = Model(df_stand, df_stand.to_numpy())\n",
    "\n",
    "#print(\"Condition number of original dataset : \", condition_number(features_ckd))\n",
    "print(\"Condition number of normalized dataset : \", condition_number(features_ckd_norm))\n",
    "print(\"Condition number of standardized dataset : \", condition_number(features_ckd_stand))\n",
    "print(\"---------------------------------------------------------\")\n",
    "#print(\"Error after training original dataset : \", ckd_model.crossValidation(10))\n",
    "print(\"Error after training normalized dataset : \", ckd_model_norm.crossValidation(10), ckd_model_norm.weigth_arr)\n",
    "\n",
    "print(\"Error after training standardized dataset : \", ckd_model_stand.crossValidation(10),ckd_model_stand.avg_weigth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of normalized dataset :  17.632795820381226\n",
      "Condition number of standardized dataset :  1.6891992219130847\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10768\\3016631408.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"---------------------------------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#print(\"Error after training original dataset : \", battery_model.crossValidation(10))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error after training normalized dataset : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbattery_model_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrossValidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbattery_model_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_weigth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error after training standardized dataset : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbattery_model_stand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrossValidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbattery_model_stand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_weigth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10768\\1378140961.py\u001b[0m in \u001b[0;36mcrossValidation\u001b[1;34m(self, folds)\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[1;31m# train each training set with fit() to get weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mtrain_experiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_experiments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_experiment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweigth_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;31m# get errors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10768\\1378140961.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_data)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mrow_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[0my_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow_i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# extracting value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 \u001b[0mx_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# extracting features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m                 \u001b[0mx_i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# because of bias term\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mdelta\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mx_i\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_i\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_prev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "battery_model = Model(df_battery, battery_data, \"This is the model for the battery dataset\")\n",
    "features_battery = df_battery.drop(['ID', 'label'], axis=1)\n",
    "\n",
    "def condition_number(features):\n",
    "    A = features.to_numpy()\n",
    "    U, S, V = np.linalg.svd(A, full_matrices=False)\n",
    "    condition_num = np.max(S) / np.min(S[np.nonzero(S)])\n",
    "    return condition_num\n",
    "\n",
    "# normalize data\n",
    "df_norm = battery_model.normalize(True, False)\n",
    "features_battery_norm = df_norm.drop(['ID', 'label'], axis=1)\n",
    "battery_model_norm = Model(df_norm, df_norm.to_numpy())\n",
    "\n",
    "# standardize data\n",
    "df_stand = battery_model.normalize(False, True)\n",
    "features_battery_stand = df_stand.drop(['ID', 'label'], axis=1)\n",
    "battery_model_stand = Model(df_stand, df_stand.to_numpy())\n",
    "\n",
    "#print(\"Condition number of original dataset : \", condition_number(features_battery))\n",
    "print(\"Condition number of normalized dataset : \", condition_number(features_battery_norm))\n",
    "print(\"Condition number of standardized dataset : \", condition_number(features_battery_stand))\n",
    "print(\"---------------------------------------------------------\")\n",
    "#print(\"Error after training original dataset : \", battery_model.crossValidation(10))\n",
    "print(\"Error after training normalized dataset : \", battery_model_norm.crossValidation(10), battery_model_norm.avg_weigth)\n",
    "print(\"Error after training standardized dataset : \", battery_model_stand.crossValidation(10),battery_model_stand.avg_weigth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
