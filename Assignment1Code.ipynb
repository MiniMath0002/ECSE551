{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "# Group 13\n",
    "Mathieu Mailhot - Isabel Lougheed - Frank-Lucas Pantazis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- CKD: 28 numerical features, 1 target binary classification variable (\"Normal\" / \"CKD\")\\n- Battery: 32 real-valued features, 2 classes (\"Normal\" / \"Defective\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "- CKD: 28 numerical features, 1 target binary classification variable (\"Normal\" / \"CKD\")\n",
    "- Battery: 32 real-valued features, 2 classes (\"Normal\" / \"Defective\")\n",
    "\"\"\"\n",
    "\n",
    "# load data sets\n",
    "\n",
    "# Calculate cross entropy or/ Information Gain for all the data without the threshold\n",
    "\n",
    "# statistical analysis on the datasets\n",
    "\n",
    "# - normalize\n",
    "\n",
    "# models: all features, selective features based on statistical analysis (dropping features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data sets\n",
    "df_CKD = pd.read_csv(\"CKD.csv\")\n",
    "df_battery = pd.read_csv(\"Battery_Dataset.csv\")\n",
    "\n",
    "# Convert \"CKD\" to 1 and \"Normal\" to 0\n",
    "df_CKD[\"label\"] = df_CKD[\"label\"].replace({\"CKD\": 1, \"Normal\": 0})\n",
    "# Convert \"Defective\" to 1 and \"Normal\" to 0\n",
    "df_battery[\"label\"] = df_battery[\"label\"].replace({\"Defective\": 1, \"Normal\": 0})\n",
    "\n",
    "# Convert to a numpy array\n",
    "CKD_data = df_CKD.to_numpy()\n",
    "battery_data = df_battery.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Analysis Block\n",
    "\n",
    "# Class for the analysis\n",
    "class Stat_analysis:\n",
    "    def __init__(self, data, name, save_folder):\n",
    "        self.data = data\n",
    "        self.name = name\n",
    "        self.save_folder = save_folder\n",
    "        self.feature_distribution()\n",
    "        self.class_distrubution()\n",
    "\n",
    "    # Function to create a distribution for each feature\n",
    "    def feature_distribution(self):\n",
    "        for i in range(self.data.shape[1] - 2): # remove 1 and last column as we do not need them for the distribution of the features\n",
    "            feature_num = i + 1\n",
    "\n",
    "            plt.hist(self.data[:,feature_num], bins=20, edgecolor=\"black\")\n",
    "            plt.xlabel(\"Value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.title(f\"{self.name} Distribution of Feature {feature_num}\")\n",
    "\n",
    "            filename = os.path.join(self.save_folder, f\"{self.name}_feature{feature_num}_distribution.png\")\n",
    "            plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "    \n",
    "    # Function to create a distribution for the class\n",
    "    def class_distrubution(self):\n",
    "        my_bins = [-0.5, 0.5, 1.5]\n",
    "        class_0 = \"Normal\"\n",
    "        if self.name == \"CKD\":\n",
    "            class_1 = \"CKD\"\n",
    "        else:\n",
    "            class_1 = \"Defective\"\n",
    "\n",
    "        plt.hist(self.data[:,self.data.shape[1] - 1], bins=my_bins, edgecolor=\"black\", align=\"mid\", rwidth=0.6)\n",
    "        plt.xticks([0, 1], [class_0, class_1])\n",
    "        plt.xlabel(\"Category\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(f\"{self.name} Distribution of Class\")\n",
    "\n",
    "        filename = os.path.join(self.save_folder, f\"{self.name}_class_distribution.png\")\n",
    "        plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "# Perform the stastical analysis\n",
    "CKD_stat = Stat_analysis(CKD_data, \"CKD\", \"CKD_distribution\")\n",
    "battery_stat = Stat_analysis(battery_data, \"Battery\", \"Battery_distribution\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,  dataframe, data_array, description:str = None):\n",
    "        self.max_iteration = 1000\n",
    "        self.tolerance = 1**-3\n",
    "        self.step_size = 1\n",
    "\n",
    "        self.dataframe = dataframe\n",
    "        self.data_array = data_array\n",
    "        self.description = description\n",
    "        \n",
    "        self.accuracy_arr = 0\n",
    "        self.avg_accuracy = 0\n",
    "        \n",
    "        self.weigth_arr = 0\n",
    "        self.avg_weigth = 0\n",
    "    \n",
    "    def normalize(self, normalize_by_max:bool, standardize:bool): \n",
    "        # Normalize the dataset\n",
    "        # I think you should only normalize by max OR standardize, and I think standardizing would produce better results\n",
    "\n",
    "        # Separate features and target\n",
    "        df = self.dataframe # make sure data is a data frame\n",
    "        features = df.drop(['ID', 'label'], axis=1)\n",
    "        target = df['label']\n",
    "\n",
    "        df_norm = df # if normalize_by_max = false and standardize = false, will return original df\n",
    "\n",
    "        if normalize_by_max:\n",
    "            # normalizing by extremas, scales to [0,1]\n",
    "            # ensures data is well-conditioned\n",
    "            features_normalized = (features - features.min())/(features.max() - features.min())\n",
    "            df_norm = pd.concat([df[['ID']], features_normalized, df[['label']]], axis=1)\n",
    "\n",
    "        if standardize:\n",
    "            # z score normalization, good for gaussian distributions\n",
    "            # forces std 1 and mean 0\n",
    "            features_standardized = (features - features.mean())/features.std()\n",
    "            df_norm = pd.concat([df[['ID']], features_standardized, df[['label']]], axis=1)\n",
    "\n",
    "        # returns a pandas dataframe\n",
    "        return df_norm\n",
    "    \n",
    "    def crossValidation(self, folds:int): \n",
    "        # Split dataset into folds\n",
    "        # I think that self.data should only include non test data\n",
    "        data = self.data_array[:,1:] # removing first column (ID)\n",
    "        fold_size = len(data) // folds\n",
    "        validation_experiments = []\n",
    "        train_experiments = []\n",
    "\n",
    "        for i in range(folds):\n",
    "            if i==(folds-1):\n",
    "                # how should i deal with uneven split ??? is it okay for the last fold to be smaller?\n",
    "                validation_fold = data[(i*fold_size):]  #df.iloc[(i*fold_size):(len(df))]\n",
    "                train_fold = data[:(i*fold_size)] #pd.concat([df.iloc[:(i*fold_size)], df.iloc[(len(df)):]])\n",
    "\n",
    "            else:\n",
    "                validation_fold = data[(i*fold_size):(i*fold_size + fold_size)] #df.iloc[(i*fold_size):(i*fold_size + fold_size)]\n",
    "                train_fold = np.vstack([data[:(i*fold_size)], data[(i*fold_size + fold_size):]]) #pd.concat([df.iloc[:(i*fold_size)], df.iloc[(i*fold_size + fold_size):]])\n",
    "            validation_experiments.append(validation_fold)\n",
    "            train_experiments.append(train_fold)\n",
    "\n",
    "        # Train\n",
    "        avg_error = 0\n",
    "\n",
    "        for i in range(folds):\n",
    "            # train each training set with fit() to get weights\n",
    "            train_experiment = train_experiments[i]\n",
    "            w = self.fit(train_experiment)   \n",
    "\n",
    "            # get errors \n",
    "            validation_experiment = validation_experiments[i]\n",
    "            error = self.Accu_eval(w, validation_experiment)\n",
    "            avg_error += error\n",
    "\n",
    "        avg_error = avg_error/folds\n",
    "\n",
    "        return avg_error\n",
    "\n",
    "    def fit(self,train_data):\n",
    "        \n",
    "        # Trains using gradient descent: Lecture 5 slide 55-58\n",
    "\n",
    "        w_prev = np.zeros(train_data.shape[1]) # (number of columns of train_data - 1 to remove label, + 1 to account for bias term)\n",
    "        w_prev[-1]=1 # bias term\n",
    "        w_new = np.zeros(len(w_prev))\n",
    "        w_diff = 1\n",
    "\n",
    "        delta = np.zeros(len(w_prev))\n",
    "\n",
    "        for k in range(self.max_iteration):\n",
    "            \n",
    "            for row_i in train_data:\n",
    "                x_i = row_i[:-1] # extracting features\n",
    "                x_i = np.append(x_i, 1) # because of bias term\n",
    "                y_i = row_i[-1] # extracting value\n",
    "                \n",
    "                delta += (y_i-self.predict(w_prev,x_i))*x_i\n",
    "\n",
    "            w_new = w_prev - self.step_size*delta\n",
    "            w_diff = np.linalg.norm(w_prev-w_new)\n",
    "            \n",
    "            if (w_diff**2<self.tolerance):\n",
    "                return w_new\n",
    "        # Means there was a problem\n",
    "        return -1\n",
    "\n",
    "    \n",
    "    def predict(self,w,x):\n",
    "        # Predicts output: function at bottom Lecture 5 slide 44\n",
    "        a = np.dot(w,x)\n",
    "        return 1/(1+np.exp(-a))\n",
    "\n",
    "    def Accu_eval(self,w,validate_data): # Used MSE\n",
    "        # Validation\n",
    "        error = 0\n",
    "        for row_i in validate_data:\n",
    "            x_i = row_i[:-1] # extracting features\n",
    "            x_i = np.append(x_i, 1) # because of bias term\n",
    "            y_i = row_i[-1] # extracting value\n",
    "\n",
    "            error += abs(y_i-self.predict(w,x_i))**2 \n",
    "        return error/len(validate_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of original dataset :  1178.126833757448\n",
      "Condition number of normalized dataset :  16.272627194785652\n",
      "Condition number of standardized dataset :  1.6816260377340286\n",
      "---------------------------------------------------------\n",
      "Error after training original dataset :  [0.3593876  0.33580925 0.36373738 0.36223034 0.36767075 0.35798222\n",
      " 0.36939785 0.37156406 0.37481658 0.36474501 0.36398977 0.36019274\n",
      " 0.36937529 0.36753336 0.3627901  0.36210817 0.36464621 0.36861166\n",
      " 0.3566093  0.36199498 0.47000481 0.49618697 0.48537032 0.49111759\n",
      " 0.49453358 0.49131355 0.5        0.5        0.30338807]\n",
      "Error after training normalized dataset :  [0.38319449 0.33351565 0.36394263 0.36236054 0.36750006 0.35858971\n",
      " 0.36993136 0.3720782  0.3751317  0.36522074 0.3639623  0.36025588\n",
      " 0.3698658  0.36807489 0.36298735 0.36243639 0.36473984 0.3684353\n",
      " 0.35724273 0.36406999 0.36278499 0.37029041 0.37244596 0.37130714\n",
      " 0.36152882 0.36043204 0.35121047 0.35321716 0.30338807]\n",
      "Error after training standardized dataset :  [1.11652029 0.79215058 1.06167989 1.07899172 1.05026805 1.02817642\n",
      " 1.09930229 1.03533679 1.07625151 1.04530179 1.02969363 1.00483423\n",
      " 1.05528962 1.05094413 1.09082726 1.03835569 1.07717235 1.02989237\n",
      " 1.09222779 1.04952044 1.02053485 1.11291443 1.07098913 1.05948187\n",
      " 1.04638611 1.05296625 1.10227065 1.05848059 0.30338807]\n"
     ]
    }
   ],
   "source": [
    "# testing whole model for CKD data with cross validation\n",
    "\n",
    "\n",
    "ckd_model = Model(df_CKD, CKD_data, \"This is the model for the CKD dataset\")\n",
    "features_ckd = df_CKD.drop(['ID', 'label'], axis=1)\n",
    "\n",
    "def condition_number(features):\n",
    "    A = features.to_numpy()\n",
    "    U, S, V = np.linalg.svd(A, full_matrices=False)\n",
    "    condition_num = np.max(S) / np.min(S[np.nonzero(S)])\n",
    "    return condition_num\n",
    "\n",
    "# normalize data\n",
    "df_norm = ckd_model.normalize(True, False)\n",
    "features_ckd_norm = df_norm.drop(['ID', 'label'], axis=1)\n",
    "ckd_model_norm = Model(df_norm, df_norm.to_numpy())\n",
    "\n",
    "# standardize data\n",
    "df_stand = ckd_model.normalize(False, True)\n",
    "features_ckd_stand = df_stand.drop(['ID', 'label'], axis=1)\n",
    "ckd_model_stand = Model(df_stand, df_stand.to_numpy())\n",
    "\n",
    "print(\"Condition number of original dataset : \", condition_number(features_ckd))\n",
    "print(\"Condition number of normalized dataset : \", condition_number(features_ckd_norm))\n",
    "print(\"Condition number of standardized dataset : \", condition_number(features_ckd_stand))\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"Error after training original dataset : \", ckd_model.crossValidation(10))\n",
    "print(\"Error after training normalized dataset : \", ckd_model_norm.crossValidation(10))\n",
    "print(\"Error after training standardized dataset : \", ckd_model_stand.crossValidation(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
