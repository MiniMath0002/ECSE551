{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "# Group 13\n",
    "Mathieu Mailhot - Isabel Lougheed - Frank-Lucas Pantazis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- CKD: 28 numerical features, 1 target binary classification variable (\"Normal\" / \"CKD\")\\n- Battery: 32 real-valued features, 2 classes (\"Normal\" / \"Defective\")\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "- CKD: 28 numerical features, 1 target binary classification variable (\"Normal\" / \"battery\")\n",
    "- Battery: 32 real-valued features, 2 classes (\"Normal\" / \"Defective\")\n",
    "\"\"\"\n",
    "\n",
    "# load data sets\n",
    "\n",
    "# Calculate cross entropy or/ Information Gain for all the data without the threshold\n",
    "\n",
    "# statistical analysis on the datasets\n",
    "\n",
    "# - normalize\n",
    "\n",
    "# models: all features, selective features based on statistical analysis (dropping features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data sets\n",
    "df_CKD = pd.read_csv(\"CKD.csv\")\n",
    "df_battery = pd.read_csv(\"Battery_Dataset.csv\")\n",
    "\n",
    "# Convert \"CKD\" to 1 and \"Normal\" to 0\n",
    "df_CKD[\"label\"] = df_CKD[\"label\"].replace({\"CKD\": 1, \"Normal\": 0})\n",
    "# Convert \"Defective\" to 1 and \"Normal\" to 0\n",
    "df_battery[\"label\"] = df_battery[\"label\"].replace({\"Defective\": 1, \"Normal\": 0})\n",
    "\n",
    "# Convert to a numpy array\n",
    "CKD_data = df_CKD.to_numpy()\n",
    "battery_data = df_battery.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Analysis Block\n",
    "\n",
    "# Class for the analysis\n",
    "class Stat_analysis:\n",
    "    def __init__(self, data, name, save_folder):\n",
    "        self.data = data\n",
    "        self.name = name\n",
    "        self.save_folder = save_folder\n",
    "        self.feature_distribution()\n",
    "        self.class_distrubution()\n",
    "\n",
    "    # Function to create a distribution for each feature\n",
    "    def feature_distribution(self):\n",
    "        for i in range(self.data.shape[1] - 2): # remove 1 and last column as we do not need them for the distribution of the features\n",
    "            feature_num = i + 1\n",
    "\n",
    "            plt.hist(self.data[:,feature_num], bins=20, edgecolor=\"black\")\n",
    "            plt.xlabel(\"Value\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.title(f\"{self.name} Distribution of Feature {feature_num}\")\n",
    "\n",
    "            filename = os.path.join(self.save_folder, f\"{self.name}_feature{feature_num}_distribution.png\")\n",
    "            plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "    \n",
    "    # Function to create a distribution for the class\n",
    "    def class_distrubution(self):\n",
    "        my_bins = [-0.5, 0.5, 1.5]\n",
    "        class_0 = \"Normal\"\n",
    "        if self.name == \"CKD\":\n",
    "            class_1 = \"CKD\"\n",
    "        else:\n",
    "            class_1 = \"Defective\"\n",
    "\n",
    "        plt.hist(self.data[:,self.data.shape[1] - 1], bins=my_bins, edgecolor=\"black\", align=\"mid\", rwidth=0.6)\n",
    "        plt.xticks([0, 1], [class_0, class_1])\n",
    "        plt.xlabel(\"Category\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(f\"{self.name} Distribution of Class\")\n",
    "\n",
    "        filename = os.path.join(self.save_folder, f\"{self.name}_class_distribution.png\")\n",
    "        plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "# Perform the stastical analysis\n",
    "CKD_stat = Stat_analysis(CKD_data, \"CKD\", \"CKD_distribution\")\n",
    "battery_stat = Stat_analysis(battery_data, \"Battery\", \"Battery_distribution\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,  dataframe, data_array, description:str = None):\n",
    "        \n",
    "        # Hyperparameter variables for Adam Gradient Descent Algorithm\n",
    "        self.max_iteration = 10000\n",
    "        self.tolerance = 10**-3\n",
    "        self.step_size = 0.01\n",
    "        self.b_1 = 0.93\n",
    "        \n",
    "        # Data variables\n",
    "        self.dataframe = dataframe\n",
    "        self.data_array = data_array\n",
    "        self.description = description\n",
    "        \n",
    "        # Training and Validation variables\n",
    "        self.accuracy_arr = []\n",
    "        self.avg_accuracy = 0\n",
    "        \n",
    "        self.weigth_arr = []\n",
    "        self.avg_weigth = 0\n",
    "    \n",
    "    def normalize(self, normalize_by_max:bool, standardize:bool): \n",
    "        # Normalize the dataset\n",
    "        # I think you should only normalize by max OR standardize, and I think standardizing would produce better results\n",
    "\n",
    "        # Separate features and target\n",
    "        df = self.dataframe # make sure data is a data frame\n",
    "        features = df.drop(['ID', 'label'], axis=1)\n",
    "        target = df['label']\n",
    "\n",
    "        df_norm = df # if normalize_by_max = false and standardize = false, will return original df\n",
    "\n",
    "        if normalize_by_max:\n",
    "            # normalizing by extremas, scales to [0,1]\n",
    "            # ensures data is well-conditioned\n",
    "            features_normalized = (features - features.min())/(features.max() - features.min())\n",
    "            df_norm = pd.concat([df[['ID']], features_normalized, df[['label']]], axis=1)\n",
    "\n",
    "        if standardize:\n",
    "            # z score normalization, good for gaussian distributions\n",
    "            # forces std 1 and mean 0\n",
    "            features_standardized = (features - features.mean())/features.std()\n",
    "            df_norm = pd.concat([df[['ID']], features_standardized, df[['label']]], axis=1)\n",
    "\n",
    "        # returns a pandas dataframe\n",
    "        return df_norm\n",
    "    \n",
    "    def crossValidation(self, folds:int): \n",
    "        # Split dataset into folds\n",
    "        # I think that self.data should only include non test data\n",
    "        data = self.data_array[:,1:] # removing first column (ID)\n",
    "        fold_size = len(data) // folds\n",
    "        validation_experiments = []\n",
    "        train_experiments = []\n",
    "\n",
    "        for i in range(folds):\n",
    "            if i==(folds-1):\n",
    "                # how should i deal with uneven split ??? is it okay for the last fold to be smaller?\n",
    "                validation_fold = data[(i*fold_size):]  #df.iloc[(i*fold_size):(len(df))]\n",
    "                train_fold = data[:(i*fold_size)] #pd.concat([df.iloc[:(i*fold_size)], df.iloc[(len(df)):]])\n",
    "\n",
    "            else:\n",
    "                validation_fold = data[(i*fold_size):(i*fold_size + fold_size)] #df.iloc[(i*fold_size):(i*fold_size + fold_size)]\n",
    "                train_fold = np.vstack([data[:(i*fold_size)], data[(i*fold_size + fold_size):]]) #pd.concat([df.iloc[:(i*fold_size)], df.iloc[(i*fold_size + fold_size):]])\n",
    "            validation_experiments.append(validation_fold)\n",
    "            train_experiments.append(train_fold)\n",
    "\n",
    "        # Train\n",
    "        avg_error1 = 0\n",
    "        avg_error2 = 0\n",
    "        np.array\n",
    "        for i in range(folds):\n",
    "            # train each training set with fit() to get weights\n",
    "            train_experiment = train_experiments[i]\n",
    "            w = self.fit(train_experiment)\n",
    "            self.weigth_arr.append(w.tolist())\n",
    "            # get errors \n",
    "            validation_experiment = validation_experiments[i]\n",
    "            error1,error2 = self.Accu_eval(w, validation_experiment) \n",
    "            \n",
    "            avg_error1 += error1\n",
    "            avg_error2 += error2\n",
    "        \n",
    "        self.avg_weigth = np.mean(np.array(self.weigth_arr),axis = 0)\n",
    "        \n",
    "  \n",
    "        avg_error1 = avg_error1/folds\n",
    "        avg_error2 = avg_error2/folds\n",
    "        return (avg_error1,avg_error2)\n",
    "\n",
    "    def fit(self,train_data):\n",
    "        \n",
    "        # Trains using gradient descent: Lecture 5 slide 55-58\n",
    "\n",
    "        w_prev = np.ones(train_data.shape[1])*0.1 # (number of columns of train_data - 1 to remove label, + 1 to account for bias term)\n",
    "        m = 0\n",
    "        for epoch in range(self.max_iteration):\n",
    "            delta = np.zeros(len(w_prev))\n",
    "\n",
    "            for row_i in train_data:\n",
    "                y_i = row_i[-1] # extracting value\n",
    "                x_i = row_i.copy() # extracting features\n",
    "                x_i[-1] = 1 # because of bias term\n",
    "                delta += x_i*(y_i-self.predict(w_prev,x_i))\n",
    "\n",
    "            m = self.b_1 * m + (1-self.b_1) * delta\n",
    "            w_new = w_prev + self.step_size * m # to be able to reach better precision need to devide step size by num iteration\n",
    "       \n",
    "            w_diff = np.linalg.norm(w_new - w_prev)\n",
    "            \n",
    "            w_prev = w_new\n",
    "            \n",
    "            if (w_diff**2<self.tolerance):\n",
    "                return w_new\n",
    "           \n",
    "            \n",
    "        # Means there was a problem\n",
    "        print(\"Not converged\", w_new , w_diff)\n",
    "        return w_new\n",
    "\n",
    "    \n",
    "    def predict(self,w,x):\n",
    "        # Predicts output: function at bottom Lecture 5 slide 44\n",
    "        a = w.T @ x\n",
    "        return 1/(1+np.exp(-np.clip(a, -500, 500)))\n",
    "\n",
    "    def Accu_eval(self,w,validate_data): # Used MSE\n",
    "        # Validation\n",
    "        error = 0\n",
    "        correct_prediction = 0\n",
    "        for row_i in validate_data:\n",
    "            y_i = row_i[-1] # extracting value\n",
    "            x_i = row_i.copy() # extracting features\n",
    "            x_i[-1] = 1\n",
    "            \n",
    "            error += (y_i-self.predict(w,x_i))**2 \n",
    "            if ((y_i-self.predict(w,x_i))<=0.5):\n",
    "                correct_prediction+=1\n",
    "            \n",
    "        return (error/len(validate_data), 1-correct_prediction/len(validate_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of normalized dataset :  16.272627194785652\n",
      "Condition number of standardized dataset :  1.6816260377340286\n",
      "---------------------------------------------------------\n",
      "Error after training normalized dataset :  (0.210624438089218, 0.16363636363636364) [[-1.388483900520639, 5.378751515011773, -0.5122352533582972, -0.2583136879542226, -0.18983052878476656, -0.02974513986772246, -0.282649543144278, -1.1644466814414245, -0.9067668160423416, -0.40623361015646436, 0.23650155960519312, 0.7524265471395645, -0.13614778002974506, -0.28506762754678294, -0.7245981182278461, -0.16316134613449917, 0.36180120997761034, 0.24124775206975163, 0.4072821098162617, 0.35412890574298805, 0.3163026357272578, -1.0432217522485654, -0.5509806290536292, 0.40672245481357666, -0.28893174224284074, 0.16940716747135887, 0.34518117192083625, 0.2527877726780731, -0.2857297162604279], [-1.6195990571363437, 4.959709199176352, -0.08214205281584519, -0.026097158674979307, -0.11932896032843217, 0.035757209502644036, -0.6475201989817994, -0.6695809395009766, -0.64094623849027, -0.33161245730848643, 0.5975464915960502, 0.7035876486369241, -0.11041549108286569, -0.0905386921270276, -0.2970129468622281, -3.482512695596998e-05, 0.0733451588638164, 0.1254321944671836, 0.12835083241241557, 0.16354707356885906, 0.14737444757873142, -1.1560958773189904, -0.303246713592721, 0.32524781443775247, -0.39537163874915127, -0.5073041533047784, 0.16708925213373965, 0.5161561609890235, -0.15558250741900373], [-1.7643240527288024, 4.958048088705626, -0.25886172944602837, -0.2924081873433594, -0.19138718283992215, -0.3346730265323996, -0.1561003290420429, -0.9409435415651677, -0.47691155319540424, -0.19164509033891072, 0.42751898858831494, 0.6536875452184792, 0.018716678426135832, -0.29180817277020016, -0.44429341458649063, 0.29653105458864076, 0.10311387382271768, -0.052773313830476634, 0.16790725546937535, 0.16022719505309027, 0.22644853374931267, -1.1589786926442354, -0.33803283762232794, 0.28699572785806593, 0.04650699131341289, -0.20376258417912047, -0.03348920700558489, 0.2925343783117945, 0.03679765844823982], [-1.7025621796793446, 5.021119198296869, -0.36517390598705435, -0.40811945467503186, 0.018838407024079865, 0.2566552518559324, -0.28985977521798717, -0.9924187110479179, -0.4960079667798056, -0.08404047426913816, 0.3469513995369765, 0.37276642505465957, -0.34043447452884684, -0.39930321721769724, -0.38681780552596756, 0.057005554896492366, 0.024163366045846263, 0.07576008237626539, 0.51321690766566, 0.25765071357176395, 0.23845284665559566, -1.2256410604330266, -0.2108579508787865, 0.1963212069021255, -0.1869982256543889, 0.055191613124480496, 0.23659296264139332, 0.4026914064207404, -0.2357211281986288], [-1.5299400006971935, 5.080247532706654, -0.08049299281666665, -0.7084279749086813, 0.007592857279386102, 0.12686278581490476, -0.39091014003568547, -1.1811287796219767, -0.5784647432098026, 0.06297183504631676, 0.2817284936701166, 0.5399577291662738, -0.17257460987828388, -0.04556847461416288, -0.5087335279501657, 0.071628599470839, 0.11031866979214468, 0.18453933278340262, 0.003062102680930657, 0.10476070132799885, -0.07970917878256338, -0.9073360825653973, -0.5594206938671631, 0.3387144634866774, 0.09007340020226946, -0.5049759337859143, -0.17783268170712657, 0.8176473750127267, 0.07145847603766528], [-1.3377059893082384, 5.213629329869169, -0.20536568940119532, -0.2398903340332385, -0.2591782924681391, -0.029650709305377526, -0.3828673189166221, -1.2315027114805825, -0.7916996747792435, -0.06707131706542833, 0.8636114170155285, 0.6208974855814586, -0.27694338112136413, -0.3157178739879567, -0.5487396465772701, 0.3021342406528664, 0.025068706337161642, 0.04029361463743992, 0.253411608589389, -0.036943127856739845, 0.12986046097815382, -0.9193221925600379, -0.34186302489315334, -0.1665668087158821, -0.0396551253547758, -0.24302608009737467, 0.020013039860678063, 0.2559036209167707, 0.06040741353452245], [-1.5174095947935546, 5.151364492165109, -0.09426750250422886, -0.1853291946403927, -0.0048150688361645304, 0.10172807085727609, -0.42306658957355925, -1.0067746797848984, -0.7423028810994385, -0.17745784145382065, 0.8099609301604792, 0.3120546190557938, -0.298959459320743, -0.06601231460354136, -0.5183716267941614, -0.0796014430391625, -0.09357796880642229, -0.029665474661022418, 0.0915537271488586, 0.26124050073743116, -0.028853974647237185, -0.8607057356088752, -0.4586234074151987, -0.13673975221764878, 0.09521471244798722, 0.3510253645621502, 0.010338780247161696, 0.41714485122130546, -0.21097586536001592], [-1.5187631844363951, 5.111490974769937, -0.2839779703046658, -0.32941600126094855, -0.2231151959953786, 0.20491825000234393, -0.5240598132338459, -1.1897675113544306, -0.6374510709365494, -0.30598813483008613, 0.8689160769211907, 0.862469814215315, -0.04001688631665701, -0.3940358984917436, -0.10619867610067743, 0.2599885441544493, -0.08499493245331778, 0.25906417132002185, -0.08105004347117457, 0.3741451403645352, 0.4747748168048022, -0.8594102288490658, -0.22572564509296145, -0.033714803471944735, -0.28289075766904564, -0.1747789858046913, -0.03841263813049937, 0.24849683628515795, -0.3817375805382348], [-1.5618999001318903, 5.133391432339749, -0.19352034996307851, -0.3422412496283881, -0.42756875983647136, -0.07800254905684449, -0.3296263413691272, -1.2186071274690833, -0.8390728905274343, 0.005894255873709708, 0.525080241997822, 0.640536220773681, -0.03166745176283933, 0.18317151367394915, -0.6536194443505708, 0.13804809965262835, 0.10302448452920585, 0.22509656278179813, 0.13021451060935604, 0.2388378990062, 0.3723161885560921, -0.8500733713579147, -0.49540054614968304, -0.010746023066258083, -0.03141488485771522, -0.12129800967441759, -0.1775444700371285, 0.3349533676090597, -0.10916913423062896], [-1.7000096530097337, 5.104821591013022, -0.0705619848833848, -0.24706545517355882, -0.27721928499758186, -0.05110024465012653, -0.2588104280486816, -1.4276339042748467, -1.0079447006049413, -0.41070073846060107, 0.5042086843580397, 0.8079772728978989, 0.043648785352040094, -0.1580266730142645, -0.27713939702768076, -0.13059600338806665, -0.03669178594441251, 0.1602141019205959, 0.2600934975759899, 0.11941127458374343, 0.35237154283433686, -0.8599063173959457, -0.8579569785206506, -0.09496176996524158, 0.12099702941636707, 0.017587372020340858, 0.10391402323156282, 0.8359813980332464, -0.22316084993504418]]\n",
      "Error after training standardized dataset :  (0.21264059079105485, 0.17878787878787877) [ 0.02900555  0.00839274  0.00074762  0.01523842 -0.00338331 -0.01000443\n",
      "  0.01549812  0.02439339  0.00829792  0.01135928]\n"
     ]
    }
   ],
   "source": [
    "# testing whole model for CKD data with cross validation\n",
    "\n",
    "\n",
    "ckd_model = Model(df_CKD, CKD_data, \"This is the model for the CKD dataset\")\n",
    "features_ckd = df_CKD.drop(['ID', 'label'], axis=1)\n",
    "\n",
    "def condition_number(features):\n",
    "    A = features.to_numpy()\n",
    "    U, S, V = np.linalg.svd(A, full_matrices=False)\n",
    "    condition_num = np.max(S) / np.min(S[np.nonzero(S)])\n",
    "    return condition_num\n",
    "\n",
    "# normalize data\n",
    "df_norm = ckd_model.normalize(True, False)\n",
    "features_ckd_norm = df_norm.drop(['ID', 'label'], axis=1)\n",
    "ckd_model_norm = Model(df_norm, df_norm.to_numpy())\n",
    "\n",
    "# standardize data\n",
    "df_stand = ckd_model.normalize(False, True)\n",
    "features_ckd_stand = df_stand.drop(['ID', 'label'], axis=1)\n",
    "ckd_model_stand = Model(df_stand, df_stand.to_numpy())\n",
    "\n",
    "#print(\"Condition number of original dataset : \", condition_number(features_ckd))\n",
    "print(\"Condition number of normalized dataset : \", condition_number(features_ckd_norm))\n",
    "print(\"Condition number of standardized dataset : \", condition_number(features_ckd_stand))\n",
    "print(\"---------------------------------------------------------\")\n",
    "#print(\"Error after training original dataset : \", ckd_model.crossValidation(10))\n",
    "print(\"Error after training normalized dataset : \", ckd_model_norm.crossValidation(10), ckd_model_norm.weigth_arr)\n",
    "\n",
    "print(\"Error after training standardized dataset : \", ckd_model_stand.crossValidation(10),ckd_model_stand.avg_weigth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of normalized dataset :  17.632795820381226\n",
      "Condition number of standardized dataset :  1.6891992219130847\n",
      "---------------------------------------------------------\n",
      "Error after training normalized dataset :  (0.0860907227531073, 0.060091324200913274)\n",
      "Error after training standardized dataset :  (0.08748855983724914, 0.06009132420091327)\n"
     ]
    }
   ],
   "source": [
    "battery_model = Model(df_battery, battery_data, \"This is the model for the battery dataset\")\n",
    "features_battery = df_battery.drop(['ID', 'label'], axis=1)\n",
    "\n",
    "def condition_number(features):\n",
    "    A = features.to_numpy()\n",
    "    U, S, V = np.linalg.svd(A, full_matrices=False)\n",
    "    condition_num = np.max(S) / np.min(S[np.nonzero(S)])\n",
    "    return condition_num\n",
    "\n",
    "# normalize data\n",
    "df_norm = battery_model.normalize(True, False)\n",
    "features_battery_norm = df_norm.drop(['ID', 'label'], axis=1)\n",
    "battery_model_norm = Model(df_norm, df_norm.to_numpy())\n",
    "\n",
    "# standardize data\n",
    "df_stand = battery_model.normalize(False, True)\n",
    "features_battery_stand = df_stand.drop(['ID', 'label'], axis=1)\n",
    "battery_model_stand = Model(df_stand, df_stand.to_numpy())\n",
    "\n",
    "#print(\"Condition number of original dataset : \", condition_number(features_battery))\n",
    "print(\"Condition number of normalized dataset : \", condition_number(features_battery_norm))\n",
    "print(\"Condition number of standardized dataset : \", condition_number(features_battery_stand))\n",
    "print(\"---------------------------------------------------------\")\n",
    "#print(\"Error after training original dataset : \", battery_model.crossValidation(10))\n",
    "print(\"Error after training normalized dataset : \", battery_model_norm.crossValidation(10))\n",
    "print(\"Error after training standardized dataset : \", battery_model_stand.crossValidation(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
