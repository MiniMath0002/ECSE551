{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "# Group 13\n",
    "Mathieu Mailhot - Isabel Lougheed - Frank-Lucas Pantazis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "#https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34\n",
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "folds = 10 # between 5 and 10 # best value at the moment when folds = 30\n",
    "\n",
    "# Loading Training data\n",
    "df_train = pd.read_csv('train.csv', encoding='utf-8', encoding_errors='ignore') # errors were not pertinent characters\n",
    "df_train[\"subreddit\"] = df_train[\"subreddit\"].map({\"Boston\": 0, \"Canberra\": 1,\"Geneva\":2,\"Ottawa\":3})\n",
    "\n",
    "y = df_train[\"subreddit\"]\n",
    "X = df_train[\"body\"]\n",
    "\n",
    "\n",
    "# Loading Test Data\n",
    "df_test = pd.read_csv('test.csv', encoding='utf-8', encoding_errors='ignore') # errors were not pertinent characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Helper Functions</h2>\n",
    "<h3>Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokenizer2(subreddit_post, stop_word_catalogue=None):\n",
    "    subreddit_post = subreddit_post.lower()\n",
    "    words = word_tokenize(subreddit_post)\n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    \n",
    "    lemmatized_words = []\n",
    "    \n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(words):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        condition = True\n",
    "        if (stop_word_catalogue!=None):\n",
    "            condition = word not in stop_word_catalogue\n",
    "        if(condition):\n",
    "            if word.isalnum():\n",
    "                lemmatized_word = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                #print(word,lemmatized_word)\n",
    "                lemmatized_words.append(lemmatized_word)                   \n",
    "    return lemmatized_words\n",
    "\n",
    "def lemmatize_tokenizer(subreddit_post, stop_word_catalogue=None):\n",
    "    subreddit_post = subreddit_post.lower()\n",
    "    words = word_tokenize(subreddit_post)\n",
    "\n",
    "    lemmatized_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word in words:\n",
    "        if word.isalpha():\n",
    "            lemmatized_word = word_Lemmatized.lemmatize(word)\n",
    "            lemmatized_words.append(lemmatized_word)  \n",
    " \n",
    "    return lemmatized_words\n",
    "\n",
    "def preprocessing_data(X, stop_word_catalogue,version=True):\n",
    "    lemmatized_data = []\n",
    "    for i in range(len(X)):\n",
    "        post = X[i]\n",
    "        if (version):\n",
    "            lemmatized_post = lemmatize_tokenizer(post,stop_word_catalogue)\n",
    "        else:\n",
    "            lemmatized_post = lemmatize_tokenizer2(post,stop_word_catalogue)\n",
    "        lemmatized_data.append(\" \".join(lemmatized_post))\n",
    "    return lemmatized_data\n",
    "\n",
    "def feature_extraction(X,ngram_range,min_df,caption=\"\"):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range,min_df=min_df)\n",
    "    X_disp = vectorizer.fit_transform(X)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(caption,\"features:\", len(feature_names))\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train and Tunning<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# This function does all the tunning for each model\n",
    "def hyperparamaterTunning(X,param, folds, model, verbose_val=1, Y=y):\n",
    "    \n",
    "    model_gridSearch = GridSearchCV(model, param_grid=param,cv=folds, verbose=verbose_val) # According to doc the data will be split the same way accross all calls\n",
    "\n",
    "    model_best_clf = model_gridSearch.fit(X,Y)\n",
    "\n",
    "    cv_results = model_gridSearch.cv_results_\n",
    "    \n",
    "    print()\n",
    "\n",
    "    print(f\"Best Parameters: {model_best_clf.best_params_}\")\n",
    "    try:\n",
    "\n",
    "        best_index = model_gridSearch.best_index_\n",
    "\n",
    "        score = []\n",
    "        for fold in range(folds):\n",
    "            score.append(model_gridSearch.cv_results_[f\"split{fold}_test_score\"][best_index].item())\n",
    "\n",
    "        print(f\"Cross-validation Accuracies: {score}\")\n",
    "    except:\n",
    "        pass\n",
    "    print(f\"Mean Accuracy: {model_best_clf.best_score_:.4f}\")\n",
    "\n",
    "    return model_best_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Preprocessing </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Word Catalogues\n",
    "nltk_stop_words = stopwords.words('english')\n",
    "sklean_stop_words = list(text.ENGLISH_STOP_WORDS)\n",
    "stop_words_arr = list(text.ENGLISH_STOP_WORDS.union(nltk_stop_words))\n",
    "\n",
    "# Trainning Data\n",
    "#pp_df_train_1 = X.tolist()\n",
    "pp_df_train_1 = preprocessing_data(X,nltk_stop_words,False)\n",
    "pp_df_train_2 = preprocessing_data(X,sklean_stop_words,False)\n",
    "pp_df_train_3 = preprocessing_data(X,stop_words_arr,False)\n",
    "\n",
    "#pp_df_train = [X,pp_df_train_1,pp_df_train_2,pp_df_train_3,pp_df_train_4]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Augmentation<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n",
      "390\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation\n",
    "import nlpaug.augmenter.word as naw\n",
    "from langdetect import detect\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "aug = naw.SynonymAug(aug_src='wordnet', aug_p=0.3)\n",
    "augmented = []\n",
    "augmented_y = []\n",
    "\n",
    "pp_train = pp_df_train_2\n",
    "numbers = range(len(pp_train))\n",
    "picked_numbers = random.sample(numbers, int(len(pp_train)*0.3))\n",
    "\n",
    "for i in picked_numbers:\n",
    "\n",
    "    post = pp_train[i]\n",
    "    try:\n",
    "        if (detect(post)==\"en\"):\n",
    "            augmented.append(aug.augment(post)[0])\n",
    "            augmented_y.append(y[i].item())\n",
    "        else:\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(augmented))\n",
    "print(len(augmented_y))\n",
    "\n",
    "augmented_df_train = pp_train + augmented\n",
    "augmented_df_y = y.tolist() + augmented_y\n",
    "\n",
    "X_arr = np.array(augmented_df_train)\n",
    "Y_arr = np.array(augmented_df_y)\n",
    "\n",
    "indices = np.arange(len(X_arr))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Reshuffling data --> due to addition of data augmentation\n",
    "X_shuffled = X_arr[indices]\n",
    "y_shuffled = Y_arr[indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train<h2>\n",
    "<h3>Feature Extraction<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10299\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tfidf_primary = TfidfVectorizer(ngram_range=(1,1),sublinear_tf=True,stop_words=\"english\")\n",
    "X_uni_primary = tfidf_primary.fit_transform(X_shuffled)\n",
    "print(len(tfidf_primary.get_feature_names_out()))\n",
    "\n",
    "naiveBayes = CountVectorizer(max_features=3000, ngram_range=(1, 1), stop_words=\"english\")\n",
    "X_naive_bayes = naiveBayes.fit_transform(X).toarray()\n",
    "print(len(naiveBayes.get_feature_names_out()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Reduction</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1790\n"
     ]
    }
   ],
   "source": [
    "# TRUCATION\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# Step 1: Fit initial SVD with many components\n",
    "svd = TruncatedSVD(n_components=2000)  # Adjust based on data\n",
    "X_svd = svd.fit_transform(X_uni_primary)\n",
    "print(len(X_svd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Naive Bayes Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "class NaiveBayes:\n",
    "    def __init__(self, x_all, y_all, feature_vectoriser):\n",
    "        self.x_all = self.clean_text_data(x_all) # Make lists of strings\n",
    "        self.y_all = y_all\n",
    "        self.feature_vectoriser = feature_vectoriser\n",
    "\n",
    "        self.folds_features_probability = 0 # array of dict\n",
    "        self.folds_accuracy = 0\n",
    "        self.avg_accuracy = 0\n",
    "\n",
    "    \n",
    "    def calc_probability(self, x, y): # Train/Fit # Mathieu\n",
    "        # Create an empty dictionnary with the 3000 most common words for each subreddit.\n",
    "        features_probability_boston = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "        features_probability_canberra = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "        features_probability_geneva = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "        features_probability_ottawa = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "\n",
    "        # Initialize the count for the total number of text from each subreddit\n",
    "        count_boston = 0\n",
    "        count_canberra = 0\n",
    "        count_geneva = 0\n",
    "        count_ottowa = 0\n",
    "\n",
    "        # Add 1 to the word in the dictionnary when the word is present in the text\n",
    "        for i in range(y.shape[0]):\n",
    "            if y[i] == 0:\n",
    "                count_boston += 1\n",
    "                self.add_probability(features_probability_boston, x[i])\n",
    "            if y[i] == 1:\n",
    "                count_canberra += 1\n",
    "                self.add_probability(features_probability_canberra, x[i])\n",
    "            if y[i] == 2:\n",
    "                count_geneva += 1\n",
    "                self.add_probability(features_probability_geneva, x[i])\n",
    "            else:\n",
    "                count_ottowa += 1\n",
    "                self.add_probability(features_probability_ottawa, x[i])\n",
    "\n",
    "        # Add the total count of each city to a variable called \"city_count\" and the probability of each city in a variable called \"city_probability\" in each one of the dictionary\n",
    "        features_probability_boston[\"city_count\"] = count_boston\n",
    "        features_probability_boston[\"city_probability\"] = count_boston / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "        features_probability_canberra[\"city_count\"] = count_canberra\n",
    "        features_probability_canberra[\"city_probability\"] = count_canberra / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "        features_probability_geneva[\"city_count\"] = count_geneva\n",
    "        features_probability_geneva[\"city_probability\"] = count_geneva / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "        features_probability_ottawa[\"city_count\"] = count_ottowa\n",
    "        features_probability_ottawa[\"city_probability\"] = count_ottowa / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "                \n",
    "        return features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa # return all dictionaries\n",
    "\n",
    "    def clean_text_data(self, x): # Helper function to make a list of lists of words \n",
    "        # Take text remove all capitalized letters, removed special characters and make an array of words.\n",
    "        cleaned_data = [\n",
    "            re.sub(r'[^a-z0-9\\s]', '', text.lower()).split()\n",
    "            for text in x\n",
    "        ]\n",
    "        print(\"This is the cleaned data\", cleaned_data[0])\n",
    "        return cleaned_data # return a list of lists of words (better to use lists for this since numpy is mostly for numerical values)\n",
    "    \n",
    "    def add_probability(self, city_dict, x): # Helper function to update probabilities given a dict and a list of words\n",
    "        for word in set(x): # Creates a set from words(unique elements)\n",
    "            if word in city_dict:\n",
    "                city_dict[word] += 1\n",
    "                \n",
    "    \n",
    "    def predict(self, features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa, x_i): # (Is x_i in formula equal to 1?)\n",
    "        # Initialize probabilities for each subreddit\n",
    "        prob_boston = features_probability_boston[\"city_probability\"]\n",
    "        prob_canberra = features_probability_canberra[\"city_probability\"]\n",
    "        prob_geneva = features_probability_geneva[\"city_probability\"]\n",
    "        prob_ottowa = features_probability_ottawa[\"city_probability\"]\n",
    "\n",
    "        for word in x_i:\n",
    "            if word in features_probability_boston: # All have the same most common words\n",
    "                # Laplace smoothing\n",
    "                prob_boston = prob_boston * ((features_probability_boston[word] + 1) / (features_probability_boston[\"city_count\"] + 2))\n",
    "                prob_canberra = prob_canberra * ((features_probability_canberra[word] + 1) / (features_probability_canberra[\"city_count\"] + 2))\n",
    "                prob_geneva = prob_geneva * ((features_probability_geneva[word] + 1) / (features_probability_geneva[\"city_count\"] + 2))\n",
    "                prob_ottowa = prob_ottowa * ((features_probability_ottawa[word] + 1) / (features_probability_ottawa[\"city_count\"] + 2))\n",
    "        \n",
    "        probabilities = np.array([prob_boston, prob_canberra, prob_geneva, prob_ottowa])\n",
    "        \n",
    "        return np.argmax(probabilities)\n",
    "\n",
    "    def accu_eval(self, x, y): # Issy\n",
    "        # Validation\n",
    "        # Returns Accuracy = 1 - Error\n",
    "\n",
    "        num_correct_labels = 0\n",
    "\n",
    "        # Get probabilities / train model\n",
    "        features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa = self.calc_probability(x,y)\n",
    "\n",
    "        # Predict\n",
    "        for i in range(len(x)):\n",
    "            predicted_label = self.predict(features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa, x[i])\n",
    "            if predicted_label == y[i]:\n",
    "                num_correct_labels += 1\n",
    "\n",
    "        # Get accuracy\n",
    "        accuracy = num_correct_labels/len(y)\n",
    "        return accuracy\n",
    "\n",
    "    def crossValidation(self, k): # Issy (PS: I think we are allowed to use the method from sklearn)\n",
    "        kf = KFold(n_splits=k, shuffle=True, random_state=9)\n",
    "        accuracies = []\n",
    "\n",
    "        # Split into train and validation sets\n",
    "        for train_indices, val_indices in kf.split(self.x_all):\n",
    "            x_train = [self.x_all[i] for i in train_indices] # separate x into training subset\n",
    "            x_val = [self.x_all[i] for i in val_indices] # separate x into validating subset\n",
    "\n",
    "            y_train = [self.y_all[i] for i in train_indices] # separate y into training subset\n",
    "            y_val = [self.y_all[i] for i in val_indices] # separate y into validating subset\n",
    "\n",
    "            \n",
    "            # For each set, get probabilities / train with training set\n",
    "            features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa = self.calc_probability(x_train,np.array(y_train))\n",
    "\n",
    "            # check with validation subset\n",
    "            num_correct_labels = 0\n",
    "            for i in range(len(y_val)):\n",
    "                predicted_label = self.predict(features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa, x_val[i])\n",
    "                if predicted_label == y_val[i]:\n",
    "                    num_correct_labels += 1\n",
    "        \n",
    "            # calculate accuracy\n",
    "            accuracy = num_correct_labels / len(y_val)\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "        self.folds_accuracy = accuracies\n",
    "        self.avg_accuracy = np.mean(accuracies)\n",
    "        return self.avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the cleaned data ['i', 'had', 'to', 'put', 'in', 'a', 'drain', 'wellfrench', 'drain', 'and', 'the', 'ground', 'about', '6', 'inches', 'down', 'was', 'all', 'mud', 'and', 'clay', 'i', 'was', 'ass', 'over', 'end', 'in', 'this', 'hole', 'scooping', 'clay', 'mud', 'and', 'was', 'joined', 'by', 'probably', 'ten', 'of', 'these', 'mud', 'daubers', 'for', 'a', 'couple', 'hours', 'they', 'never', 'bothered', 'me', 'at', 'all', 'in', 'their', 'own', 'little', 'way', 'they', 'were', 'kinda', 'helping', 'out', 'i', 'suppose', 'theyd', 'build', 'nests', 'in', 'the', 'garage', 'where', 'i', 'workout', 'and', 'aside', 'from', 'almost', 'smacking', 'into', 'each', 'other', 'they', 'never', 'bothered', 'me', 'theyd', 'just', 'go', 'back', 'and', 'forth', 'building', 'their', 'mud', 'tubes', 'and', 'filling', 'them', 'with', 'paralyzed', 'spiders', 'i', 'think', 'they', 'helped', 'with', 'garden', 'pests', 'and', 'we', 'had', 'so', 'many', 'spiders', 'i', 'didnt', 'mind', 'them', 'culling', 'that', 'herd', 'either']\n",
      "0.6257142857142857\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes set up\n",
    "naiveBayes = NaiveBayes(X.to_numpy().flatten(), y.to_numpy(), feature_vectoriser=naiveBayes)\n",
    "\n",
    "accuracy = naiveBayes.crossValidation(28)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "\n",
      "Best Parameters: {'alpha': 0.2}\n",
      "Cross-validation Accuracies: [0.6642857142857143, 0.7285714285714285, 0.7142857142857143, 0.7, 0.6857142857142857, 0.6857142857142857, 0.6714285714285714, 0.6785714285714286, 0.7428571428571429, 0.7214285714285714]\n",
      "Mean Accuracy: 0.6993\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "param_grid_NB_1 = {'alpha': np.arange(0.1, 1.1, 0.1).tolist()}\n",
    "\n",
    "NB = hyperparamaterTunning(X_uni_primary, param_grid_NB_1 ,folds, MultinomialNB(),Y=y_shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SVM Models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "[CV 1/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.838 total time=   3.5s\n",
      "[CV 2/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.788 total time=   2.0s\n",
      "[CV 3/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.804 total time=   2.1s\n",
      "[CV 4/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.816 total time=   2.8s\n",
      "[CV 5/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.821 total time=   1.9s\n",
      "[CV 6/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.788 total time=   2.3s\n",
      "[CV 7/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.827 total time=   3.2s\n",
      "[CV 8/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.788 total time=   2.2s\n",
      "[CV 9/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.827 total time=   2.3s\n",
      "[CV 10/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.799 total time=   1.6s\n",
      "\n",
      "Best Parameters: {'C': 0.7, 'loss': 'hinge', 'max_iter': 5000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "Cross-validation Accuracies: [0.8379888268156425, 0.7877094972067039, 0.8044692737430168, 0.8156424581005587, 0.8212290502793296, 0.7877094972067039, 0.8268156424581006, 0.7877094972067039, 0.8268156424581006, 0.7988826815642458]\n",
      "Mean Accuracy: 0.8095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC, SVC, NuSVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "param_grid_SVC_1 = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"C\": [0.7],\n",
    "     \"loss\": [\"hinge\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [5000]\n",
    "     }]\n",
    "\n",
    "param_grid_SVC_2= {\n",
    "     \"kernel\":[\"poly\"],\n",
    "     \"gamma\": [\"scale\"],\n",
    "     \"degree\":[2,3],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\":[10000]\n",
    "     }\n",
    "\n",
    "SVMModel_tunned_1b = hyperparamaterTunning(X_svd, param_grid_SVC_1, folds, LinearSVC(fit_intercept=True),verbose_val=3,Y=y_shuffled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TO DELETE <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(score, gap):\n",
    "    print(f\"Total Accuracy  :\", round(np.mean(score/gap),2),\"%\")\n",
    "    print(\"-\"*30)\n",
    "    diplay(\"Boston\",2,score, gap)\n",
    "    diplay(\"Canberra\",0,score, gap)\n",
    "    diplay(\"Ottawa\",1,score, gap)\n",
    "    diplay(\"Geneva\",3,score, gap)\n",
    "\n",
    "def diplay(caption,index,score,gap):\n",
    "    print(f\"\\t{caption:<8}: {round(score[index]/gap,2)} % --> {str(score[index]):>3}/150\")\n",
    "\n",
    "def test_score(y_pred_):\n",
    "    gap = len(y_pred_)//4\n",
    "    idx = 0\n",
    "    answ  = [1,3,0,2]\n",
    "    score = np.array([0,0,0,0])\n",
    "    for i in range(len(y_pred_)):\n",
    "        if (i==((idx+1)*gap)):\n",
    "            idx+=1\n",
    "        score[idx]+=int(y_pred_[i] == answ[idx])\n",
    "    report(score,gap)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Export Solution <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy  : 0.7 %\n",
      "------------------------------\n",
      "\tBoston  : 0.69 % --> 104/150\n",
      "\tCanberra: 0.7 % --> 105/150\n",
      "\tOttawa  : 0.61 % -->  91/150\n",
      "\tGeneva  : 0.81 % --> 122/150\n"
     ]
    }
   ],
   "source": [
    "pp_df_test = preprocessing_data(df_test[\"body\"],[])\n",
    "X_test = tfidf_primary.transform(pp_df_test)\n",
    "\n",
    "y_pred = SVMModel_tunned_1b.predict(svd.transform(X_test))\n",
    "\n",
    "test_score(y_pred)\n",
    "if (False):\n",
    "    with open(\"output7.csv\", mode='w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # Write a header (optional, if you want)\n",
    "        writer.writerow([\"id\",\"subreddit\"])\n",
    "        map = [\"Boston\",\"Canberra\",\"Geneva\",\"Ottawa\"]\n",
    "        # Write the features from the array\n",
    "        for i in range(len(y_pred)):\n",
    "            output = y_pred[i]       \n",
    "            writer.writerow([i, map[output]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST (0.71)\n",
    "# Unshuffled though\n",
    "# SVD n_components: 2000\n",
    "# pp_train = pp_df_train_2\n",
    "# Param {'C': 0.5, 'loss': 'hinge', 'max_iter': 5000, 'penalty': 'l2', 'tol': 0.0001}\n",
    "# tfidf: TfidfVectorizer(ngram_range=(1,1),sublinear_tf=True,stop_words=\"english\")\n",
    "# Augm: aug_p=0.3 and 0.5 of dataset\n",
    "# CV = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST (0.71)\n",
    "# shuffled\n",
    "# SVD n_components: 2000\n",
    "# pp_train = pp_df_train_2\n",
    "# Param {'C': 0.7000000000000002, 'loss': 'hinge', 'max_iter': 5000, 'penalty': 'l2', 'tol': 0.0001}\n",
    "# tfidf: TfidfVectorizer(ngram_range=(1,1),sublinear_tf=True,stop_words=\"english\")\n",
    "# Augm: aug_p=0.3 and 0.3 of dataset\n",
    "# CV = 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST (0.71)\n",
    "# shuffled\n",
    "# SVD n_components: 2000\n",
    "# pp_train = pp_df_train_2\n",
    "# Param {'C': 0.6, 'loss': 'hinge', 'max_iter': 5000, 'penalty': 'l2', 'tol': 0.0001}\n",
    "# tfidf: TfidfVectorizer(ngram_range=(1,1),sublinear_tf=True,stop_words=\"english\")\n",
    "# Augm: aug_p=0.3 and 0.5 of dataset\n",
    "# CV = 6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
