{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "# Group 13\n",
    "Mathieu Mailhot - Isabel Lougheed - Frank-Lucas Pantazis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import text\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Parameters<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "folds = 10 \n",
    "\n",
    "# For Linear SVM Model\n",
    "C = 0.7\n",
    "aug_similarity = 0.7\n",
    "aug_quantity = 0\n",
    "stop_word_package = 1 # 0 is nltk and 1 is sklearn stop word package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading Training data\n",
    "df_train = pd.read_csv('train.csv', encoding='utf-8', encoding_errors='ignore') # errors were not pertinent characters\n",
    "df_train[\"subreddit\"] = df_train[\"subreddit\"].map({\"Boston\": 0, \"Canberra\": 1,\"Geneva\":2,\"Ottawa\":3})\n",
    "\n",
    "y = df_train[\"subreddit\"]\n",
    "X = df_train[\"body\"]\n",
    "\n",
    "\n",
    "# Loading Test Data\n",
    "df_test = pd.read_csv('test.csv', encoding='utf-8', encoding_errors='ignore') # errors were not pertinent characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Helper Functions</h2>\n",
    "<h3>Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokenizer2(subreddit_post, stop_word_catalogue=None):\n",
    "    subreddit_post = subreddit_post.lower()\n",
    "    words = word_tokenize(subreddit_post)\n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    \n",
    "    lemmatized_words = []\n",
    "    \n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(words):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        condition = True\n",
    "        if (stop_word_catalogue!=None):\n",
    "            condition = word not in stop_word_catalogue\n",
    "        if(condition):\n",
    "            if word.isalnum():\n",
    "                lemmatized_word = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                #print(word,lemmatized_word)\n",
    "                lemmatized_words.append(lemmatized_word)                   \n",
    "    return lemmatized_words\n",
    "\n",
    "def lemmatize_tokenizer(subreddit_post, stop_word_catalogue=None):\n",
    "    subreddit_post = subreddit_post.lower()\n",
    "    words = word_tokenize(subreddit_post)\n",
    "\n",
    "    lemmatized_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word in words:\n",
    "        if word.isalpha():\n",
    "            lemmatized_word = word_Lemmatized.lemmatize(word)\n",
    "            lemmatized_words.append(lemmatized_word)  \n",
    " \n",
    "    return lemmatized_words\n",
    "\n",
    "def preprocessing_data(X, stop_word_catalogue,version=True):\n",
    "    lemmatized_data = []\n",
    "    for i in range(len(X)):\n",
    "        post = X[i]\n",
    "        if (version):\n",
    "            lemmatized_post = lemmatize_tokenizer(post,stop_word_catalogue)\n",
    "        else:\n",
    "            lemmatized_post = lemmatize_tokenizer2(post,stop_word_catalogue)\n",
    "        lemmatized_data.append(\" \".join(lemmatized_post))\n",
    "    return lemmatized_data\n",
    "\n",
    "def feature_extraction(X,ngram_range,min_df,caption=\"\"):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range,min_df=min_df)\n",
    "    X_disp = vectorizer.fit_transform(X)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(caption,\"features:\", len(feature_names))\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train and Tunning<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# This function does all the tunning for each model\n",
    "def hyperparamaterTunning(X,param, folds, model, verbose_val=1, Y=y):\n",
    "    model_gridSearch = GridSearchCV(model, param_grid=param,cv=folds, verbose=verbose_val) # According to doc the data will be split the same way accross all calls\n",
    "\n",
    "    model_best_clf = model_gridSearch.fit(X,Y)\n",
    "\n",
    "    cv_results = model_gridSearch.cv_results_\n",
    "    \n",
    "    print()\n",
    "\n",
    "    print(f\"Best Parameters: {model_best_clf.best_params_}\")\n",
    "    try:\n",
    "\n",
    "        best_index = model_gridSearch.best_index_\n",
    "\n",
    "        score = []\n",
    "        for fold in range(folds):\n",
    "            score.append(model_gridSearch.cv_results_[f\"split{fold}_test_score\"][best_index].item())\n",
    "\n",
    "        print(f\"Cross-validation Accuracies: {np.round(score,2).tolist()}\")\n",
    "    except:\n",
    "        pass\n",
    "    print(f\"Mean Accuracy: {model_best_clf.best_score_:.4f}\")\n",
    "\n",
    "    return model_best_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Preprocessing </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Word Catalogues\n",
    "nltk_stop_words = stopwords.words('english')\n",
    "sklean_stop_words = list(text.ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Trainning Data\n",
    "pp_df_train_1 = preprocessing_data(X,nltk_stop_words,False)\n",
    "pp_df_train_2 = preprocessing_data(X,sklean_stop_words,False)\n",
    "\n",
    "pp_train = pp_df_train_1\n",
    "stop_words_selected = nltk_stop_words\n",
    "if (stop_word_package==1):\n",
    "    pp_train = pp_df_train_2\n",
    "    stop_words_selected = sklean_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Augmentation<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_synonyms(word, tag = None):\n",
    "    synonyms = set()\n",
    "    for synonym in wn.synsets(word, pos=tag):\n",
    "            for lemma in synonym.lemmas():\n",
    "                if (not lemma.name()[0].isupper()):\n",
    "                    synonyms.add(lemma.name())\n",
    "    return sorted(list(synonyms))\n",
    "\n",
    "def spacyWordnetMapping(tag):\n",
    "    # since lemmatized --> only      \n",
    "    if (tag==\"VERB\"):\n",
    "        return \"v\"\n",
    "    if (tag==\"ADJ\"):\n",
    "        return \"a\"\n",
    "    if (tag==\"ADV\"):\n",
    "        return \"r\"\n",
    "    if (tag==\"NOUN\"):\n",
    "        return \"n\"\n",
    "    return -1\n",
    "# Replace words with their first synonym deterministically\n",
    "def SynonymAug(text, aug_p, random_state = 42):\n",
    "    random.seed(random_state)\n",
    "    newPost = []\n",
    "    # Assumed input is lemmatized\n",
    "    doc = nlp(text)\n",
    "    # Create a list of (word, POS) pairs\n",
    "    numbers = range(len(doc))\n",
    "    aug_idx = random.sample(numbers, int(len(doc)*aug_p))\n",
    "    for token in doc:\n",
    "        word = token.text\n",
    "        tag = spacyWordnetMapping(token.pos_)\n",
    "        if (tag!=-1):\n",
    "            synonyms = get_synonyms(word,tag)\n",
    "            if (len(synonyms)!=0):\n",
    "                newPost.append(synonyms[0])\n",
    "        else:\n",
    "            newPost.append(word)\n",
    "    return \" \".join(newPost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation\n",
    "import nlpaug.augmenter.word as naw\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "\n",
    "# Random Seed \n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "DetectorFactory.seed = 1\n",
    "aug_val = round(1-aug_similarity,1)\n",
    "aug = naw.SynonymAug(aug_src='wordnet', aug_p=aug_val)\n",
    "augmented = []\n",
    "augmented_y = []\n",
    "\n",
    "numbers = range(len(pp_train))\n",
    "picked_numbers = random.sample(numbers, int(len(pp_train)*aug_quantity))\n",
    "\n",
    "for i in picked_numbers:\n",
    "\n",
    "    post = pp_train[i]\n",
    "    if (detect(post)==\"en\"):\n",
    "\n",
    "        #augmented.append(SynonymAug(post,0.3))\n",
    "        augmented.append(aug.augment(post)[0])\n",
    "        augmented_y.append(y[i].item())\n",
    "\n",
    "\n",
    "print(len(augmented))\n",
    "print(len(augmented_y))\n",
    "\n",
    "augmented_df_train = pp_train + augmented\n",
    "augmented_df_y = y.tolist() + augmented_y\n",
    "\n",
    "X_arr = np.array(augmented_df_train)\n",
    "Y_arr = np.array(augmented_df_y)\n",
    "\n",
    "#X_shuffled = pp_train\n",
    "#y_shuffled = y.tolist()\n",
    "if (aug_quantity!=0):\n",
    "    indices = np.arange(len(X_arr))\n",
    "#print(indices)\n",
    "    np.random.shuffle(indices)\n",
    "    X_shuffled = X_arr[indices]\n",
    "    y_shuffled = Y_arr[indices]\n",
    "else:\n",
    "    X_shuffled = pp_train\n",
    "    y_shuffled = y.tolist()\n",
    "# Reshuffling data --> due to addition of data augmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train<h2>\n",
    "<h3>Feature Extraction<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9471\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tfidf_primary = TfidfVectorizer(ngram_range=(1,1),sublinear_tf=True,stop_words=stop_words_selected)\n",
    "X_uni_primary = tfidf_primary.fit_transform(X_shuffled)\n",
    "print(len(tfidf_primary.get_feature_names_out()))\n",
    "\n",
    "naiveBayes = CountVectorizer(max_features=3000, ngram_range=(1, 1), stop_words=\"english\")\n",
    "X_naive_bayes = naiveBayes.fit_transform(X).toarray()\n",
    "print(len(naiveBayes.get_feature_names_out()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Reduction</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n"
     ]
    }
   ],
   "source": [
    "# TRUCATION\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# Step 1: Fit initial SVD with many components\n",
    "svd = TruncatedSVD(n_components=2000,random_state=42) \n",
    "X_svd = svd.fit_transform(X_uni_primary)\n",
    "print(len(X_svd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Naive Bayes Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "class NaiveBayes:\n",
    "    def __init__(self, x_all, y_all, feature_vectoriser):\n",
    "        self.x_all = self.clean_text_data(x_all) # Make lists of strings\n",
    "        self.y_all = y_all\n",
    "        self.feature_vectoriser = feature_vectoriser\n",
    "\n",
    "        self.folds_features_probability = 0 # array of dict\n",
    "        self.folds_accuracy = 0\n",
    "        self.avg_accuracy = 0\n",
    "\n",
    "    \n",
    "    def calc_probability(self, x, y): # Train/Fit # Mathieu\n",
    "        # Create an empty dictionnary with the 3000 most common words for each subreddit.\n",
    "        features_probability_boston = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "        features_probability_canberra = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "        features_probability_geneva = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "        features_probability_ottawa = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "\n",
    "        # Initialize the count for the total number of text from each subreddit\n",
    "        count_boston = 0\n",
    "        count_canberra = 0\n",
    "        count_geneva = 0\n",
    "        count_ottowa = 0\n",
    "\n",
    "        # Add 1 to the word in the dictionnary when the word is present in the text\n",
    "        for i in range(y.shape[0]):\n",
    "            if y[i] == 0:\n",
    "                count_boston += 1\n",
    "                self.add_probability(features_probability_boston, x[i])\n",
    "            if y[i] == 1:\n",
    "                count_canberra += 1\n",
    "                self.add_probability(features_probability_canberra, x[i])\n",
    "            if y[i] == 2:\n",
    "                count_geneva += 1\n",
    "                self.add_probability(features_probability_geneva, x[i])\n",
    "            else:\n",
    "                count_ottowa += 1\n",
    "                self.add_probability(features_probability_ottawa, x[i])\n",
    "\n",
    "        # Add the total count of each city to a variable called \"city_count\" and the probability of each city in a variable called \"city_probability\" in each one of the dictionary\n",
    "        features_probability_boston[\"city_count\"] = count_boston\n",
    "        features_probability_boston[\"city_probability\"] = count_boston / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "        features_probability_canberra[\"city_count\"] = count_canberra\n",
    "        features_probability_canberra[\"city_probability\"] = count_canberra / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "        features_probability_geneva[\"city_count\"] = count_geneva\n",
    "        features_probability_geneva[\"city_probability\"] = count_geneva / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "        features_probability_ottawa[\"city_count\"] = count_ottowa\n",
    "        features_probability_ottawa[\"city_probability\"] = count_ottowa / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "                \n",
    "        return features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa # return all dictionaries\n",
    "\n",
    "    def clean_text_data(self, x): # Helper function to make a list of lists of words \n",
    "        # Take text remove all capitalized letters, removed special characters and make an array of words.\n",
    "        cleaned_data = [\n",
    "            re.sub(r'[^a-z0-9\\s]', '', text.lower()).split()\n",
    "            for text in x\n",
    "        ]\n",
    "        print(\"This is the cleaned data\", cleaned_data[0])\n",
    "        return cleaned_data # return a list of lists of words (better to use lists for this since numpy is mostly for numerical values)\n",
    "    \n",
    "    def add_probability(self, city_dict, x): # Helper function to update probabilities given a dict and a list of words\n",
    "        for word in set(x): # Creates a set from words(unique elements)\n",
    "            if word in city_dict:\n",
    "                city_dict[word] += 1\n",
    "                \n",
    "    \n",
    "    def predict(self, features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa, x_i): # (Is x_i in formula equal to 1?)\n",
    "        # Initialize probabilities for each subreddit\n",
    "        prob_boston = features_probability_boston[\"city_probability\"]\n",
    "        prob_canberra = features_probability_canberra[\"city_probability\"]\n",
    "        prob_geneva = features_probability_geneva[\"city_probability\"]\n",
    "        prob_ottowa = features_probability_ottawa[\"city_probability\"]\n",
    "\n",
    "        for word in x_i:\n",
    "            if word in features_probability_boston: # All have the same most common words\n",
    "                # Laplace smoothing\n",
    "                prob_boston = prob_boston * ((features_probability_boston[word] + 1) / (features_probability_boston[\"city_count\"] + 2))\n",
    "                prob_canberra = prob_canberra * ((features_probability_canberra[word] + 1) / (features_probability_canberra[\"city_count\"] + 2))\n",
    "                prob_geneva = prob_geneva * ((features_probability_geneva[word] + 1) / (features_probability_geneva[\"city_count\"] + 2))\n",
    "                prob_ottowa = prob_ottowa * ((features_probability_ottawa[word] + 1) / (features_probability_ottawa[\"city_count\"] + 2))\n",
    "        \n",
    "        probabilities = np.array([prob_boston, prob_canberra, prob_geneva, prob_ottowa])\n",
    "        \n",
    "        return np.argmax(probabilities)\n",
    "\n",
    "    def accu_eval(self, x, y): # Issy\n",
    "        # Validation\n",
    "        # Returns Accuracy = 1 - Error\n",
    "\n",
    "        num_correct_labels = 0\n",
    "\n",
    "        # Get probabilities / train model\n",
    "        features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa = self.calc_probability(x,y)\n",
    "\n",
    "        # Predict\n",
    "        for i in range(len(x)):\n",
    "            predicted_label = self.predict(features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa, x[i])\n",
    "            if predicted_label == y[i]:\n",
    "                num_correct_labels += 1\n",
    "\n",
    "        # Get accuracy\n",
    "        accuracy = num_correct_labels/len(y)\n",
    "        return accuracy\n",
    "\n",
    "    def crossValidation(self, k): # Issy (PS: I think we are allowed to use the method from sklearn)\n",
    "        kf = KFold(n_splits=k, shuffle=True, random_state=9)\n",
    "        accuracies = []\n",
    "\n",
    "        # Split into train and validation sets\n",
    "        for train_indices, val_indices in kf.split(self.x_all):\n",
    "            x_train = [self.x_all[i] for i in train_indices] # separate x into training subset\n",
    "            x_val = [self.x_all[i] for i in val_indices] # separate x into validating subset\n",
    "\n",
    "            y_train = [self.y_all[i] for i in train_indices] # separate y into training subset\n",
    "            y_val = [self.y_all[i] for i in val_indices] # separate y into validating subset\n",
    "\n",
    "            \n",
    "            # For each set, get probabilities / train with training set\n",
    "            features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa = self.calc_probability(x_train,np.array(y_train))\n",
    "\n",
    "            # check with validation subset\n",
    "            num_correct_labels = 0\n",
    "            for i in range(len(y_val)):\n",
    "                predicted_label = self.predict(features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa, x_val[i])\n",
    "                if predicted_label == y_val[i]:\n",
    "                    num_correct_labels += 1\n",
    "        \n",
    "            # calculate accuracy\n",
    "            accuracy = num_correct_labels / len(y_val)\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "        self.folds_accuracy = accuracies\n",
    "        print(np.round(self.folds_accuracy,2).tolist())\n",
    "        self.avg_accuracy = np.round(np.mean(accuracies),4)\n",
    "\n",
    "        return self.avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the cleaned data ['i', 'had', 'to', 'put', 'in', 'a', 'drain', 'wellfrench', 'drain', 'and', 'the', 'ground', 'about', '6', 'inches', 'down', 'was', 'all', 'mud', 'and', 'clay', 'i', 'was', 'ass', 'over', 'end', 'in', 'this', 'hole', 'scooping', 'clay', 'mud', 'and', 'was', 'joined', 'by', 'probably', 'ten', 'of', 'these', 'mud', 'daubers', 'for', 'a', 'couple', 'hours', 'they', 'never', 'bothered', 'me', 'at', 'all', 'in', 'their', 'own', 'little', 'way', 'they', 'were', 'kinda', 'helping', 'out', 'i', 'suppose', 'theyd', 'build', 'nests', 'in', 'the', 'garage', 'where', 'i', 'workout', 'and', 'aside', 'from', 'almost', 'smacking', 'into', 'each', 'other', 'they', 'never', 'bothered', 'me', 'theyd', 'just', 'go', 'back', 'and', 'forth', 'building', 'their', 'mud', 'tubes', 'and', 'filling', 'them', 'with', 'paralyzed', 'spiders', 'i', 'think', 'they', 'helped', 'with', 'garden', 'pests', 'and', 'we', 'had', 'so', 'many', 'spiders', 'i', 'didnt', 'mind', 'them', 'culling', 'that', 'herd', 'either']\n",
      "[0.58, 0.64, 0.66, 0.61, 0.63, 0.71, 0.54, 0.61, 0.59, 0.69]\n",
      "0.6257\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes set up\n",
    "customNaiveBayes = NaiveBayes(X.to_numpy().flatten(), y.to_numpy(), feature_vectoriser=naiveBayes)\n",
    "\n",
    "accuracy = customNaiveBayes.crossValidation(folds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "\n",
      "Best Parameters: {'alpha': 0.1}\n",
      "Cross-validation Accuracies: [0.67, 0.66, 0.78, 0.69, 0.67, 0.67, 0.66, 0.74, 0.74, 0.74]\n",
      "Mean Accuracy: 0.7014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "param_grid_NB_1 = {'alpha': np.arange(0.1, 1.1, 0.1).tolist()}\n",
    "\n",
    "NB = hyperparamaterTunning(X_uni_primary, param_grid_NB_1 ,folds, MultinomialNB(),Y=y_shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SVM Models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "[CV 1/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.679 total time=   0.4s\n",
      "[CV 2/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.657 total time=   0.5s\n",
      "[CV 3/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.764 total time=   0.5s\n",
      "[CV 4/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.750 total time=   0.6s\n",
      "[CV 5/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.736 total time=   0.4s\n",
      "[CV 6/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.657 total time=   1.2s\n",
      "[CV 7/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.686 total time=   1.6s\n",
      "[CV 8/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.729 total time=   1.2s\n",
      "[CV 9/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.714 total time=   1.3s\n",
      "[CV 10/10] END C=0.7, loss=hinge, max_iter=5000, penalty=l2, tol=0.0001;, score=0.721 total time=   1.7s\n",
      "\n",
      "Best Parameters: {'C': 0.7, 'loss': 'hinge', 'max_iter': 5000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "Cross-validation Accuracies: [0.68, 0.66, 0.76, 0.75, 0.74, 0.66, 0.69, 0.73, 0.71, 0.72]\n",
      "Mean Accuracy: 0.7093\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC, SVC, NuSVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "param_grid_SVC_1 = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"C\": [C],\n",
    "     \"loss\": [\"hinge\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [5000]\n",
    "     }]\n",
    "\n",
    "\n",
    "SVMModel_tunned_1b = hyperparamaterTunning(X_svd, param_grid_SVC_1, folds, LinearSVC(fit_intercept=True),verbose_val=3,Y=y_shuffled)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
