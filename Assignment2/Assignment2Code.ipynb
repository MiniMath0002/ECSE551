{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "# Group 13\n",
    "Mathieu Mailhot - Isabel Lougheed - Frank-Lucas Pantazis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "## Checkout link: https://www.analyticsvidhya.com/blog/2021/11/a-guide-to-building-an-end-to-end-multiclass-text-classification-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   body\n",
      "0     I had to put in a drain well/french drain, and...\n",
      "1     I've worked with James from Prova accountants ...\n",
      "2     https://lebonmelange.com.au/ is a Gungahlin ca...\n",
      "3     What I love about Canberra are the town planni...\n",
      "4     Canberra has a bigger issue with strata. Rates...\n",
      "...                                                 ...\n",
      "1395  Take the train to La pleine and walk to joncti...\n",
      "1396  IIL alumni here!\\n\\nGeneva private schools are...\n",
      "1397  I'm really sorry to hear about your bad experi...\n",
      "1398  They quite easy to handle. Typically, they don...\n",
      "1399  **Specialization is authentic.** You don?t go ...\n",
      "\n",
      "[1400 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "folds = 30 # between 5 and 10 # best value at the moment when folds = 30\n",
    "\n",
    "# Loading Training data\n",
    "df_train = pd.read_csv('train.csv', encoding='utf-8', encoding_errors='ignore') # errors were not pertinent characters\n",
    "df_train[\"subreddit\"] = df_train[\"subreddit\"].map({\"Boston\": 0, \"Canberra\": 1,\"Geneva\":2,\"Ottawa\":3})\n",
    "\n",
    "y = df_train[\"subreddit\"]\n",
    "X = df_train.drop(\"subreddit\",axis=1)\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Loading Test Data\n",
    "df_test = pd.read_csv('test.csv', encoding='utf-8', encoding_errors='ignore') # errors were not pertinent characters\n",
    "#X_test = df_test[\"body\"] # Not what we should do with the ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Stop Word Size: 318\n",
      "Custom Stop Word Size: 351\n"
     ]
    }
   ],
   "source": [
    "# Test different stop word libraries\n",
    "# Checkout: https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a/\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english',ngram_range=(1, 1),min_df=2)\n",
    "\n",
    "X_disp = vectorizer.fit_transform(df_train[\"body\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "Boston_counts = X_disp[y == 0].sum(axis=0).A1 # Sum occurrences for class 'Boston'\n",
    "Canberra_counts = X_disp[y == 1].sum(axis=0).A1 # Sum occurrences for class 'Canberra'\n",
    "Geneva_counts = X_disp[y == 2].sum(axis=0).A1 # Sum occurrences for class 'Geneva'\n",
    "Ottawa_counts = X_disp[y == 3].sum(axis=0).A1 # Sum occurrences for class 'Ottawa'\n",
    "\n",
    "stop_words = []\n",
    "for i in range(len(feature_names)):\n",
    "    k = [Boston_counts[i].item(),Canberra_counts[i].item(),Geneva_counts[i].item(),Ottawa_counts[i].item()]\n",
    "    a = sum([Boston_counts[i].item(),Canberra_counts[i].item(),Geneva_counts[i].item(),Ottawa_counts[i].item()])\n",
    "    crit  = abs(min(k) - max(k))/ max(k)\n",
    "    if(crit<0.20):\n",
    "        stop_words.append(feature_names[i])\n",
    "print(\"Initial Stop Word Size:\",len(text.ENGLISH_STOP_WORDS))\n",
    "stop_words_custom = list(text.ENGLISH_STOP_WORDS.union(stop_words))\n",
    "stop_words_custom.append(\"ve\")\n",
    "stop_words_custom.append(\"don\")\n",
    "print(\"Custom Stop Word Size:\",len(stop_words_custom))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram (size: 5328)\n",
      "['00' '000' '01' ... 'zoom' 'zucchini' 'zurich']\n",
      "Unigram & Bigram (size: 2249)\n",
      "Bigram (size: 2253)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import NMF\n",
    "# Observations\n",
    "# - bigram -> worse performance\n",
    "# - sublinear_tf -> seems to improve accuracy\n",
    "# - decreasing max_features -> seems to decrease accuracy (feature reduction)\n",
    "\n",
    "# TODO\n",
    "# - Create custom stop word list since default one might not be suited for our case according to documentation: https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n",
    "# - explore different ways to extract features from text data\n",
    "\n",
    "# Instantiate Vectorizer\n",
    "tfidf_uni = TfidfVectorizer(ngram_range=(1, 1), sublinear_tf=True, min_df=2, stop_words=stop_words_custom)\n",
    "tfidf_uni_bi = TfidfVectorizer(ngram_range=(1, 2),sublinear_tf=True, min_df=5, stop_words=stop_words_custom)\n",
    "tfidf_bi = TfidfVectorizer(ngram_range=(2, 2), sublinear_tf=True,min_df=2, stop_words=stop_words_custom)\n",
    "naiveBayes_uni = CountVectorizer(max_features=3000, ngram_range=(1, 1), stop_words=stop_words_custom)\n",
    "\n",
    "\n",
    "# Fit Vectorizer from data\n",
    "X_uni = tfidf_uni.fit_transform(df_train[\"body\"]).toarray()\n",
    "\n",
    "X_uni_bi = tfidf_uni_bi.fit_transform(df_train[\"body\"]).toarray()\n",
    "X_bi = tfidf_bi.fit_transform(df_train[\"body\"]).toarray()\n",
    "X_naive_bayes = naiveBayes_uni.fit_transform(df_train[\"body\"]).toarray()\n",
    "\n",
    "X_test = tfidf_uni.transform(df_test[\"body\"])\n",
    "\n",
    "\n",
    "print(\"Unigram\", \"(size:\",str(len(tfidf_uni.get_feature_names_out()))+\")\")\n",
    "print(tfidf_uni.get_feature_names_out())\n",
    "#print(tfidf_uni.get_feature_names_out())\n",
    "print(\"Unigram & Bigram\", \"(size:\",str(len(tfidf_uni_bi.get_feature_names_out()))+\")\")\n",
    "#print(tfidf_uni_bi.get_feature_names_out())\n",
    "print(\"Bigram\", \"(size:\",str(len(tfidf_bi.get_feature_names_out()))+\")\")\n",
    "#print(tfidf_bi.get_feature_names_out())\n",
    "\n",
    "# LOOK INTO NMF AND WHAT IT CAN DO TO HELP US\n",
    "#nmf = NMF(100).fit(X_uni)\n",
    "\n",
    "#for topic_idx, topic in enumerate(nmf.components_):\n",
    "#    top_features_ind = topic.argsort()[-10:]\n",
    "#    top_features = tfidf_uni.get_feature_names_out()[top_features_ind]\n",
    "#    print(top_features)\n",
    "    \n",
    "# To get a better idea of the extracted features\n",
    "with open(\"features.csv\", mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write a header (optional, if you want)\n",
    "    writer.writerow([\"Feature Name\"])\n",
    "    # Write the features from the array\n",
    "    for feature in tfidf_uni.get_feature_names_out():\n",
    "        writer.writerow([feature])\n",
    "\n",
    "with open(\"featuresNaiveBayes.csv\", mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write a header (optional, if you want)\n",
    "    writer.writerow([\"Feature Name\"])\n",
    "    # Write the features from the array\n",
    "    for feature in naiveBayes_uni.get_feature_names_out():\n",
    "        writer.writerow([feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draft of Feature Visualizer\n",
    "# Maybe should put all of it in an excel and then display it?\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english',ngram_range=(1, 1),min_df=10)\n",
    "\n",
    "X_disp = vectorizer.fit_transform(df_train[\"body\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "Boston_counts = X_disp[y == 0].sum(axis=0).A1 # Sum occurrences for class 'Boston'\n",
    "Canberra_counts = X_disp[y == 1].sum(axis=0).A1 # Sum occurrences for class 'Canberra'\n",
    "Geneva_counts = X_disp[y == 2].sum(axis=0).A1 # Sum occurrences for class 'Geneva'\n",
    "Ottawa_counts = X_disp[y == 3].sum(axis=0).A1 # Sum occurrences for class 'Ottawa'\n",
    "\n",
    "\n",
    "header = [\"features\",\"Boston\",\"Canberra\",\"Geneva\",\"Ottawa\"]\n",
    "table = []\n",
    "for i in range(len(feature_names)):\n",
    "    k = [Boston_counts[i].item(),Canberra_counts[i].item(),Geneva_counts[i].item(),Ottawa_counts[i].item()]\n",
    "    a = sum([Boston_counts[i].item(),Canberra_counts[i].item(),Geneva_counts[i].item(),Ottawa_counts[i].item()])\n",
    "    crit  = abs(min(k) - max(k))/ max(k)\n",
    "    if (crit>0.20):\n",
    "        table.append([feature_names[i],Boston_counts[i].item(),Canberra_counts[i].item(),Geneva_counts[i].item(),Ottawa_counts[i].item(),a])\n",
    "    \n",
    "\n",
    "\n",
    "if (True):\n",
    "    with open(\"featureVisualiser4.csv\", mode='w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # Write a header (optional, if you want)\n",
    "        writer.writerow(header)\n",
    "        # Write the features from the array\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "    print(table)\n",
    "    \n",
    "# Plot a grouped bar chart\n",
    "# y_pos = np.arange(len(feature_names))*2 # Word indices\n",
    "# width = 0.4  # Bar width\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# ax.barh(y_pos + 3*width/2, Boston_counts, width, label=\"Boston\", color='red')\n",
    "# ax.barh(y_pos + width/2, Canberra_counts, width, label=\"Canberra\", color='orange')\n",
    "# ax.barh(y_pos - width/2, Geneva_counts, width, label=\"Geneva\", color='blue')\n",
    "# ax.barh(y_pos - 3*width/2, Ottawa_counts, width, label=\"Ottawa\", color='green')\n",
    "# \n",
    "# # Formatting\n",
    "# ax.set_yticks(y_pos, labels=feature_names)\n",
    "# \n",
    "# ax.set_xlabel(\"Word Count\")\n",
    "# ax.set_title(\"Feature Appearance in Each Class\")\n",
    "# ax.legend()\n",
    "# \n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Helper Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# This function does all the tunning for each model\n",
    "def hyperparamaterTunning(X, param, folds, model, verbose_val=1):\n",
    "    \n",
    "    model_gridSearch = GridSearchCV(model, param_grid=param,cv=folds, verbose=verbose_val) # According to doc the data will be split the same way accross all calls\n",
    "\n",
    "    model_best_clf = model_gridSearch.fit(X,y)\n",
    "\n",
    "    cv_results = model_gridSearch.cv_results_\n",
    "\n",
    "    \n",
    "    print()\n",
    "\n",
    "    print(f\"Best Parameters: {model_best_clf.best_params_}\")\n",
    "    try:\n",
    "\n",
    "        best_index = model_gridSearch.best_index_\n",
    "\n",
    "        score = []\n",
    "        for fold in range(folds):\n",
    "            score.append(model_gridSearch.cv_results_[f\"split{fold}_test_score\"][best_index].item())\n",
    "\n",
    "        print(f\"Cross-validation Accuracies: {score}\")\n",
    "    except:\n",
    "        pass\n",
    "    print(f\"Mean Accuracy: {model_best_clf.best_score_:.4f}\")\n",
    "\n",
    "    return model_best_clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Naive Bayes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, x_all, y_all, feature_vectoriser):\n",
    "        self.x_all = self.clean_text_data(x_all) # Make lists of strings\n",
    "        self.y_all = y_all\n",
    "        self.feature_vectoriser = feature_vectoriser\n",
    "\n",
    "        self.folds_features_probability = 0 # array of dict\n",
    "        self.folds_accuracy = 0\n",
    "        self.avg_accuracy = 0\n",
    "\n",
    "    \n",
    "    def calc_probability(self, x, y): # Train/Fit # Mathieu\n",
    "        # Create an empty dictionnary with the 3000 most common words for each subreddit.\n",
    "        features_probability_boston = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "        features_probability_canberra = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "        features_probability_geneva = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "        features_probability_ottawa = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "\n",
    "        # Initialize the count for the total number of text from each subreddit\n",
    "        count_boston = 0\n",
    "        count_canberra = 0\n",
    "        count_geneva = 0\n",
    "        count_ottowa = 0\n",
    "\n",
    "        # Add 1 to the word in the dictionnary when the word is present in the text\n",
    "        for i in range(y.shape[0]):\n",
    "            if y[i] == 0:\n",
    "                count_boston += 1\n",
    "                self.add_probability(features_probability_boston, x[i])\n",
    "            if y[i] == 1:\n",
    "                count_canberra += 1\n",
    "                self.add_probability(features_probability_canberra, x[i])\n",
    "            if y[i] == 2:\n",
    "                count_geneva += 1\n",
    "                self.add_probability(features_probability_geneva, x[i])\n",
    "            else:\n",
    "                count_ottowa += 1\n",
    "                self.add_probability(features_probability_ottawa, x[i])\n",
    "\n",
    "        # Add the total count of each city to a variable called \"city_count\" and the probability of each city in a variable called \"city_probability\" in each one of the dictionary\n",
    "        features_probability_boston[\"city_count\"] = count_boston\n",
    "        features_probability_boston[\"city_probability\"] = count_boston / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "        features_probability_canberra[\"city_count\"] = count_canberra\n",
    "        features_probability_canberra[\"city_probability\"] = count_canberra / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "        features_probability_geneva[\"city_count\"] = count_geneva\n",
    "        features_probability_geneva[\"city_probability\"] = count_geneva / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "        features_probability_ottawa[\"city_count\"] = count_ottowa\n",
    "        features_probability_ottawa[\"city_probability\"] = count_ottowa / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "                \n",
    "        return features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa # return all dictionaries\n",
    "\n",
    "    def clean_text_data(self, x): # Helper function to make a list of lists of words \n",
    "        # Take text remove all capitalized letters, removed special characters and make an array of words.\n",
    "        cleaned_data = [\n",
    "            re.sub(r'[^a-z0-9\\s]', '', text.lower()).split()\n",
    "            for text in x\n",
    "        ]\n",
    "        print(\"This is the cleaned data\", cleaned_data[0])\n",
    "        return cleaned_data # return a list of lists of words (better to use lists for this since numpy is mostly for numerical values)\n",
    "    \n",
    "    def add_probability(self, city_dict, x): # Helper function to update probabilities given a dict and a list of words\n",
    "        for word in set(x): # Creates a set from words(unique elements)\n",
    "            if word in city_dict:\n",
    "                city_dict[word] += 1\n",
    "                \n",
    "    \n",
    "    def predict(self, features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa, x_i): # (Is x_i in formula equal to 1?)\n",
    "        # Initialize probabilities for each subreddit\n",
    "        prob_boston = features_probability_boston[\"city_probability\"]\n",
    "        prob_canberra = features_probability_canberra[\"city_probability\"]\n",
    "        prob_geneva = features_probability_geneva[\"city_probability\"]\n",
    "        prob_ottowa = features_probability_ottawa[\"city_probability\"]\n",
    "\n",
    "        for word in x_i:\n",
    "            if word in features_probability_boston: # All have the same most common words\n",
    "                # Laplace smoothing\n",
    "                prob_boston = prob_boston * ((features_probability_boston[word] + 1) / (features_probability_boston[\"city_count\"] + 2))\n",
    "                prob_canberra = prob_canberra * ((features_probability_canberra[word] + 1) / (features_probability_canberra[\"city_count\"] + 2))\n",
    "                prob_geneva = prob_geneva * ((features_probability_geneva[word] + 1) / (features_probability_geneva[\"city_count\"] + 2))\n",
    "                prob_ottowa = prob_ottowa * ((features_probability_ottawa[word] + 1) / (features_probability_ottawa[\"city_count\"] + 2))\n",
    "        \n",
    "        probabilities = np.array([prob_boston, prob_canberra, prob_geneva, prob_ottowa])\n",
    "        \n",
    "        return np.argmax(probabilities)\n",
    "\n",
    "    def accu_eval(self, x, y): # Issy\n",
    "        # Validation\n",
    "        # Returns Accuracy = 1 - Error\n",
    "\n",
    "        num_correct_labels = 0\n",
    "\n",
    "        # Get probabilities / train model\n",
    "        features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa = self.calc_probability(x,y)\n",
    "\n",
    "        # Predict\n",
    "        for i in range(len(x)):\n",
    "            predicted_label = self.predict(features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa, x[i])\n",
    "            if predicted_label == y[i]:\n",
    "                num_correct_labels += 1\n",
    "\n",
    "        # Get accuracy\n",
    "        accuracy = num_correct_labels/len(y)\n",
    "        return accuracy\n",
    "\n",
    "    def crossValidation(self, k): # Issy (PS: I think we are allowed to use the method from sklearn)\n",
    "        kf = KFold(n_splits=k, shuffle=True, random_state=9)\n",
    "        accuracies = []\n",
    "\n",
    "        # Split into train and validation sets\n",
    "        for train_indices, val_indices in kf.split(self.x_all):\n",
    "            x_train = [self.x_all[i] for i in train_indices] # separate x into training subset\n",
    "            x_val = [self.x_all[i] for i in val_indices] # separate x into validating subset\n",
    "\n",
    "            y_train = [self.y_all[i] for i in train_indices] # separate y into training subset\n",
    "            y_val = [self.y_all[i] for i in val_indices] # separate y into validating subset\n",
    "\n",
    "            \n",
    "            # For each set, get probabilities / train with training set\n",
    "            features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa = self.calc_probability(x_train,np.array(y_train))\n",
    "\n",
    "            # check with validation subset\n",
    "            num_correct_labels = 0\n",
    "            for i in range(len(y_val)):\n",
    "                predicted_label = self.predict(features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa, x_val[i])\n",
    "                if predicted_label == y_val[i]:\n",
    "                    num_correct_labels += 1\n",
    "        \n",
    "            # calculate accuracy\n",
    "            accuracy = num_correct_labels / len(y_val)\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "        self.folds_accuracy = accuracies\n",
    "        self.avg_accuracy = np.mean(accuracies)\n",
    "        return self.avg_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the cleaned data ['i', 'had', 'to', 'put', 'in', 'a', 'drain', 'wellfrench', 'drain', 'and', 'the', 'ground', 'about', '6', 'inches', 'down', 'was', 'all', 'mud', 'and', 'clay', 'i', 'was', 'ass', 'over', 'end', 'in', 'this', 'hole', 'scooping', 'clay', 'mud', 'and', 'was', 'joined', 'by', 'probably', 'ten', 'of', 'these', 'mud', 'daubers', 'for', 'a', 'couple', 'hours', 'they', 'never', 'bothered', 'me', 'at', 'all', 'in', 'their', 'own', 'little', 'way', 'they', 'were', 'kinda', 'helping', 'out', 'i', 'suppose', 'theyd', 'build', 'nests', 'in', 'the', 'garage', 'where', 'i', 'workout', 'and', 'aside', 'from', 'almost', 'smacking', 'into', 'each', 'other', 'they', 'never', 'bothered', 'me', 'theyd', 'just', 'go', 'back', 'and', 'forth', 'building', 'their', 'mud', 'tubes', 'and', 'filling', 'them', 'with', 'paralyzed', 'spiders', 'i', 'think', 'they', 'helped', 'with', 'garden', 'pests', 'and', 'we', 'had', 'so', 'many', 'spiders', 'i', 'didnt', 'mind', 'them', 'culling', 'that', 'herd', 'either']\n",
      "0.6235714285714286\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes set up\n",
    "naiveBayes = NaiveBayes(X.to_numpy().flatten(), y.to_numpy(), feature_vectoriser=naiveBayes_uni)\n",
    "\n",
    "accuracy = naiveBayes.crossValidation(28)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 11 candidates, totalling 330 fits\n",
      "\n",
      "Best Parameters: {'alpha': np.float64(1.01)}\n",
      "Cross-validation Accuracies: [0.7021276595744681, 0.7021276595744681, 0.7021276595744681, 0.7021276595744681, 0.723404255319149, 0.8085106382978723, 0.7446808510638298, 0.7446808510638298, 0.7872340425531915, 0.6382978723404256, 0.6595744680851063, 0.8297872340425532, 0.7446808510638298, 0.8297872340425532, 0.7659574468085106, 0.7446808510638298, 0.5957446808510638, 0.7021276595744681, 0.7659574468085106, 0.7872340425531915, 0.6956521739130435, 0.6956521739130435, 0.6304347826086957, 0.6956521739130435, 0.8695652173913043, 0.7391304347826086, 0.7608695652173914, 0.6086956521739131, 0.8043478260869565, 0.717391304347826]\n",
      "Mean Accuracy: 0.7299\n"
     ]
    }
   ],
   "source": [
    "# Compare with Naive Bayes model from sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "param_grid_NB_1 = {'alpha': np.arange(0.01, 1.11, 0.1)}\n",
    "\n",
    "NB = hyperparamaterTunning(X_uni, param_grid_NB_1 ,folds, MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Logistic Regression Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 80 folds for each of 6 candidates, totalling 480 fits\n",
      "\n",
      "Best Parameters: {'l1_ratio': np.float64(0.0), 'max_iter': 1000, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "Cross-validation Accuracies: [0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.7222222222222222, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.8333333333333334, 0.6111111111111112, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.6111111111111112, 0.8888888888888888, 0.8888888888888888, 0.6111111111111112, 0.5555555555555556, 0.8888888888888888, 0.7777777777777778, 0.7058823529411765, 0.7647058823529411, 0.7058823529411765, 0.5294117647058824, 0.7058823529411765, 0.5882352941176471, 0.6470588235294118, 0.9411764705882353, 0.7058823529411765, 0.7647058823529411, 0.8823529411764706, 0.6470588235294118, 0.6470588235294118, 0.7058823529411765, 0.7647058823529411, 0.7647058823529411, 0.7058823529411765, 0.6470588235294118, 0.5294117647058824, 0.8235294117647058, 0.6470588235294118, 0.7058823529411765, 0.8235294117647058, 0.7647058823529411, 0.5882352941176471, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.6470588235294118, 0.6470588235294118, 0.7647058823529411, 0.5882352941176471, 0.7058823529411765, 0.7058823529411765, 0.7647058823529411, 0.8235294117647058, 0.8235294117647058, 0.6470588235294118, 0.7647058823529411, 0.7647058823529411]\n",
      "Mean Accuracy: 0.7283\n",
      "Fitting 80 folds for each of 6 candidates, totalling 480 fits\n",
      "\n",
      "Best Parameters: {'l1_ratio': np.float64(0.0), 'max_iter': 1000, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "Cross-validation Accuracies: [0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.7777777777777778, 0.6666666666666666, 0.6111111111111112, 0.7222222222222222, 0.8333333333333334, 0.6111111111111112, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.6666666666666666, 0.6111111111111112, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.8888888888888888, 0.9444444444444444, 0.6111111111111112, 0.6111111111111112, 0.9444444444444444, 0.7222222222222222, 0.5882352941176471, 0.7647058823529411, 0.7058823529411765, 0.5294117647058824, 0.6470588235294118, 0.5882352941176471, 0.6470588235294118, 0.9411764705882353, 0.7058823529411765, 0.7647058823529411, 0.8823529411764706, 0.6470588235294118, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.7647058823529411, 0.6470588235294118, 0.7058823529411765, 0.4117647058823529, 0.7647058823529411, 0.6470588235294118, 0.8235294117647058, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.8235294117647058, 0.5882352941176471, 0.7058823529411765, 0.7647058823529411, 0.7647058823529411, 0.8235294117647058, 0.9411764705882353, 0.6470588235294118, 0.7647058823529411, 0.5882352941176471]\n",
      "Mean Accuracy: 0.7304\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_logModel_1 = [\n",
    "    {\"penalty\":[\"elasticnet\"],\n",
    "     \"l1_ratio\": np.arange(0, 1.2, 0.2), # 0 is only l2 penalty, 1 is only l1 penalty\n",
    "     \"solver\":[\"saga\"],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "logModel_tunned_1a = hyperparamaterTunning(X_uni, param_grid_logModel_1, folds, LogisticRegression(fit_intercept=True))\n",
    "logModel_tunned_1b = hyperparamaterTunning(X_uni_bi, param_grid_logModel_1, folds, LogisticRegression(fit_intercept=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 80 folds for each of 12 candidates, totalling 960 fits\n",
      "\n",
      "Best Parameters: {'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs', 'tol': 0.0001}\n",
      "Cross-validation Accuracies: [0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.7777777777777778, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.8333333333333334, 0.6111111111111112, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.6111111111111112, 0.8888888888888888, 0.8888888888888888, 0.6111111111111112, 0.5555555555555556, 0.8888888888888888, 0.7777777777777778, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.5294117647058824, 0.7058823529411765, 0.5882352941176471, 0.6470588235294118, 0.9411764705882353, 0.7058823529411765, 0.7647058823529411, 0.8823529411764706, 0.6470588235294118, 0.6470588235294118, 0.7058823529411765, 0.7647058823529411, 0.7647058823529411, 0.7058823529411765, 0.6470588235294118, 0.5294117647058824, 0.8235294117647058, 0.6470588235294118, 0.7058823529411765, 0.8823529411764706, 0.7647058823529411, 0.5882352941176471, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.6470588235294118, 0.6470588235294118, 0.7647058823529411, 0.5882352941176471, 0.7058823529411765, 0.7058823529411765, 0.7647058823529411, 0.8235294117647058, 0.8235294117647058, 0.6470588235294118, 0.7647058823529411, 0.7647058823529411]\n",
      "Mean Accuracy: 0.7290\n",
      "Fitting 80 folds for each of 12 candidates, totalling 960 fits\n",
      "\n",
      "Best Parameters: {'max_iter': 1000, 'penalty': 'l2', 'solver': 'sag', 'tol': 0.0001}\n",
      "Cross-validation Accuracies: [0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.7777777777777778, 0.6666666666666666, 0.6111111111111112, 0.7222222222222222, 0.8333333333333334, 0.6111111111111112, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.6666666666666666, 0.6111111111111112, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.8888888888888888, 0.9444444444444444, 0.6111111111111112, 0.6111111111111112, 0.9444444444444444, 0.7222222222222222, 0.5882352941176471, 0.7647058823529411, 0.7058823529411765, 0.5294117647058824, 0.6470588235294118, 0.5882352941176471, 0.6470588235294118, 0.9411764705882353, 0.7058823529411765, 0.7647058823529411, 0.8823529411764706, 0.6470588235294118, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.7647058823529411, 0.6470588235294118, 0.7058823529411765, 0.4117647058823529, 0.7647058823529411, 0.6470588235294118, 0.8235294117647058, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.8235294117647058, 0.5882352941176471, 0.7058823529411765, 0.7647058823529411, 0.7647058823529411, 0.8235294117647058, 0.9411764705882353, 0.6470588235294118, 0.7647058823529411, 0.5882352941176471]\n",
      "Mean Accuracy: 0.7304\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_logModel_2 = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"solver\":[\"sag\",\"lbfgs\",\"newton-cg\"],\n",
    "     \"tol\":[1e-4,1e-5],\n",
    "     \"max_iter\": [1000,2000]\n",
    "     }]\n",
    "logModel_tunned_2a = hyperparamaterTunning(X_uni, param_grid_logModel_2, folds, LogisticRegression(fit_intercept=True))\n",
    "\n",
    "logModel_tunned_2b = hyperparamaterTunning(X_uni_bi, param_grid_logModel_2, folds, LogisticRegression(fit_intercept=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SVM Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 20 candidates, totalling 600 fits\n",
      "\n",
      "Best Parameters: {'C': 0.1, 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "Cross-validation Accuracies: [0.6808510638297872, 0.7021276595744681, 0.7021276595744681, 0.7021276595744681, 0.7021276595744681, 0.8085106382978723, 0.7446808510638298, 0.7446808510638298, 0.8297872340425532, 0.6808510638297872, 0.6595744680851063, 0.7872340425531915, 0.7659574468085106, 0.8723404255319149, 0.7021276595744681, 0.7659574468085106, 0.6170212765957447, 0.6808510638297872, 0.7659574468085106, 0.7659574468085106, 0.6956521739130435, 0.7391304347826086, 0.6521739130434783, 0.6521739130434783, 0.8043478260869565, 0.6956521739130435, 0.6739130434782609, 0.6739130434782609, 0.8260869565217391, 0.7391304347826086]\n",
      "Mean Accuracy: 0.7278\n",
      "Fitting 30 folds for each of 20 candidates, totalling 600 fits\n",
      "\n",
      "Best Parameters: {'C': 0.8, 'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "Cross-validation Accuracies: [0.6595744680851063, 0.6595744680851063, 0.723404255319149, 0.6382978723404256, 0.6808510638297872, 0.7446808510638298, 0.7446808510638298, 0.7021276595744681, 0.8723404255319149, 0.6382978723404256, 0.6595744680851063, 0.723404255319149, 0.7872340425531915, 0.8297872340425532, 0.723404255319149, 0.723404255319149, 0.5531914893617021, 0.7021276595744681, 0.7021276595744681, 0.723404255319149, 0.5869565217391305, 0.7391304347826086, 0.6739130434782609, 0.6739130434782609, 0.6956521739130435, 0.8260869565217391, 0.6521739130434783, 0.6521739130434783, 0.6956521739130435, 0.6956521739130435]\n",
      "Mean Accuracy: 0.7028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "param_grid_SVC_1 = [\n",
    "    {\"penalty\":[\"l1\",\"l2\"],\n",
    "     \"C\": np.arange(0.1,1.1,0.1).tolist(),\n",
    "     \"loss\": [\"squared_hinge\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "SVMModel_tunned_1a = hyperparamaterTunning(X_uni, param_grid_SVC_1, folds, LinearSVC(fit_intercept=True))\n",
    "\n",
    "SVMModel_tunned_1b = hyperparamaterTunning(X_uni_bi, param_grid_SVC_1, folds, LinearSVC(fit_intercept=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 70 folds for each of 10 candidates, totalling 700 fits\n",
      "\n",
      "Best Parameters: {'C': 0.5, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "Cross-validation Accuracies: [0.9, 0.7, 0.65, 0.7, 0.8, 0.65, 0.75, 0.75, 0.85, 0.75, 0.7, 0.75, 0.75, 0.65, 0.75, 0.85, 0.75, 0.75, 0.8, 0.95, 0.75, 0.8, 0.7, 0.4, 0.7, 0.85, 0.9, 0.7, 0.75, 0.8, 0.8, 0.8, 0.7, 0.85, 0.65, 0.65, 0.8, 0.75, 0.65, 0.65, 0.7, 0.8, 0.75, 0.8, 0.85, 0.8, 0.8, 0.75, 0.7, 0.9, 0.7, 0.65, 0.65, 0.75, 0.5, 0.85, 0.7, 0.75, 0.8, 0.8, 0.75, 0.65, 0.8, 0.6, 0.7, 0.8, 0.9, 0.8, 0.7, 0.75]\n",
      "Mean Accuracy: 0.7471\n",
      "Fitting 70 folds for each of 10 candidates, totalling 700 fits\n",
      "\n",
      "Best Parameters: {'C': 0.1, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "Cross-validation Accuracies: [0.75, 0.65, 0.6, 0.65, 0.8, 0.7, 0.7, 0.65, 0.8, 0.75, 0.65, 0.8, 0.75, 0.7, 0.75, 0.75, 0.65, 0.8, 0.7, 0.85, 0.7, 0.8, 0.65, 0.4, 0.75, 0.85, 0.9, 0.65, 0.75, 0.75, 0.75, 0.75, 0.8, 0.8, 0.6, 0.55, 0.7, 0.7, 0.45, 0.5, 0.7, 0.8, 0.8, 0.75, 0.75, 0.8, 0.7, 0.65, 0.7, 0.85, 0.65, 0.75, 0.6, 0.7, 0.6, 0.85, 0.75, 0.65, 0.8, 0.75, 0.65, 0.75, 0.75, 0.65, 0.7, 0.65, 0.75, 0.75, 0.7, 0.7]\n",
      "Mean Accuracy: 0.7129\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "param_grid_SVC_2 = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"C\": np.arange(0.1,1.1,0.1).tolist(),\n",
    "     \"loss\": [\"hinge\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "# X_uni --> only discarded term that are smaller than 2\n",
    "\n",
    "# MAX VALUE: 0.7443 -> Best Parameters: {'C': 0.6, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.0001} using X1_uni and 70 folds\n",
    "\n",
    "SVMModel_tunned_2a = hyperparamaterTunning(X_uni, param_grid_SVC_2, folds, LinearSVC(fit_intercept=True)) # Best one so far\n",
    "SVMModel_tunned_2b = hyperparamaterTunning(X_uni_bi, param_grid_SVC_2, folds, LinearSVC(fit_intercept=True)) # Best one so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# LOOK INTO THIS OR ELSE DELETE\n",
    "param_grid_SDG = [\n",
    "    {\"penalty\":[\"l1\",\"l2\"],\n",
    "     \"loss\": [\"hinge\", \"log_loss\", \"modified_huber\", \"squared_hinge\", \"perceptron\", \"squared_error\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "param_grid_SDG = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"alpha\":[1e-3],\n",
    "     \"loss\": [\"hinge\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "# SDGModel = hyperparamaterTunning(X_uni_bi, param_grid_SDG, folds, SGDClassifier(fit_intercept=True),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forest Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 80 folds for each of 6 candidates, totalling 480 fits\n",
      "\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 18, 'max_features': 'sqrt'}\n",
      "Cross-validation Accuracies: [0.7777777777777778, 0.5, 0.6666666666666666, 0.5555555555555556, 0.5, 0.6666666666666666, 0.5555555555555556, 0.6666666666666666, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.5, 0.5, 0.7222222222222222, 0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.6666666666666666, 0.8888888888888888, 0.8333333333333334, 0.5555555555555556, 0.8333333333333334, 0.6111111111111112, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.7222222222222222, 0.5882352941176471, 0.5294117647058824, 0.47058823529411764, 0.5882352941176471, 0.6470588235294118, 0.5882352941176471, 0.6470588235294118, 0.8235294117647058, 0.6470588235294118, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.5882352941176471, 0.5882352941176471, 0.5882352941176471, 0.47058823529411764, 0.4117647058823529, 0.5882352941176471, 0.4117647058823529, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.5882352941176471, 0.7647058823529411, 0.6470588235294118, 0.5882352941176471, 0.5882352941176471, 0.8235294117647058, 0.5882352941176471, 0.7058823529411765, 0.4117647058823529, 0.7058823529411765, 0.5882352941176471, 0.6470588235294118, 0.7647058823529411, 0.5294117647058824, 0.8235294117647058]\n",
      "Mean Accuracy: 0.6683\n",
      "Fitting 80 folds for each of 6 candidates, totalling 480 fits\n",
      "\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 18, 'max_features': 'sqrt'}\n",
      "Cross-validation Accuracies: [0.6666666666666666, 0.5555555555555556, 0.5555555555555556, 0.6666666666666666, 0.5, 0.6666666666666666, 0.5555555555555556, 0.7222222222222222, 0.7777777777777778, 0.6111111111111112, 0.6666666666666666, 0.5555555555555556, 0.5555555555555556, 0.6111111111111112, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.6111111111111112, 0.7222222222222222, 0.6111111111111112, 0.6666666666666666, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.6111111111111112, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.5555555555555556, 0.7777777777777778, 0.7777777777777778, 0.6470588235294118, 0.5882352941176471, 0.6470588235294118, 0.5882352941176471, 0.6470588235294118, 0.5882352941176471, 0.5882352941176471, 0.7647058823529411, 0.7058823529411765, 0.8235294117647058, 0.7058823529411765, 0.5294117647058824, 0.5294117647058824, 0.6470588235294118, 0.5294117647058824, 0.8235294117647058, 0.47058823529411764, 0.5882352941176471, 0.5882352941176471, 0.6470588235294118, 0.6470588235294118, 0.5882352941176471, 0.5882352941176471, 0.5882352941176471, 0.5882352941176471, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.6470588235294118, 0.5882352941176471, 0.7647058823529411, 0.6470588235294118, 0.6470588235294118, 0.47058823529411764, 0.8235294117647058, 0.5294117647058824, 0.5294117647058824, 0.5882352941176471, 0.6470588235294118, 0.8823529411764706]\n",
      "Mean Accuracy: 0.6656\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid_rf = [{\n",
    "\"criterion\":[\"gini\", \"entropy\", \"log_loss\"],\n",
    "\"max_features\":[\"sqrt\", \"log2\"],\n",
    "\"max_depth\": [18] # Need to look into what values to use here\n",
    "}]\n",
    "rF = hyperparamaterTunning(X_uni, param_grid_rf, folds, RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 3 1 3 1 0 1 1 1 1 1 3 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 2 3 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 1 2 1 0 0 1 1 3 1 2 1 3 0 1 1 1 2 1 1 1 1 1 0 1 1 1 1 1 2 1\n",
      " 1 2 3 1 1 1 1 1 1 1 1 1 0 1 2 2 3 1 1 1 1 1 3 1 3 1 1 2 1 1 1 3 1 1 1 1 1\n",
      " 1 2 1 1 1 1 2 0 1 1 1 2 1 1 0 1 0 1 1 3 1 1 1 1 1 3 1 0 1 1 1 3 1 3 1 1 2\n",
      " 1 0 3 1 0 2 2 1 3 2 0 3 2 1 2 1 1 1 0 1 3 1 3 3 2 3 3 3 3 3 1 3 1 3 2 1 3\n",
      " 2 0 0 3 1 3 3 3 0 0 3 3 3 3 3 3 3 3 3 3 0 3 3 3 1 3 3 3 0 3 3 2 3 0 3 3 3\n",
      " 2 1 3 1 3 3 3 0 2 3 2 1 0 1 3 3 2 1 3 3 3 3 3 1 0 3 1 3 3 3 3 3 3 0 3 0 3\n",
      " 3 3 2 3 3 0 3 3 3 1 3 3 1 3 3 3 1 2 1 0 1 3 3 3 3 1 2 3 3 3 3 3 3 3 3 0 0\n",
      " 3 0 3 3 3 0 0 1 3 0 1 0 3 0 0 3 2 0 3 1 3 0 0 1 1 1 0 0 2 0 0 0 1 0 3 0 0\n",
      " 3 0 0 0 0 0 1 0 0 0 0 0 3 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 3 0 0 1 2 0\n",
      " 3 0 0 0 3 0 3 1 0 0 0 3 0 0 0 1 3 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 3\n",
      " 0 2 0 0 0 0 2 0 0 0 0 2 0 0 0 0 0 3 2 0 0 0 3 0 0 3 0 0 0 0 0 3 1 0 3 0 2\n",
      " 1 0 0 2 0 0 2 1 1 0 2 2 2 2 2 2 2 2 3 2 0 2 0 2 2 0 2 2 2 2 2 2 2 2 2 0 2\n",
      " 2 2 2 1 2 2 1 2 2 2 3 3 1 2 1 2 2 2 0 2 2 3 2 2 2 2 2 1 2 2 0 0 2 2 2 2 2\n",
      " 2 2 1 2 2 1 2 2 1 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2 2 2 2 3 2 0 2 3 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 1 1 0 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 0 2 2 2\n",
      " 2 2 2 0 3 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "y_pred = SVMModel_tunned_2a.predict(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "with open(\"output2.csv\", mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write a header (optional, if you want)\n",
    "    writer.writerow([\"Feature Name\"])\n",
    "    map = [\"Boston\",\"Canberra\",\"Geneva\",\"Ottawa\"]\n",
    "    # Write the features from the array\n",
    "    for i in range(len(y_pred)):\n",
    "        output = y_pred[i]\n",
    "        writer.writerow([i, map[output]])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
