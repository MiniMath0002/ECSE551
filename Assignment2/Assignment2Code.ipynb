{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "# Group 13\n",
    "Mathieu Mailhot - Isabel Lougheed - Frank-Lucas Pantazis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "## Checkout link: https://www.analyticsvidhya.com/blog/2021/11/a-guide-to-building-an-end-to-end-multiclass-text-classification-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   body\n",
      "0     I had to put in a drain well/french drain, and...\n",
      "1     I've worked with James from Prova accountants ...\n",
      "2     https://lebonmelange.com.au/ is a Gungahlin ca...\n",
      "3     What I love about Canberra are the town planni...\n",
      "4     Canberra has a bigger issue with strata. Rates...\n",
      "...                                                 ...\n",
      "1395  Take the train to La pleine and walk to joncti...\n",
      "1396  IIL alumni here!\\r\\n\\r\\nGeneva private schools...\n",
      "1397  I'm really sorry to hear about your bad experi...\n",
      "1398  They quite easy to handle. Typically, they don...\n",
      "1399  **Specialization is authentic.** You don?t go ...\n",
      "\n",
      "[1400 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "folds = 5 # between 5 and 10\n",
    "\n",
    "# Loading Training data\n",
    "df_train = pd.read_csv('train.csv', encoding='utf-8', encoding_errors='ignore') # errors were not pertinent characters\n",
    "df_train[\"subreddit\"] = df_train[\"subreddit\"].map({\"Boston\": 0, \"Canberra\": 1,\"Geneva\":2,\"Ottawa\":3})\n",
    "\n",
    "y = df_train[\"subreddit\"]\n",
    "X = df_train.drop(\"subreddit\",axis=1)\n",
    "\n",
    "# Loading Test Data\n",
    "df_test = pd.read_csv('test.csv', encoding='utf-8', encoding_errors='ignore') # errors were not pertinent characters\n",
    "X_test = df_test[\"body\"] # Not what we should do with the ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram (size: 12557)\n",
      " ['00' '000' '0001' ... 'zucchini' 'zurich' 'zxwr7mvro1z3kabb'] \n",
      "\n",
      "Unigram & Bigram (size: 62167)\n",
      " ['00' '00 33' '00 avec' ... 'zurich tried' 'zxwr7mvro1z3kabb'\n",
      " 'zxwr7mvro1z3kabb year'] \n",
      "\n",
      "Bigram (size: 49610)\n",
      " ['00 33' '00 avec' '00 bit' ... 'zurich migrations' 'zurich tried'\n",
      " 'zxwr7mvro1z3kabb year']\n",
      "{'00': 0, '000': 0, '01': 0, '02': 0, '03': 0, '04': 0, '10': 0, '100': 0, '1000': 0, '11': 0}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Observations\n",
    "# - bigram -> worse performance\n",
    "# - sublinear_tf -> seems to improve accuracy\n",
    "# - decreasing max_features -> seems to decrease accuracy (feature reduction)\n",
    "\n",
    "# TODO\n",
    "# - Create custom stop word list since default one might not be suited for our case according to documentation: https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n",
    "# - explore different ways to extract features from text data\n",
    "\n",
    "# Instantiate Vectorizer\n",
    "tfidf_uni = TfidfVectorizer(ngram_range=(1, 1), sublinear_tf=True, stop_words='english')\n",
    "tfidf_uni_bi = TfidfVectorizer(ngram_range=(1, 2),sublinear_tf=True, stop_words='english')\n",
    "tfidf_bi = TfidfVectorizer(ngram_range=(2, 2),  stop_words='english')\n",
    "naiveBayes_uni = CountVectorizer(max_features=3000, ngram_range=(1, 1), stop_words='english')\n",
    "\n",
    "# Fit Vectorizer from data\n",
    "X_uni = tfidf_uni.fit_transform(df_train[\"body\"]).toarray()\n",
    "X_uni_bi = tfidf_uni_bi.fit_transform(df_train[\"body\"]).toarray()\n",
    "X_bi = tfidf_bi.fit_transform(df_train[\"body\"]).toarray()\n",
    "X_naive_bayes = naiveBayes_uni.fit_transform(df_train[\"body\"]).toarray()\n",
    "\n",
    "print(\"Unigram\", \"(size:\",str(len(tfidf_uni.get_feature_names_out()))+\")\\n\",tfidf_uni.get_feature_names_out(),\"\\n\")\n",
    "print(\"Unigram & Bigram\", \"(size:\",str(len(tfidf_uni_bi.get_feature_names_out()))+\")\\n\",tfidf_uni_bi.get_feature_names_out(),\"\\n\")\n",
    "print(\"Bigram\", \"(size:\",str(len(tfidf_bi.get_feature_names_out()))+\")\\n\", tfidf_bi.get_feature_names_out())\n",
    "\n",
    "# To get a better idea of the extracted features\n",
    "with open(\"features.csv\", mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write a header (optional, if you want)\n",
    "    writer.writerow([\"Feature Name\"])\n",
    "    # Write the features from the array\n",
    "    for feature in tfidf_uni.get_feature_names_out():\n",
    "        writer.writerow([feature])\n",
    "\n",
    "\n",
    "feature_dict = {word: 0 for word in naiveBayes_uni.get_feature_names_out()}\n",
    "\n",
    "with open(\"featuresNaiveBayes.csv\", mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write a header (optional, if you want)\n",
    "    writer.writerow([\"Feature Name\"])\n",
    "    # Write the features from the array\n",
    "    for feature in naiveBayes_uni.get_feature_names_out():\n",
    "        writer.writerow([feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparamater Optimisation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# This function does all the tunning for each model\n",
    "def hyperparamaterTunning(X, param, folds, model):\n",
    "    \n",
    "    model_gridSearch = GridSearchCV(model, param_grid=param,cv=folds, verbose=True) # According to doc the data will be split the same way accross all calls\n",
    "\n",
    "    model_best_clf = model_gridSearch.fit(X,y)\n",
    "\n",
    "    print(\"Best Parameters:\", model_best_clf.best_params_)\n",
    "\n",
    "    print(\"Accuracy:\", model_best_clf.best_score_)\n",
    "\n",
    "    return model_best_clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Naive Bayes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naiveBayes:\n",
    "    def __init__(self, x_all, y_all, feature_vectoriser):\n",
    "        self.x_all = x_all\n",
    "        self.y_all = y_all\n",
    "        self.features_probability = dict()\n",
    "\n",
    "        self.folds_features_probability = 0 # array of dict\n",
    "        self.folds_accuracy = 0\n",
    "        self.avg_accuracy = 0\n",
    "\n",
    "    \n",
    "    def calc_probability(self): # Train/Fit # Mathieu\n",
    "\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x_i): # Issy\n",
    "        pass # return y (0,1,2,3)\n",
    "\n",
    "    def accu_eval(self, x, y): # Issy\n",
    "        pass\n",
    "\n",
    "    def crossValidation(self, k): # Issy (PS: I think we are allowed to use the method from sklearn)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Logistic Regression Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Parameters: {'l1_ratio': 0.0, 'max_iter': 1000, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "Accuracy: 0.7050000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_logModel_1 = [\n",
    "    {\"penalty\":[\"elasticnet\"],\n",
    "     \"l1_ratio\": np.arange(0, 1.2, 0.2), # 0 is only l2 penalty, 1 is only l1 penalty\n",
    "     \"solver\":[\"saga\"],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "logModel_tunned_1 = hyperparamaterTunning(X_uni, param_grid_logModel_1, folds, LogisticRegression(fit_intercept=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Parameters: {'max_iter': 1000, 'penalty': 'l2', 'solver': 'sag', 'tol': 0.0001}\n",
      "Accuracy: 0.7050000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_logModel_2 = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"solver\":[\"sag\",\"lbfgs\",\"newton-cg\"],\n",
    "     \"tol\":[1e-4,1e-5],\n",
    "     \"max_iter\": [1000,2000]\n",
    "     }]\n",
    "logModel_tunned_2 = hyperparamaterTunning(X_uni, param_grid_logModel_2, folds, LogisticRegression(fit_intercept=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Linear SVC Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best Parameters: {'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "Accuracy: 0.7092857142857143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "param_grid_SVC_1 = [\n",
    "    {\"penalty\":[\"l1\",\"l2\"],\n",
    "     \"loss\": [\"squared_hinge\"],\n",
    "     \"tol\":[1e-4,1e-5],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "SVMModel_tunned_1 = hyperparamaterTunning(X_uni, param_grid_SVC_1, folds, LinearSVC(fit_intercept=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best Parameters: {'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "Accuracy: 0.7142857142857142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "param_grid_SVC_2 = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"loss\": [\"hinge\",\"squared_hinge\"],\n",
    "     \"tol\":[1e-4,1e-5],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "SVMModel_tunned_2 = hyperparamaterTunning(X_uni_bi, param_grid_SVC_2, folds, LinearSVC(fit_intercept=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forest Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Best Parameters: {'criterion': 'entropy', 'max_depth': 19, 'max_features': 'sqrt'}\n",
      "Accuracy: 0.6635714285714285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid_rf = [{\n",
    "\"criterion\":[\"gini\", \"entropy\", \"log_loss\"],\n",
    "\"max_features\":[\"sqrt\", \"log2\"],\n",
    "\"max_depth\": range(10,20) # Need to look into what values to use here\n",
    "}]\n",
    "rF = hyperparamaterTunning(X_uni, param_grid_rf, folds, RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
