{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "# Group 13\n",
    "Mathieu Mailhot - Isabel Lougheed - Frank-Lucas Pantazis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Text Preprocessing\n",
    "import nltk\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "## Checkout link: https://www.analyticsvidhya.com/blog/2021/11/a-guide-to-building-an-end-to-end-multiclass-text-classification-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       I had to put in a drain well/french drain, and...\n",
      "1       I've worked with James from Prova accountants ...\n",
      "2       https://lebonmelange.com.au/ is a Gungahlin ca...\n",
      "3       What I love about Canberra are the town planni...\n",
      "4       Canberra has a bigger issue with strata. Rates...\n",
      "                              ...                        \n",
      "1395    Take the train to La pleine and walk to joncti...\n",
      "1396    IIL alumni here!\\n\\nGeneva private schools are...\n",
      "1397    I'm really sorry to hear about your bad experi...\n",
      "1398    They quite easy to handle. Typically, they don...\n",
      "1399    **Specialization is authentic.** You don?t go ...\n",
      "Name: body, Length: 1400, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "folds = 30 # between 5 and 10 # best value at the moment when folds = 30\n",
    "\n",
    "# Loading Training data\n",
    "df_train = pd.read_csv('train.csv', encoding='utf-8', encoding_errors='ignore') # errors were not pertinent characters\n",
    "df_train[\"subreddit\"] = df_train[\"subreddit\"].map({\"Boston\": 0, \"Canberra\": 1,\"Geneva\":2,\"Ottawa\":3})\n",
    "\n",
    "y = df_train[\"subreddit\"]\n",
    "X = df_train[\"body\"]\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Loading Test Data\n",
    "df_test = pd.read_csv('test.csv', encoding='utf-8', encoding_errors='ignore') # errors were not pertinent characters\n",
    " # Not what we should do with the ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Helper Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokenizer(subreddit_post, stop_word_catalogue=None):\n",
    "    subreddit_post = subreddit_post.lower()\n",
    "    words = word_tokenize(subreddit_post)\n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    \n",
    "    lemmatized_words = []\n",
    "    \n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(words):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        condition = True\n",
    "        if (stop_word_catalogue!=None):\n",
    "            condition = word not in stop_word_catalogue\n",
    "        if(condition):\n",
    "            if word.isalpha():\n",
    "                lemmatized_word = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                #print(word,lemmatized_word)\n",
    "                lemmatized_words.append(lemmatized_word)                   \n",
    "    return lemmatized_words\n",
    "\n",
    "def preprocessing_data(X, stop_word_catalogue):\n",
    "    lemmatized_data = []\n",
    "    for i in range(len(X)):\n",
    "        post = X[i]\n",
    "        lemmatized_post = lemmatize_tokenizer(post,stop_word_catalogue)\n",
    "        lemmatized_data.append(\" \".join(lemmatized_post))\n",
    "    return lemmatized_data\n",
    "\n",
    "def feature_extraction(X,ngram_range,min_df,caption=\"\"):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range,min_df=min_df)\n",
    "    X_disp = vectorizer.fit_transform(X)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(caption,\"features:\", len(feature_names))\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Preprocessing </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Word Catalogues\n",
    "nltk_stop_words = stopwords.words('english')\n",
    "sklean_stop_words = list(text.ENGLISH_STOP_WORDS)\n",
    "stop_words_arr = list(text.ENGLISH_STOP_WORDS.union(nltk_stop_words))\n",
    "\n",
    "# Trainning Data\n",
    "pp_df_train_1 = preprocessing_data(X,[])\n",
    "pp_df_train_2 = preprocessing_data(X,nltk_stop_words)\n",
    "pp_df_train_3 = preprocessing_data(X,sklean_stop_words)\n",
    "pp_df_train_4 = preprocessing_data(X,stop_words_arr)\n",
    "\n",
    "pp_df_train = [X,pp_df_train_1,pp_df_train_2,pp_df_train_3,pp_df_train_4]\n",
    "\n",
    "# Testing Data\n",
    "lemmatized_df_test = []\n",
    "for i in range(len(df_test[\"body\"])):\n",
    "    post = df_test[\"body\"][i]\n",
    "    lemmatized_post_test = lemmatize_tokenizer(post,stop_words_arr)\n",
    "    lemmatized_df_test.append(\" \".join(lemmatized_post_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpreprocessed features: 5631\n",
      "Preprocessed 1 features: 4385\n",
      "Preprocessed 2 features: 4272\n",
      "Preprocessed 3 features: 4175\n",
      "Preprocessed 4 features: 4156\n"
     ]
    }
   ],
   "source": [
    "# Lemmatized data\n",
    "for i in range(len(pp_df_train)): \n",
    "    elem = pp_df_train[i]\n",
    "    if (i==0):\n",
    "        feature_extraction(elem,(1,1),2,f\"Unpreprocessed\")\n",
    "    else:\n",
    "        feature_extraction(elem,(1,1),2,f\"Preprocessed {i}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Important words<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningfull_words_list ={ \"features\", \"place\", \"city\", \"live\", \"area\", \"home\", \"local\", \"park\", \"house\", \n",
    "    \"street\", \"town\", \"building\", \"centre\", \"apartment\", \"housing\", \"space\", \"private\", \n",
    "    \"neighbour\", \"quality\", \"location\", \"property\", \"view\", \"garden\", \"living\", \"flat\", \n",
    "    \"landlord\", \"suburb\", \"rental\", \"neighborhood\", \"community\", \"state\", \n",
    "    \"public\", \"bus\", \"car\", \"drive\", \"train\", \"tax\", \"cost\", \"rent\", \"price\", \n",
    "    \"job\", \"money\", \"income\", \"family\", \"food\", \"health\", \"education\", \"shopping\", \n",
    "    \"restaurant\", \"work\", \"school\", \"university\", \"college\", \"traffic\", \"parking\", \n",
    "    \"safety\", \"business\", \"market\", \"weather\", \"snow\", \"summer\", \"lake\", \"mountain\", \n",
    "    \"river\", \"suburb\", \"downtown\", \"north\", \"south\", \"east\", \"west\", \"coast\", \"council\", \n",
    "    \"state\", \"village\", \"region\", \"distance\", \"commute\",\n",
    "    \"belconnen\", \"civic\", \"kingston\", \"fyshwick\", \"gungahlin\", \"woden\", \"ontario\", \"neighborhood\", \n",
    "    \"neighbourhood\", \"rideau\", \"servette\", \"belmont\", \"bilingual\", \"capital\", \"chinese\", \"clinic\", \n",
    "    \"dickson\", \"east\", \"europe\", \"farm\", \"grocery\", \"immigration\", \"jonction\", \"lady\", \"loud\", \"mall\", \n",
    "    \"metrolinx\", \"montreux\", \"officer\", \"paquis\", \"port\", \"province\", \"rain\", \"reading\", \"resolve\", \n",
    "    \"rush\", \"sector\", \"se\", \"separate\", \"toronto\", \"village\", \"wind\", \"vancouver\", \"vieux\", \"windsor\", \n",
    "    \"bell\", \"bakery\", \"bar\", \"boston\", \"canton\", \"centre\", \"city\", \"club\", \"commune\", \"downtown\", \"drive\", \n",
    "    \"family\", \"flat\", \"food\", \"france\", \"gardens\", \"glen\", \"grocery\", \"home\", \"housing\", \"indirect\", \"island\", \n",
    "    \"land\", \"location\", \"market\", \"metro\", \"neighbour\", \"neighborhood\", \"north\", \"paris\", \"place\", \"pool\", \n",
    "    \"province\", \"queensland\", \"rain\", \"river\", \"road\", \"shop\", \"shopping\", \"suburb\", \"village\", \"walk\", \"water\", \n",
    "    \"west\", \"woden\",\n",
    "    \"brisbane\", \"brookline\", \"byward\", \"carouge\", \"canal\", \"centre\", \"china\", \"classic\", \"climate\", \n",
    "    \"commuter\", \"cornavin\", \"davis\", \"dealer\", \"dickson\", \"downtown\", \"expat\", \"fenway\", \"ferney\", \n",
    "    \"garage\", \"gentleman\", \"grocery\", \"hassle\", \"hospital\", \"indoor\", \"kanata\", \"kitchen\", \"lansdowne\", \n",
    "    \"london\", \"malden\", \"majority\", \"master\", \"metrolinx\", \"mont\", \"montreal\", \"nsw\", \"ont\", \"orleans\", \n",
    "    \"parkway\", \"queanbeyan\", \"regina\", \"subway\", \"subaru\", \"sydney\", \"toronto\", \"vancouver\", \"ville\", \n",
    "    \"wellington\", \"western\", \"windsor\", \"braddon\", \"canberrans\", \"carleton\", \"centretown\", \"chestnut\", \n",
    "    \"citizenship\", \"commercial\", \"comparison\", \"competitive\", \"coworkers\", \"cultural\", \"cumming\", \n",
    "    \"culture\", \"deep\", \"detour\", \"discover\", \"divert\", \"donut\", \"dossier\", \"employ\", \"employment\", \n",
    "    \"enforce\", \"entertainment\", \"exchange\", \"famous\", \"frontaliers\", \"garbage\", \"gate\", \"gender\", \n",
    "    \"glebe\", \"growth\", \"hassle\", \"honest\", \"huntsville\", \"identify\", \"integrate\", \"interior\", \"introduce\", \n",
    "    \"itinerary\", \"jacket\", \"july\", \"kanata\", \"lifestyle\", \"loop\", \"lunch\", \"mayor\", \"membership\", \n",
    "    \"metro\", \"mexican\", \"midnight\", \"minority\", \"monday\", \"monster\", \"natural\", \"newbury\", \"operation\", \n",
    "    \"orleans\", \"outdoor\", \"park\", \"peux\", \"phd\", \"photograph\", \"plan\", \"plow\", \"post\", \"precinct\", \"press\", \n",
    "    \"promotion\", \"pump\", \"propose\", \"quebec\", \"realistic\", \"recycle\", \"reference\", \"reflect\", \"refund\", \n",
    "    \"remind\", \"repeat\", \"represent\", \"reservation\", \"retail\", \"rip\", \"rise\", \"roller\",\n",
    "    \"acton\", \"advertise\", \"alewife\", \"arlington\", \"avenue\", \"barrier\", \"billion\", \"bishop\", \"boat\", \n",
    "    \"boston\", \"braddon\", \"brockton\", \"bro\", \"cambridge\", \"canal\", \"canton\", \"cent\", \"chateau\", \"channel\", \n",
    "    \"cher\", \"chicago\", \"clearing\", \"cliff\", \"columbus\", \"cornavin\", \"countryside\", \"darling\", \"downtown\", \n",
    "    \"eaux\", \"embassy\", \"esplanade\", \"fais\", \"faith\", \"fen\", \"fence\", \"flood\", \"footpath\", \"franais\", \"frontire\", \n",
    "    \"gatineau\", \"geneva\", \"gva\", \"hayden\", \"hudson\", \"institute\", \"jardin\", \"jfk\", \"kenmore\", \"lyon\", \"london\", \n",
    "    \"louvre\", \"malden\", \"manchester\", \"meadow\", \"melbourne\", \"metropolitan\", \"meyrin\", \"miami\", \"montreal\", \n",
    "    \"nyc\", \"ontario\", \"ottawa\", \"paris\", \"park\", \"perception\", \"quincy\", \"regina\", \"residential\", \"riverside\", \n",
    "    \"rome\", \"saconnex\", \"sbb\", \"sherbrooke\", \"shore\", \"shut\", \"slide\", \"south\", \"spain\", \"sydney\", \"temple\", \"toronto\", \n",
    "    \"vancouver\", \"vives\", \"washington\", \"west\", \"wellington\", \"zoo\",\n",
    "    \"danvers\", \"dcembre\", \"genevan\", \"gex\", \"halifax\", \"hilarious\", \"hindmarsh\", \"hong\", \"idp\", \n",
    "    \"jamison\", \"jan\", \"jean\", \"kendall\", \"kippax\", \"kong\", \"lancy\", \"logan\", \"meadow\", \"melrose\", \n",
    "    \"merivale\", \"mgb\", \"mawson\", \"monitor\", \"murray\", \"nyon\", \"pacific\", \"parc\", \"parliament\", \n",
    "    \"philipp\", \"plaza\", \"providence\", \"rochester\", \"rural\", \"saint\", \"somerset\", \"somerville\", \n",
    "    \"southie\", \"southside\", \"suburban\", \"sunset\", \"survey\", \"territory\", \"tugger\", \"vanier\", \n",
    "    \"versoix\", \"virginia\", \"warren\", \"wednesday\", \"windy\", \"whatsapp\", \"toyota\",\n",
    "    \"barrhaven\", \"basel\", \"bathurst\", \"bernie\", \"bostonian\", \"boulangerie\", \"boulevard\", \n",
    "    \"bowral\", \"brighton\", \"burlington\", \"calwell\", \"charlestown\", \"charnwood\", \"chicago\", \n",
    "    \"claridge\", \"clover\", \"coastal\", \"collaboration\", \"colonization\", \"colony\", \"commune\", \n",
    "    \"community\", \"coordinate\", \"council\", \"corriidor\", \"culturally\", \"darwin\", \"decide\", \n",
    "    \"dublin\", \"dunsborough\", \"escalator\", \"florida\", \"france\", \"fremantle\", \"goulburn\", \n",
    "    \"greatly\", \"grouse\", \"gta\", \"heavily\", \"hilton\", \"honk\", \"institute\", \"intensive\", \"interstate\", \n",
    "    \"ireland\", \"jamaica\", \"japan\", \"joint\", \"jura\", \"katoomba\", \"kenya\", \"kilometre\", \"kitchen\", \n",
    "    \"krispy\", \"liverpool\", \"luxembourg\", \"manuka\", \"melbourne\", \"mines\", \"motorbike\", \"montreal\", \n",
    "    \"ncc\", \"newly\", \"ny\", \"oak\", \"obama\", \"october\", \"olympic\", \"paradise\", \"parliament\", \n",
    "    \"parkdale\", \"parkour\", \"perth\", \"pioneer\", \"pound\", \"port\", \"postcard\", \"prescription\",\n",
    "    \"puplinge\", \"qld\", \"quebec\", \"riverside\", \"roadway\", \"roxbury\", \"salem\", \"seattle\", \"sandy\", \n",
    "    \"seaport\", \"spain\", \"spanish\", \"stoneham\", \"suburbs\", \"sydney\", \"tavern\", \"territoire\", \"theater\", \n",
    "    \"tuggeranong\", \"vancouver\", \"waltham\", \"wanniassa\", \"ward\", \"warehouse\", \"wellington\", \"westboro\", \n",
    "    \"weston\", \"wetland\", \"wholesale\", \"wilson\", \"winchester\", \"winery\", \"yarralumla\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stop Word Custom (TO MODIFY)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ad', 'aid', 'bias', 'coach', 'come', 'correct', 'director', 'effective', 'engagement', 'flip', 'good', 'imagine', 'invest', 'new', 'pack', 'perfect', 'rid', 'rink', 'significantly', 'situation', 'star', 'strategy', 'suddenly', 'sure', 'thing', 'think', 'trend', 'utility']\n",
      "Initial Stop Word Size: 318\n",
      "Custom Stop Word Size: 346\n"
     ]
    }
   ],
   "source": [
    "# Creating new stopword list\n",
    "vectorizer = CountVectorizer(stop_words='english',ngram_range=(1, 1),min_df=1)\n",
    "X_stop = vectorizer.fit_transform(pp_df_train_1)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Checkout: https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a/\n",
    "Boston_counts = X_stop[y == 0].sum(axis=0).A1 # Sum occurrences for class 'Boston'\n",
    "Canberra_counts = X_stop[y == 1].sum(axis=0).A1 # Sum occurrences for class 'Canberra'\n",
    "Geneva_counts = X_stop[y == 2].sum(axis=0).A1 # Sum occurrences for class 'Geneva'\n",
    "Ottawa_counts = X_stop[y == 3].sum(axis=0).A1 # Sum occurrences for class 'Ottawa'\n",
    "\n",
    "header = [\"features\",\"Boston\",\"Canberra\",\"Geneva\",\"Ottawa\",\"Count\"]\n",
    "custom = []\n",
    "for i in range(len(feature_names)):\n",
    "    k = [Boston_counts[i].item(),Canberra_counts[i].item(),Geneva_counts[i].item(),Ottawa_counts[i].item()]\n",
    "    a = sum([Boston_counts[i].item(),Canberra_counts[i].item(),Geneva_counts[i].item(),Ottawa_counts[i].item()])\n",
    "    crit  = abs(min(k) - max(k))/ a\n",
    "    if (crit<=0.1 and (crit*a)/max(k)<=0.20):\n",
    "        custom.append(feature_names[i])\n",
    "print(custom)\n",
    "print(\"Initial Stop Word Size:\",len(text.ENGLISH_STOP_WORDS))\n",
    "stop_words_custom = list(text.ENGLISH_STOP_WORDS.union(custom))\n",
    "print(\"Custom Stop Word Size:\",len(stop_words_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram (size: 12835)\n",
      "['00' '000' '0001' ... 'zucchini' 'zurich' 'zxwr7mvro1z3kabb']\n",
      "Unigram & Bigram (size: 5000)\n",
      "Bigram (size: 2246)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import NMF\n",
    "# Observations\n",
    "# - bigram -> worse performance\n",
    "# - sublinear_tf -> seems to improve accuracy\n",
    "# - decreasing max_features -> seems to decrease accuracy (feature reduction)\n",
    "\n",
    "# TODO\n",
    "# - Create custom stop word list since default one might not be suited for our case according to documentation: https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n",
    "# - explore different ways to extract features from text data\n",
    "\n",
    "# Instantiate Vectorizer\n",
    "tfidf_uni = TfidfVectorizer(ngram_range=(1, 1), sublinear_tf=True, stop_words=stop_words_custom)\n",
    "tfidf_uni_bi = TfidfVectorizer(max_features=5000, ngram_range=(1, 2),sublinear_tf=True, stop_words=stop_words_custom)\n",
    "tfidf_bi = TfidfVectorizer(ngram_range=(2, 2), sublinear_tf=True,min_df=2, stop_words=stop_words_custom)\n",
    "naiveBayes_uni = CountVectorizer(max_features=3000, ngram_range=(1, 1), stop_words=stop_words_custom)\n",
    "\n",
    "\n",
    "# Fit Vectorizer from data\n",
    "X_uni = tfidf_uni.fit_transform(lemmatized_df_train).toarray()\n",
    "X_uni_bi = tfidf_uni_bi.fit_transform(lemmatized_df_train).toarray()\n",
    "X_bi = tfidf_bi.fit_transform(X).toarray()\n",
    "X_naive_bayes = naiveBayes_uni.fit_transform(X).toarray()\n",
    "\n",
    "\n",
    "\n",
    "#X_test = tfidf_uni_bi.transform(lemmatized_df_test)\n",
    "\n",
    "\n",
    "print(\"Unigram\", \"(size:\",str(len(tfidf_uni.get_feature_names_out()))+\")\")\n",
    "print(tfidf_uni.get_feature_names_out())\n",
    "#print(tfidf_uni.get_feature_names_out())\n",
    "print(\"Unigram & Bigram\", \"(size:\",str(len(tfidf_uni_bi.get_feature_names_out()))+\")\")\n",
    "#print(tfidf_uni_bi.get_feature_names_out())\n",
    "print(\"Bigram\", \"(size:\",str(len(tfidf_bi.get_feature_names_out()))+\")\")\n",
    "#print(tfidf_bi.get_feature_names_out())\n",
    "\n",
    "# LOOK INTO NMF AND WHAT IT CAN DO TO HELP US\n",
    "#nmf = NMF(100).fit(X_uni)\n",
    "\n",
    "#for topic_idx, topic in enumerate(nmf.components_):\n",
    "#    top_features_ind = topic.argsort()[-10:]\n",
    "#    top_features = tfidf_uni.get_feature_names_out()[top_features_ind]\n",
    "#    print(top_features)\n",
    "    \n",
    "# To get a better idea of the extracted features\n",
    "with open(\"features.csv\", mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write a header (optional, if you want)\n",
    "    writer.writerow([\"Feature Name\"])\n",
    "    # Write the features from the array\n",
    "    for feature in tfidf_uni.get_feature_names_out():\n",
    "        writer.writerow([feature])\n",
    "\n",
    "with open(\"featuresNaiveBayes.csv\", mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write a header (optional, if you want)\n",
    "    writer.writerow([\"Feature Name\"])\n",
    "    # Write the features from the array\n",
    "    for feature in naiveBayes_uni.get_feature_names_out():\n",
    "        writer.writerow([feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['now', 'cant', 'anyway', 'inc', 'ie', 'am', 'wherever', 'hers', 'first', 'during', 'your', 'our', 'of', 'up', 'found', 'flip', 'once', 'aid', 'be', 'as', 'hundred', 'else', 'whoever', 'and', 'out', 'some', 'toward', 'anyone', 'ten', 'herein', 'hence', 'was', 'above', 'himself', 'his', 'together', 'again', 'twelve', 'might', 'behind', 'towards', 'although', 'nobody', 'in', 'six', 'yourselves', 'alone', 'everything', 'former', 'with', 'i', 'see', 'everywhere', 'moreover', 'perfect', 'he', 'nor', 'whole', 'pack', 'what', 'often', 'anything', 'serious', 'otherwise', 'already', 'those', 'do', 'you', 'further', 'around', 'being', 'name', 'eg', 'besides', 'same', 'trend', 'hasnt', 'etc', 'too', 'a', 'sure', 'nevertheless', 'de', 'co', 'less', 'ever', 'seemed', 'well', 'ad', 'sincere', 'whereupon', 'per', 'itself', 'bias', 'becoming', 'latter', 'several', 'mine', 'though', 'fifty', 'more', 'side', 'due', 'both', 'give', 'afterwards', 'invest', 'few', 'thick', 'before', 'this', 'imagine', 'get', 'effective', 'thing', 'show', 'sixty', 'formerly', 'there', 'thru', 'why', 'others', 'anywhere', 'rink', 'could', 'on', 'themselves', 'whatever', 'then', 'because', 'whose', 'nine', 'un', 'will', 'through', 'they', 'five', 'have', 'situation', 'good', 'yet', 'were', 'almost', 'hereupon', 'across', 'meanwhile', 'that', 'rather', 'whereafter', 'us', 'every', 'neither', 'thin', 'perhaps', 'noone', 'been', 'front', 'new', 'are', 'latterly', 'fill', 'for', 'or', 'much', 'only', 'sometime', 'therefore', 'where', 'done', 'against', 'their', 'move', 'wherein', 'must', 'ltd', 'part', 'therein', 'put', 'whither', 'below', 'him', 'cry', 'off', 'how', 'please', 'cannot', 'couldnt', 'would', 'director', 'from', 'each', 'after', 'except', 'beforehand', 'amount', 'not', 'come', 'also', 'someone', 'which', 'such', 'no', 'among', 'under', 'its', 'by', 'she', 'it', 'last', 'call', 'keep', 'detail', 'interest', 'whether', 'whenever', 'hereby', 'one', 'most', 'either', 'many', 'so', 'became', 'four', 'fifteen', 'at', 'over', 'top', 'but', 'the', 'significantly', 'whence', 'upon', 'whom', 'my', 'amoungst', 'something', 'rid', 'suddenly', 'about', 'mill', 'thus', 'beside', 'empty', 'take', 'if', 'strategy', 'another', 'anyhow', 'between', 'since', 'con', 'least', 'made', 'namely', 'however', 'third', 'down', 'somewhere', 'three', 'describe', 'fire', 'me', 'until', 'thence', 'everyone', 'hereafter', 'become', 'her', 'via', 'two', 'own', 'correct', 'myself', 'twenty', 'whereby', 'any', 'never', 'indeed', 'than', 'elsewhere', 'who', 'within', 'when', 'ours', 'coach', 'always', 'even', 'eleven', 'throughout', 'yours', 'mostly', 'yourself', 'star', 're', 'bottom', 'here', 'seem', 'along', 'has', 'bill', 'into', 'full', 'seeming', 'may', 'seems', 'should', 'engagement', 'amongst', 'enough', 'without', 'thereafter', 'to', 'forty', 'is', 'somehow', 'becomes', 'we', 'an', 'very', 'nowhere', 'utility', 'all', 'none', 'find', 'next', 'beyond', 'still', 'can', 'thereupon', 'these', 'go', 'herself', 'nothing', 'back', 'ourselves', 'sometimes', 'thereby', 'other', 'while', 'had', 'whereas', 'system', 'onto', 'think', 'them', 'eight']\n",
      "Unigram (size: 470)\n",
      "Unigram (size: 5335)\n"
     ]
    }
   ],
   "source": [
    "#tfidf_uni_arr = []\n",
    "#X_uni_arr = []\n",
    "#for i in range(len(pp_df_train)):\n",
    "#    tfidf = TfidfVectorizer(ngram_range=(1, 1), min_df=2,sublinear_tf=True, stop_words=stop_words_custom)\n",
    "#    X_uni_arr.append(np.power(tfidf.fit_transform(pp_df_train[i]).toarray(), 2) )\n",
    "#    tfidf_uni_arr.append(tfidf)\n",
    "#    print(\"Unigram\", \"(size:\",str(len(tfidf.get_feature_names_out()))+\")\")\n",
    "\n",
    "tfidf_primary = TfidfVectorizer(vocabulary=meaningfull_words_list)\n",
    "X_uni_primary = tfidf_primary.fit(pp_df_train[0])\n",
    "tfidf_secondary = TfidfVectorizer(ngram_range=(1, 1),min_df=2,sublinear_tf=True, stop_words=stop_words_custom)\n",
    "X_uni_secondary = tfidf_secondary.fit(pp_df_train[0])\n",
    "print(\"Unigram\", \"(size:\",str(len(tfidf_primary.get_feature_names_out()))+\")\")\n",
    "print(\"Unigram\", \"(size:\",str(len(tfidf_secondary.get_feature_names_out()))+\")\")\n",
    "#tfidf_uni_bi_arr = []\n",
    "#X_uni_bi_arr = []\n",
    "#for i in range(len(pp_df_train)):\n",
    "#    tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=2, sublinear_tf=True,stop_words=\"english\")\n",
    "#    X_uni_bi_arr.append( tfidf.fit_transform(pp_df_train[i]).toarray())\n",
    "#    tfidf_uni_bi_arr.append(tfidf)\n",
    "#    print(\"Unigram & Bigram\", \"(size:\",str(len(tfidf.get_feature_names_out()))+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ad', 'add', 'advice', 'aid', 'allow', 'bias', 'coach', 'come', 'correct', 'director', 'discount', 'effective', 'engagement', 'example', 'explain', 'flip', 'good', 'hope', 'imagine', 'invest', 'need', 'new', 'open', 'pack', 'perfect', 'provide', 'return', 'rid', 'rink', 'say', 'significantly', 'situation', 'star', 'strategy', 'struggle', 'suddenly', 'sure', 'tell', 'thing', 'think', 'time', 'trend', 'utility', 'wear']\n"
     ]
    }
   ],
   "source": [
    "# Draft of Feature Visualizer\n",
    "# Maybe should put all of it in an excel and then display it?\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english',ngram_range=(1, 1),min_df=2)\n",
    "\n",
    "X_disp = vectorizer.fit_transform(pp_df_train[4])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "Boston_counts = X_disp[y == 0].sum(axis=0).A1 # Sum occurrences for class 'Boston'\n",
    "Canberra_counts = X_disp[y == 1].sum(axis=0).A1 # Sum occurrences for class 'Canberra'\n",
    "Geneva_counts = X_disp[y == 2].sum(axis=0).A1 # Sum occurrences for class 'Geneva'\n",
    "Ottawa_counts = X_disp[y == 3].sum(axis=0).A1 # Sum occurrences for class 'Ottawa'\n",
    "\n",
    "\n",
    "header = [\"features\",\"Boston\",\"Canberra\",\"Geneva\",\"Ottawa\",\"Count\"]\n",
    "custom = []\n",
    "table = []\n",
    "for i in range(len(feature_names)):\n",
    "    k = [Boston_counts[i].item(),Canberra_counts[i].item(),Geneva_counts[i].item(),Ottawa_counts[i].item()]\n",
    "    a = sum([Boston_counts[i].item(),Canberra_counts[i].item(),Geneva_counts[i].item(),Ottawa_counts[i].item()])\n",
    "    crit  = abs(min(k) - max(k))/ a\n",
    "\n",
    "    #table.append([feature_names[i],Boston_counts[i].item(),Canberra_counts[i].item(),Geneva_counts[i].item(),Ottawa_counts[i].item(),a])\n",
    "    if (crit>0.08 and (crit*a)/max(k)>0.2):\n",
    "        table.append([feature_names[i],Boston_counts[i].item(),Canberra_counts[i].item(),Geneva_counts[i].item(),Ottawa_counts[i].item(),a])\n",
    "    else:\n",
    "        custom.append(feature_names[i])\n",
    "print(custom)\n",
    "table_sorted = sorted(table, key=lambda x: x[5], reverse=True)\n",
    "\n",
    "if (True):\n",
    "    with open(\"featureVisualiser5.csv\", mode='w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # Write a header (optional, if you want)\n",
    "        writer.writerow(header)\n",
    "        # Write the features from the array\n",
    "        for row in table_sorted:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Helper Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# This function does all the tunning for each model\n",
    "def hyperparamaterTunning(X,param, folds, model, verbose_val=1, Y=y):\n",
    "    \n",
    "    model_gridSearch = GridSearchCV(model, param_grid=param,cv=folds, verbose=verbose_val) # According to doc the data will be split the same way accross all calls\n",
    "\n",
    "    model_best_clf = model_gridSearch.fit(X,Y)\n",
    "\n",
    "    cv_results = model_gridSearch.cv_results_\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(f\"Best Parameters: {model_best_clf.best_params_}\")\n",
    "    try:\n",
    "\n",
    "        best_index = model_gridSearch.best_index_\n",
    "\n",
    "        score = []\n",
    "        for fold in range(folds):\n",
    "            score.append(model_gridSearch.cv_results_[f\"split{fold}_test_score\"][best_index].item())\n",
    "\n",
    "        print(f\"Cross-validation Accuracies: {score}\")\n",
    "    except:\n",
    "        pass\n",
    "    print(f\"Mean Accuracy: {model_best_clf.best_score_:.4f}\")\n",
    "\n",
    "    return model_best_clf\n",
    "\n",
    "\n",
    "def meaninfullWords(X,y,word_list):\n",
    "    X_primary = []\n",
    "    y_primary = []\n",
    "    X_secondary= []\n",
    "    y_secondary=[]\n",
    "    for i in range(len(X)):\n",
    "        appended  = False\n",
    "        words  = X[i].split(\" \")\n",
    "        for word in words:\n",
    "            if (word in word_list):\n",
    "                X_primary.append(X[i])\n",
    "                y_primary.append(y[i])\n",
    "                appended = True\n",
    "                break\n",
    "        if (not appended):\n",
    "            X_secondary.append(X[i])\n",
    "            y_secondary.append(y[i])\n",
    "    return (X_primary,y_primary,X_secondary,y_secondary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Naive Bayes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, x_all, y_all, feature_vectoriser):\n",
    "        self.x_all = self.clean_text_data(x_all) # Make lists of strings\n",
    "        self.y_all = y_all\n",
    "        self.feature_vectoriser = feature_vectoriser\n",
    "\n",
    "        self.folds_features_probability = 0 # array of dict\n",
    "        self.folds_accuracy = 0\n",
    "        self.avg_accuracy = 0\n",
    "\n",
    "    \n",
    "    def calc_probability(self, x, y): # Train/Fit # Mathieu\n",
    "        # Create an empty dictionnary with the 3000 most common words for each subreddit.\n",
    "        features_probability_boston = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "        features_probability_canberra = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "        features_probability_geneva = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "        features_probability_ottawa = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "\n",
    "        # Initialize the count for the total number of text from each subreddit\n",
    "        count_boston = 0\n",
    "        count_canberra = 0\n",
    "        count_geneva = 0\n",
    "        count_ottowa = 0\n",
    "\n",
    "        # Add 1 to the word in the dictionnary when the word is present in the text\n",
    "        for i in range(y.shape[0]):\n",
    "            if y[i] == 0:\n",
    "                count_boston += 1\n",
    "                self.add_probability(features_probability_boston, x[i])\n",
    "            if y[i] == 1:\n",
    "                count_canberra += 1\n",
    "                self.add_probability(features_probability_canberra, x[i])\n",
    "            if y[i] == 2:\n",
    "                count_geneva += 1\n",
    "                self.add_probability(features_probability_geneva, x[i])\n",
    "            else:\n",
    "                count_ottowa += 1\n",
    "                self.add_probability(features_probability_ottawa, x[i])\n",
    "\n",
    "        # Add the total count of each city to a variable called \"city_count\" and the probability of each city in a variable called \"city_probability\" in each one of the dictionary\n",
    "        features_probability_boston[\"city_count\"] = count_boston\n",
    "        features_probability_boston[\"city_probability\"] = count_boston / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "        features_probability_canberra[\"city_count\"] = count_canberra\n",
    "        features_probability_canberra[\"city_probability\"] = count_canberra / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "        features_probability_geneva[\"city_count\"] = count_geneva\n",
    "        features_probability_geneva[\"city_probability\"] = count_geneva / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "        features_probability_ottawa[\"city_count\"] = count_ottowa\n",
    "        features_probability_ottawa[\"city_probability\"] = count_ottowa / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "                \n",
    "        return features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa # return all dictionaries\n",
    "\n",
    "    def clean_text_data(self, x): # Helper function to make a list of lists of words \n",
    "        # Take text remove all capitalized letters, removed special characters and make an array of words.\n",
    "        cleaned_data = [\n",
    "            re.sub(r'[^a-z0-9\\s]', '', text.lower()).split()\n",
    "            for text in x\n",
    "        ]\n",
    "        print(\"This is the cleaned data\", cleaned_data[0])\n",
    "        return cleaned_data # return a list of lists of words (better to use lists for this since numpy is mostly for numerical values)\n",
    "    \n",
    "    def add_probability(self, city_dict, x): # Helper function to update probabilities given a dict and a list of words\n",
    "        for word in set(x): # Creates a set from words(unique elements)\n",
    "            if word in city_dict:\n",
    "                city_dict[word] += 1\n",
    "                \n",
    "    \n",
    "    def predict(self, features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa, x_i): # (Is x_i in formula equal to 1?)\n",
    "        # Initialize probabilities for each subreddit\n",
    "        prob_boston = features_probability_boston[\"city_probability\"]\n",
    "        prob_canberra = features_probability_canberra[\"city_probability\"]\n",
    "        prob_geneva = features_probability_geneva[\"city_probability\"]\n",
    "        prob_ottowa = features_probability_ottawa[\"city_probability\"]\n",
    "\n",
    "        for word in x_i:\n",
    "            if word in features_probability_boston: # All have the same most common words\n",
    "                # Laplace smoothing\n",
    "                prob_boston = prob_boston * ((features_probability_boston[word] + 1) / (features_probability_boston[\"city_count\"] + 2))\n",
    "                prob_canberra = prob_canberra * ((features_probability_canberra[word] + 1) / (features_probability_canberra[\"city_count\"] + 2))\n",
    "                prob_geneva = prob_geneva * ((features_probability_geneva[word] + 1) / (features_probability_geneva[\"city_count\"] + 2))\n",
    "                prob_ottowa = prob_ottowa * ((features_probability_ottawa[word] + 1) / (features_probability_ottawa[\"city_count\"] + 2))\n",
    "        \n",
    "        probabilities = np.array([prob_boston, prob_canberra, prob_geneva, prob_ottowa])\n",
    "        \n",
    "        return np.argmax(probabilities)\n",
    "\n",
    "    def accu_eval(self, x, y): # Issy\n",
    "        # Validation\n",
    "        # Returns Accuracy = 1 - Error\n",
    "\n",
    "        num_correct_labels = 0\n",
    "\n",
    "        # Get probabilities / train model\n",
    "        features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa = self.calc_probability(x,y)\n",
    "\n",
    "        # Predict\n",
    "        for i in range(len(x)):\n",
    "            predicted_label = self.predict(features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa, x[i])\n",
    "            if predicted_label == y[i]:\n",
    "                num_correct_labels += 1\n",
    "\n",
    "        # Get accuracy\n",
    "        accuracy = num_correct_labels/len(y)\n",
    "        return accuracy\n",
    "\n",
    "    def crossValidation(self, k): # Issy (PS: I think we are allowed to use the method from sklearn)\n",
    "        kf = KFold(n_splits=k, shuffle=True, random_state=9)\n",
    "        accuracies = []\n",
    "\n",
    "        # Split into train and validation sets\n",
    "        for train_indices, val_indices in kf.split(self.x_all):\n",
    "            x_train = [self.x_all[i] for i in train_indices] # separate x into training subset\n",
    "            x_val = [self.x_all[i] for i in val_indices] # separate x into validating subset\n",
    "\n",
    "            y_train = [self.y_all[i] for i in train_indices] # separate y into training subset\n",
    "            y_val = [self.y_all[i] for i in val_indices] # separate y into validating subset\n",
    "\n",
    "            \n",
    "            # For each set, get probabilities / train with training set\n",
    "            features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa = self.calc_probability(x_train,np.array(y_train))\n",
    "\n",
    "            # check with validation subset\n",
    "            num_correct_labels = 0\n",
    "            for i in range(len(y_val)):\n",
    "                predicted_label = self.predict(features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa, x_val[i])\n",
    "                if predicted_label == y_val[i]:\n",
    "                    num_correct_labels += 1\n",
    "        \n",
    "            # calculate accuracy\n",
    "            accuracy = num_correct_labels / len(y_val)\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "        self.folds_accuracy = accuracies\n",
    "        self.avg_accuracy = np.mean(accuracies)\n",
    "        return self.avg_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes set up\n",
    "naiveBayes = NaiveBayes(X.to_numpy().flatten(), y.to_numpy(), feature_vectoriser=naiveBayes_uni)\n",
    "\n",
    "accuracy = naiveBayes.crossValidation(28)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 11 candidates, totalling 330 fits\n",
      "Mean Accuracy: 0.7166\n"
     ]
    }
   ],
   "source": [
    "# Compare with Naive Bayes model from sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "param_grid_NB_1 = {'alpha': np.arange(0.01, 1.11, 0.1)}\n",
    "\n",
    "NB = hyperparamaterTunning(X_uni_arr[0], param_grid_NB_1 ,folds, MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Logistic Regression Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_logModel_1 = [\n",
    "    {\"penalty\":[\"elasticnet\"],\n",
    "     \"l1_ratio\": np.arange(0, 1.2, 0.2), # 0 is only l2 penalty, 1 is only l1 penalty\n",
    "     \"solver\":[\"saga\"],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "logModel_tunned_1a = hyperparamaterTunning(X_uni, param_grid_logModel_1, folds, LogisticRegression(fit_intercept=True))\n",
    "logModel_tunned_1b = hyperparamaterTunning(X_uni_bi, param_grid_logModel_1, folds, LogisticRegression(fit_intercept=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 80 folds for each of 12 candidates, totalling 960 fits\n",
      "\n",
      "Best Parameters: {'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs', 'tol': 0.0001}\n",
      "Cross-validation Accuracies: [0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.7777777777777778, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.8333333333333334, 0.6111111111111112, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.6111111111111112, 0.8888888888888888, 0.8888888888888888, 0.6111111111111112, 0.5555555555555556, 0.8888888888888888, 0.7777777777777778, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.5294117647058824, 0.7058823529411765, 0.5882352941176471, 0.6470588235294118, 0.9411764705882353, 0.7058823529411765, 0.7647058823529411, 0.8823529411764706, 0.6470588235294118, 0.6470588235294118, 0.7058823529411765, 0.7647058823529411, 0.7647058823529411, 0.7058823529411765, 0.6470588235294118, 0.5294117647058824, 0.8235294117647058, 0.6470588235294118, 0.7058823529411765, 0.8823529411764706, 0.7647058823529411, 0.5882352941176471, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.6470588235294118, 0.6470588235294118, 0.7647058823529411, 0.5882352941176471, 0.7058823529411765, 0.7058823529411765, 0.7647058823529411, 0.8235294117647058, 0.8235294117647058, 0.6470588235294118, 0.7647058823529411, 0.7647058823529411]\n",
      "Mean Accuracy: 0.7290\n",
      "Fitting 80 folds for each of 12 candidates, totalling 960 fits\n",
      "\n",
      "Best Parameters: {'max_iter': 1000, 'penalty': 'l2', 'solver': 'sag', 'tol': 0.0001}\n",
      "Cross-validation Accuracies: [0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.7777777777777778, 0.6666666666666666, 0.6111111111111112, 0.7222222222222222, 0.8333333333333334, 0.6111111111111112, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.6666666666666666, 0.6111111111111112, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.8888888888888888, 0.9444444444444444, 0.6111111111111112, 0.6111111111111112, 0.9444444444444444, 0.7222222222222222, 0.5882352941176471, 0.7647058823529411, 0.7058823529411765, 0.5294117647058824, 0.6470588235294118, 0.5882352941176471, 0.6470588235294118, 0.9411764705882353, 0.7058823529411765, 0.7647058823529411, 0.8823529411764706, 0.6470588235294118, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.7647058823529411, 0.6470588235294118, 0.7058823529411765, 0.4117647058823529, 0.7647058823529411, 0.6470588235294118, 0.8235294117647058, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.8235294117647058, 0.5882352941176471, 0.7058823529411765, 0.7647058823529411, 0.7647058823529411, 0.8235294117647058, 0.9411764705882353, 0.6470588235294118, 0.7647058823529411, 0.5882352941176471]\n",
      "Mean Accuracy: 0.7304\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_logModel_2 = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"solver\":[\"sag\",\"lbfgs\",\"newton-cg\"],\n",
    "     \"tol\":[1e-4,1e-5],\n",
    "     \"max_iter\": [1000,2000]\n",
    "     }]\n",
    "logModel_tunned_2a = hyperparamaterTunning(X_uni, param_grid_logModel_2, folds, LogisticRegression(fit_intercept=True))\n",
    "\n",
    "logModel_tunned_2b = hyperparamaterTunning(X_uni_bi, param_grid_logModel_2, folds, LogisticRegression(fit_intercept=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SVM Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "param_grid_SVC_1 = [\n",
    "    {\"penalty\":[\"l1\",\"l2\"],\n",
    "     \"C\": np.arange(0.1,1.1,0.1).tolist(),\n",
    "     \"loss\": [\"squared_hinge\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "SVMModel_tunned_1a = hyperparamaterTunning(X_uni, param_grid_SVC_1, 14, LinearSVC(fit_intercept=True))\n",
    "\n",
    "SVMModel_tunned_1b = hyperparamaterTunning(X_uni_bi, param_grid_SVC_1, 14, LinearSVC(fit_intercept=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 10 candidates, totalling 300 fits\n",
      "Mean Accuracy: 0.7314\n",
      "Fitting 30 folds for each of 10 candidates, totalling 300 fits\n",
      "Mean Accuracy: 0.7208\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "param_grid_SVC_2 = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"C\": np.arange(0.1,1.1,0.1).tolist(),\n",
    "     \"loss\": [\"hinge\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "# X_uni --> only discarded term that are smaller than 2\n",
    "\n",
    "# MAX VALUE: 0.7415 -> Best Parameters:  {'C': 0.7000000000000001, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.0001} using X1_uni_bi and 22 folds (lemmatized words, min_df = 1 and stop_words_custom)\n",
    "meaninfullWords(X_uni)\n",
    "SVMModel_tunned_2a = hyperparamaterTunning(X_uni, param_grid_SVC_2, 30, LinearSVC(fit_intercept=True)) # Best one so far\n",
    "SVMModel_tunned_2b = hyperparamaterTunning(X_uni_bi, param_grid_SVC_2, 30, LinearSVC(fit_intercept=True)) # Best one so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "Fitting 22 folds for each of 10 candidates, totalling 220 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27244\\3927300544.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Index:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m22\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mSVMModel_tunned_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyperparamaterTunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_uni_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid_SVC_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27244\\3564071129.py\u001b[0m in \u001b[0;36mhyperparamaterTunning\u001b[1;34m(X, param, folds, model, verbose_val, Y)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel_gridSearch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfolds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# According to doc the data will be split the same way accross all calls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mmodel_best_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_gridSearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mcv_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_gridSearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frank\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frank\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frank\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     )\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frank\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frank\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frank\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frank\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frank\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frank\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frank\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frank\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frank\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    678\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frank\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n\u001b[0m\u001b[0;32m    258\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frank\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m     \u001b[0msolver_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[0m\u001b[0;32m   1187\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "param_grid_SVC_2 = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"C\": np.arange(0.1,1.1,0.1).tolist(),\n",
    "     \"loss\": [\"hinge\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [10000]\n",
    "     }]\n",
    "SVMModel_tunned_2 = []\n",
    "for i in range(len(X_uni_arr)):\n",
    "    print(\"Index:\", i)\n",
    "    for j in [22]:\n",
    "        SVMModel_tunned_2.append(hyperparamaterTunning(X_uni_arr[i], param_grid_SVC_2, j, LinearSVC(fit_intercept=True)))\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1237\n",
      "163\n",
      "163\n",
      "Unigram (size: 470)\n",
      "Unigram (size: 9125)\n",
      "Fitting 30 folds for each of 10 candidates, totalling 300 fits\n",
      "\n",
      "Best Parameters: {'C': 0.8, 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "Cross-validation Accuracies: [0.5952380952380952, 0.5476190476190477, 0.6190476190476191, 0.6190476190476191, 0.5714285714285714, 0.6904761904761905, 0.6904761904761905, 0.6097560975609756, 0.6585365853658537, 0.5853658536585366, 0.6585365853658537, 0.7317073170731707, 0.7073170731707317, 0.6097560975609756, 0.6829268292682927, 0.7073170731707317, 0.6829268292682927, 0.5365853658536586, 0.6097560975609756, 0.7804878048780488, 0.5609756097560976, 0.6341463414634146, 0.5853658536585366, 0.6585365853658537, 0.5609756097560976, 0.6341463414634146, 0.6097560975609756, 0.6097560975609756, 0.6585365853658537, 0.6341463414634146]\n",
      "Mean Accuracy: 0.6347\n",
      "Fitting 30 folds for each of 10 candidates, totalling 300 fits\n",
      "\n",
      "Best Parameters: {'C': 0.6, 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "Cross-validation Accuracies: [0.7021276595744681, 0.6382978723404256, 0.6808510638297872, 0.7659574468085106, 0.6595744680851063, 0.6595744680851063, 0.723404255319149, 0.7021276595744681, 0.8085106382978723, 0.723404255319149, 0.723404255319149, 0.7872340425531915, 0.7446808510638298, 0.851063829787234, 0.6595744680851063, 0.7446808510638298, 0.5957446808510638, 0.6170212765957447, 0.7446808510638298, 0.6808510638297872, 0.782608695652174, 0.6956521739130435, 0.717391304347826, 0.6956521739130435, 0.7391304347826086, 0.6739130434782609, 0.7608695652173914, 0.6739130434782609, 0.7391304347826086, 0.782608695652174]\n",
      "Mean Accuracy: 0.7158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=30, estimator=LinearSVC(),\n",
       "             param_grid=[{'C': [0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6,\n",
       "                                0.7000000000000001, 0.8, 0.9, 1.0],\n",
       "                          'loss': ['squared_hinge'], 'max_iter': [10000],\n",
       "                          'penalty': ['l2'], 'tol': [0.0001]}],\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_SVC_2 = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"C\": np.arange(0.1,1.1,0.1).tolist(),\n",
    "     \"loss\": [\"squared_hinge\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [10000]\n",
    "     }]\n",
    "X1,Y1,X2,Y2 = meaninfullWords(pp_df_train_1,y,meaningfull_words_list)\n",
    "\n",
    "#hyperparamaterTunning(X_uni_primary, param_grid_SVC_2, 14, LinearSVC(fit_intercept=True),Y1)\n",
    "print(len(Y1))\n",
    "print(len(Y2))\n",
    "print(len(X2))\n",
    "\n",
    "tfidf_primary = TfidfVectorizer(vocabulary=meaningfull_words_list,sublinear_tf=True)\n",
    "X_uni_primary = tfidf_primary.fit_transform(X1)\n",
    "tfidf_secondary = TfidfVectorizer(ngram_range=(1, 1),sublinear_tf=True, stop_words=stop_words_custom)\n",
    "X_uni_secondary = tfidf_secondary.fit_transform(pp_df_train_1)\n",
    "print(\"Unigram\", \"(size:\",str(len(tfidf_primary.get_feature_names_out()))+\")\")\n",
    "print(\"Unigram\", \"(size:\",str(len(tfidf_secondary.get_feature_names_out()))+\")\")\n",
    "\n",
    "\n",
    "hyperparamaterTunning(X_uni_primary, param_grid_SVC_2, 30, LinearSVC(fit_intercept=True),Y=Y1)\n",
    "hyperparamaterTunning(X_uni_secondary, param_grid_SVC_2, 30, LinearSVC(fit_intercept=True))\n",
    "#for i in range(len(X_uni_bi_arr)):\n",
    "#    print(\"Index:\", i)\n",
    "#    for j in [14, 30]:\n",
    "#        hyperparamaterTunning(X_uni_bi_arr[i], param_grid_SVC_2, j, LinearSVC(fit_intercept=True))\n",
    "#        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# LOOK INTO THIS OR ELSE DELETE\n",
    "param_grid_SDG = [\n",
    "    {\"penalty\":[\"l1\",\"l2\"],\n",
    "     \"loss\": [\"hinge\", \"log_loss\", \"modified_huber\", \"squared_hinge\", \"perceptron\", \"squared_error\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "param_grid_SDG = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"alpha\":[1e-3],\n",
    "     \"loss\": [\"hinge\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "# SDGModel = hyperparamaterTunning(X_uni_bi, param_grid_SDG, folds, SGDClassifier(fit_intercept=True),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forest Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 80 folds for each of 6 candidates, totalling 480 fits\n",
      "\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 18, 'max_features': 'sqrt'}\n",
      "Cross-validation Accuracies: [0.7777777777777778, 0.5, 0.6666666666666666, 0.5555555555555556, 0.5, 0.6666666666666666, 0.5555555555555556, 0.6666666666666666, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.5, 0.5, 0.7222222222222222, 0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.6666666666666666, 0.8888888888888888, 0.8333333333333334, 0.5555555555555556, 0.8333333333333334, 0.6111111111111112, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.7222222222222222, 0.5882352941176471, 0.5294117647058824, 0.47058823529411764, 0.5882352941176471, 0.6470588235294118, 0.5882352941176471, 0.6470588235294118, 0.8235294117647058, 0.6470588235294118, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.5882352941176471, 0.5882352941176471, 0.5882352941176471, 0.47058823529411764, 0.4117647058823529, 0.5882352941176471, 0.4117647058823529, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.5882352941176471, 0.7647058823529411, 0.6470588235294118, 0.5882352941176471, 0.5882352941176471, 0.8235294117647058, 0.5882352941176471, 0.7058823529411765, 0.4117647058823529, 0.7058823529411765, 0.5882352941176471, 0.6470588235294118, 0.7647058823529411, 0.5294117647058824, 0.8235294117647058]\n",
      "Mean Accuracy: 0.6683\n",
      "Fitting 80 folds for each of 6 candidates, totalling 480 fits\n",
      "\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 18, 'max_features': 'sqrt'}\n",
      "Cross-validation Accuracies: [0.6666666666666666, 0.5555555555555556, 0.5555555555555556, 0.6666666666666666, 0.5, 0.6666666666666666, 0.5555555555555556, 0.7222222222222222, 0.7777777777777778, 0.6111111111111112, 0.6666666666666666, 0.5555555555555556, 0.5555555555555556, 0.6111111111111112, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.6111111111111112, 0.7222222222222222, 0.6111111111111112, 0.6666666666666666, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.6111111111111112, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.5555555555555556, 0.7777777777777778, 0.7777777777777778, 0.6470588235294118, 0.5882352941176471, 0.6470588235294118, 0.5882352941176471, 0.6470588235294118, 0.5882352941176471, 0.5882352941176471, 0.7647058823529411, 0.7058823529411765, 0.8235294117647058, 0.7058823529411765, 0.5294117647058824, 0.5294117647058824, 0.6470588235294118, 0.5294117647058824, 0.8235294117647058, 0.47058823529411764, 0.5882352941176471, 0.5882352941176471, 0.6470588235294118, 0.6470588235294118, 0.5882352941176471, 0.5882352941176471, 0.5882352941176471, 0.5882352941176471, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.6470588235294118, 0.5882352941176471, 0.7647058823529411, 0.6470588235294118, 0.6470588235294118, 0.47058823529411764, 0.8235294117647058, 0.5294117647058824, 0.5294117647058824, 0.5882352941176471, 0.6470588235294118, 0.8823529411764706]\n",
      "Mean Accuracy: 0.6656\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid_rf = [{\n",
    "\"criterion\":[\"gini\", \"entropy\", \"log_loss\"],\n",
    "\"max_features\":[\"sqrt\", \"log2\"],\n",
    "\"max_depth\": [18] # Need to look into what values to use here\n",
    "}]\n",
    "rF = hyperparamaterTunning(X_uni, param_grid_rf, folds, RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Final Model<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 0 1 3 1 0 1 1 1 3 1 3 1 0 0 3 1 1 1 0 1 1 1 1 1 1 1 2 3 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 1 2 1 0 1 0 1 0 1 2 3 3 0 1 1 1 0 1 1 1 1 1 0 1 1 1 3 1 0 1\n",
      " 1 2 3 0 1 1 1 1 2 1 3 1 0 1 2 1 3 1 1 1 1 1 3 2 1 2 1 2 1 1 3 3 1 1 1 1 1\n",
      " 2 2 1 1 1 1 2 0 1 3 1 2 1 1 0 1 0 1 1 3 0 2 1 2 2 3 1 1 1 1 1 3 1 3 1 1 3\n",
      " 1 2 3 3 0 2 3 1 3 2 0 3 2 3 2 1 1 3 0 2 3 1 3 3 2 2 2 3 1 2 1 3 2 3 3 3 3\n",
      " 2 3 3 3 1 3 3 3 0 0 3 3 2 3 3 3 3 3 3 2 0 3 3 3 0 3 3 1 0 3 3 3 3 3 2 3 3\n",
      " 2 1 3 3 1 3 1 0 3 3 3 0 0 3 2 3 3 1 3 1 3 3 3 1 0 3 1 3 1 3 3 3 2 3 3 0 3\n",
      " 3 3 3 3 3 0 3 3 3 1 3 3 1 3 3 3 1 2 3 0 1 3 3 2 3 1 2 2 3 3 3 1 2 3 3 0 0\n",
      " 3 1 3 3 0 1 0 3 3 0 0 0 3 0 0 2 2 1 3 1 3 0 3 1 3 2 0 0 2 0 0 0 1 0 1 0 0\n",
      " 3 0 0 0 0 0 1 0 0 3 3 0 2 0 0 0 0 0 0 3 0 0 0 0 0 0 0 3 0 0 0 3 0 0 2 0 0\n",
      " 3 0 0 0 3 0 3 1 0 3 0 0 0 0 0 1 3 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 3\n",
      " 0 1 0 0 2 0 2 0 0 0 0 2 0 0 0 0 0 3 3 0 0 0 3 0 0 3 2 0 3 0 0 3 3 0 3 0 2\n",
      " 0 0 0 2 0 0 2 0 3 0 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 2 2 2 3 2\n",
      " 2 2 2 3 2 2 0 2 2 2 1 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2\n",
      " 2 2 3 2 2 1 2 2 2 2 2 2 2 2 2 2 2 3 2 1 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 1 0 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 3 2 2 2\n",
      " 2 2 2 0 3 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "pp_df_test = preprocessing_data(df_test[\"body\"],[])\n",
    "X_test = tfidf_uni_arr[0].transform(pp_df_test)\n",
    "#Test\n",
    "\n",
    "y_pred = SVMModel_tunned_2[0].predict(X_test)\n",
    "\n",
    "\n",
    "''' \n",
    "if (post has meaningfull Vocabulary):\n",
    "    Run Model with meaningfull words as features only\n",
    "else:\n",
    "    Run Model with all words as features\n",
    "'''\n",
    "\n",
    "print(y_pred)\n",
    "with open(\"output4.csv\", mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write a header (optional, if you want)\n",
    "    writer.writerow([\"id\",\"subreddit\"])\n",
    "    map = [\"Boston\",\"Canberra\",\"Geneva\",\"Ottawa\"]\n",
    "    # Write the features from the array\n",
    "    for i in range(len(y_pred)):\n",
    "        output = y_pred[i]\n",
    "        writer.writerow([i, map[output]])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
