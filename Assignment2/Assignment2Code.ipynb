{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "# Group 13\n",
    "Mathieu Mailhot - Isabel Lougheed - Frank-Lucas Pantazis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "## Checkout link: https://www.analyticsvidhya.com/blog/2021/11/a-guide-to-building-an-end-to-end-multiclass-text-classification-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "folds = 5 # between 5 and 10\n",
    "\n",
    "# Loading Training data\n",
    "df_train = pd.read_csv('train.csv', encoding='utf-8', encoding_errors='ignore') # errors were not pertinent characters\n",
    "df_train[\"subreddit\"] = df_train[\"subreddit\"].map({\"Boston\": 0, \"Canberra\": 1,\"Geneva\":2,\"Ottawa\":3})\n",
    "\n",
    "y = df_train[\"subreddit\"]\n",
    "X = df_train.drop(\"subreddit\",axis=1)\n",
    "\n",
    "# Loading Test Data\n",
    "df_test = pd.read_csv('test.csv', encoding='utf-8', encoding_errors='ignore') # errors were not pertinent characters\n",
    "X_test = df_test[\"body\"] # Not what we should do with the ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram (size: 3000)\n",
      " ['00' '000' '01' ... 'zone' 'zoo' 'zurich'] \n",
      "\n",
      "Unigram & Bigram (size: 3000)\n",
      " ['00' '000' '01' ... 'zero' 'zone' 'zoo'] \n",
      "\n",
      "Bigram (size: 49610)\n",
      " ['00 33' '00 avec' '00 bit' ... 'zurich migrations' 'zurich tried'\n",
      " 'zxwr7mvro1z3kabb year']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Observations\n",
    "# - bigram -> worse performance\n",
    "# - sublinear_tf -> seems to improve accuracy\n",
    "# - decreasing max_features -> seems to decrease accuracy (feature reduction)\n",
    "\n",
    "# TODO\n",
    "# - Create custom stop word list since default one might not be suited for our case according to documentation: https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n",
    "# - explore different ways to extract features from text data\n",
    "\n",
    "tfidf_uni = TfidfVectorizer(ngram_range=(1, 1), sublinear_tf=True, stop_words='english')\n",
    "tfidf_uni_bi = TfidfVectorizer(ngram_range=(1, 2),sublinear_tf=True, stop_words='english')\n",
    "tfidf_bi = TfidfVectorizer(ngram_range=(2, 2),  stop_words='english')\n",
    "\n",
    "X_uni = tfidf_uni.fit_transform(df_train[\"body\"]).toarray()\n",
    "X_uni_bi = tfidf_uni_bi.fit_transform(df_train[\"body\"]).toarray()\n",
    "X_bi = tfidf_bi.fit_transform(df_train[\"body\"]).toarray()\n",
    "\n",
    "print(\"Unigram\", \"(size:\",str(len(tfidf_uni.get_feature_names_out()))+\")\\n\",tfidf_uni.get_feature_names_out(),\"\\n\")\n",
    "print(\"Unigram & Bigram\", \"(size:\",str(len(tfidf_uni_bi.get_feature_names_out()))+\")\\n\",tfidf_uni_bi.get_feature_names_out(),\"\\n\")\n",
    "print(\"Bigram\", \"(size:\",str(len(tfidf_bi.get_feature_names_out()))+\")\\n\", tfidf_bi.get_feature_names_out())\n",
    "\n",
    "# To get a better idea of the extracted features\n",
    "with open(\"features.csv\", mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write a header (optional, if you want)\n",
    "    writer.writerow([\"Feature Name\"])\n",
    "    # Write the features from the array\n",
    "    for feature in tfidf_uni.get_feature_names_out():\n",
    "        writer.writerow([feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparamater Optimisation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# This function does all the tunning for each model\n",
    "def hyperparamaterTunning(X, param, folds, model):\n",
    "    \n",
    "    model_gridSearch = GridSearchCV(model, param_grid=param,cv=folds, verbose=True) # According to doc the data will be split the same way accross all calls\n",
    "\n",
    "    model_best_clf = model_gridSearch.fit(X,y)\n",
    "\n",
    "    print(\"Best Parameters:\", model_best_clf.best_params_)\n",
    "\n",
    "    print(\"Accuracy:\", model_best_clf.best_score_)\n",
    "\n",
    "    return model_best_clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Naive Bayes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naiveBayes:\n",
    "    def __init__(self, x_all, y_all):\n",
    "        self.x_all = x_all\n",
    "        self.y_all = y_all\n",
    "        self.features_probability = dict()\n",
    "\n",
    "        self.folds_features_probability = 0 # array of dict\n",
    "        self.folds_accuracy = 0\n",
    "        self.avg_accuracy = 0\n",
    "\n",
    "    \n",
    "    def calc_probability(self): # Train/Fit # Mathieu\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x_i): # Issy\n",
    "        pass # return y (0,1,2,3)\n",
    "\n",
    "    def accu_eval(self, x, y): # Issy\n",
    "        pass\n",
    "\n",
    "    def crossValidation(self, k): # Issy (PS: I think we are allowed to use the method from sklearn)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Logistic Regression Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_logModel_1 = [\n",
    "    {\"penalty\":[\"elasticnet\"],\n",
    "     \"l1_ratio\": np.arange(0, 1.2, 0.2), # 0 is only l2 penalty, 1 is only l1 penalty\n",
    "     \"solver\":[\"saga\"],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "logModel_tunned_1 = hyperparamaterTunning(X_uni, param_grid_logModel_1, folds, LogisticRegression(fit_intercept=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Parameters: {'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs', 'tol': 0.0001}\n",
      "Accuracy: 0.6971428571428572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_logModel_2 = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"solver\":[\"sag\",\"lbfgs\",\"newton-cg\"],\n",
    "     \"tol\":[1e-4,1e-5],\n",
    "     \"max_iter\": [1000,2000]\n",
    "     }]\n",
    "logModel_tunned_2 = hyperparamaterTunning(X_uni, param_grid_logModel_2, folds, LogisticRegression(fit_intercept=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Linear SVC Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best Parameters: {'loss': 'squared_hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "Accuracy: 0.5014285714285714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "param_grid_SVC_1 = [\n",
    "    {\"penalty\":[\"l1\",\"l2\"],\n",
    "     \"loss\": [\"squared_hinge\"],\n",
    "     \"tol\":[1e-4,1e-5],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "SVMModel_tunned_1 = hyperparamaterTunning(X_uni, param_grid_SVC_1, folds, LinearSVC(fit_intercept=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best Parameters: {'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.0001}\n",
      "Accuracy: 0.7028571428571428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "param_grid_SVC_2 = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"loss\": [\"hinge\",\"squared_hinge\"],\n",
    "     \"tol\":[1e-4,1e-5],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "SVMModel_tunned_2 = hyperparamaterTunning(X_uni_bi, param_grid_SVC_2, folds, LinearSVC(fit_intercept=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forest Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 17, 'max_features': 'sqrt'}\n",
      "Accuracy: 0.6635714285714285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid_rf = [{\n",
    "\"criterion\":[\"gini\", \"entropy\", \"log_loss\"],\n",
    "\"max_features\":[\"sqrt\", \"log2\"],\n",
    "\"max_depth\": range(10,20) # Need to look into what values to use here\n",
    "}]\n",
    "rF = hyperparamaterTunning(X_uni, param_grid_rf, folds, RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
