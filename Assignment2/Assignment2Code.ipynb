{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "# Group 13\n",
    "Mathieu Mailhot - Isabel Lougheed - Frank-Lucas Pantazis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "## Checkout link: https://www.analyticsvidhya.com/blog/2021/11/a-guide-to-building-an-end-to-end-multiclass-text-classification-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   body\n",
      "0     I had to put in a drain well/french drain, and...\n",
      "1     I've worked with James from Prova accountants ...\n",
      "2     https://lebonmelange.com.au/ is a Gungahlin ca...\n",
      "3     What I love about Canberra are the town planni...\n",
      "4     Canberra has a bigger issue with strata. Rates...\n",
      "...                                                 ...\n",
      "1395  Take the train to La pleine and walk to joncti...\n",
      "1396  IIL alumni here!\\n\\nGeneva private schools are...\n",
      "1397  I'm really sorry to hear about your bad experi...\n",
      "1398  They quite easy to handle. Typically, they don...\n",
      "1399  **Specialization is authentic.** You don?t go ...\n",
      "\n",
      "[1400 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "folds = 30 # between 5 and 10 # best value at the moment when folds = 30\n",
    "\n",
    "# Loading Training data\n",
    "df_train = pd.read_csv('train.csv', encoding='utf-8', encoding_errors='ignore') # errors were not pertinent characters\n",
    "df_train[\"subreddit\"] = df_train[\"subreddit\"].map({\"Boston\": 0, \"Canberra\": 1,\"Geneva\":2,\"Ottawa\":3})\n",
    "\n",
    "y = df_train[\"subreddit\"]\n",
    "X = df_train.drop(\"subreddit\",axis=1)\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Loading Test Data\n",
    "df_test = pd.read_csv('test.csv', encoding='utf-8', encoding_errors='ignore') # errors were not pertinent characters\n",
    "X_test = df_test[\"body\"] # Not what we should do with the ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different stop word libraries\n",
    "\n",
    "# Checkout: https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram (size: 5361)\n",
      "Unigram & Bigram (size: 8356)\n",
      "Bigram (size: 2346)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "# Observations\n",
    "# - bigram -> worse performance\n",
    "# - sublinear_tf -> seems to improve accuracy\n",
    "# - decreasing max_features -> seems to decrease accuracy (feature reduction)\n",
    "\n",
    "# TODO\n",
    "# - Create custom stop word list since default one might not be suited for our case according to documentation: https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n",
    "# - explore different ways to extract features from text data\n",
    "stop_words = {\"like\"}\n",
    "\n",
    "# Instantiate Vectorizer\n",
    "tfidf_uni = TfidfVectorizer(ngram_range=(1, 1), sublinear_tf=True, min_df=2, stop_words='english')\n",
    "tfidf_uni_bi = TfidfVectorizer(ngram_range=(1, 3),sublinear_tf=True, min_df=2, stop_words='english')\n",
    "tfidf_bi = TfidfVectorizer(ngram_range=(2, 2), sublinear_tf=True,min_df=2, stop_words='english')\n",
    "naiveBayes_uni = CountVectorizer(max_features=3000, ngram_range=(1, 1), stop_words='english')\n",
    "\n",
    "# Fit Vectorizer from data\n",
    "X_uni = tfidf_uni.fit_transform(df_train[\"body\"]).toarray()\n",
    "X_uni_bi = tfidf_uni_bi.fit_transform(df_train[\"body\"]).toarray()\n",
    "X_bi = tfidf_bi.fit_transform(df_train[\"body\"]).toarray()\n",
    "X_naive_bayes = naiveBayes_uni.fit_transform(df_train[\"body\"]).toarray()\n",
    "\n",
    "print(\"Unigram\", \"(size:\",str(len(tfidf_uni.get_feature_names_out()))+\")\")\n",
    "#print(tfidf_uni.get_feature_names_out())\n",
    "print(\"Unigram & Bigram\", \"(size:\",str(len(tfidf_uni_bi.get_feature_names_out()))+\")\")\n",
    "#print(tfidf_uni_bi.get_feature_names_out())\n",
    "print(\"Bigram\", \"(size:\",str(len(tfidf_bi.get_feature_names_out()))+\")\")\n",
    "#print(tfidf_bi.get_feature_names_out())\n",
    "\n",
    "# LOOK INTO NMF AND WHAT IT CAN DO TO HELP US\n",
    "nmf = NMF(100).fit(X_uni)\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    top_features_ind = topic.argsort()[-10:]\n",
    "    top_features = tfidf_uni.get_feature_names_out()[top_features_ind]\n",
    "    print(top_features)\n",
    "    \n",
    "# To get a better idea of the extracted features\n",
    "with open(\"features.csv\", mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write a header (optional, if you want)\n",
    "    writer.writerow([\"Feature Name\"])\n",
    "    # Write the features from the array\n",
    "    for feature in tfidf_uni.get_feature_names_out():\n",
    "        writer.writerow([feature])\n",
    "\n",
    "with open(\"featuresNaiveBayes.csv\", mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write a header (optional, if you want)\n",
    "    writer.writerow([\"Feature Name\"])\n",
    "    # Write the features from the array\n",
    "    for feature in naiveBayes_uni.get_feature_names_out():\n",
    "        writer.writerow([feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draft of Feature Visualizer\n",
    "# Maybe should put all of it in an excel and then display it?\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_disp = vectorizer.fit_transform(df_train[\"body\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "Boston_counts = X_disp[y == 0].sum(axis=0).A1 # Sum occurrences for class 'Boston'\n",
    "Canberra_counts = X_disp[y == 1].sum(axis=0).A1 # Sum occurrences for class 'Canberra'\n",
    "Geneva_counts = X_disp[y == 2].sum(axis=0).A1 # Sum occurrences for class 'Geneva'\n",
    "Ottawa_counts = X_disp[y == 3].sum(axis=0).A1 # Sum occurrences for class 'Ottawa'\n",
    "\n",
    "header = [\"features\",\"Boston\",\"Canberra\",\"Geneva\",\"Ottawa\"]\n",
    "table = []\n",
    "for i in range(len(feature_names)):\n",
    "    table.append([feature_names[i],Boston_counts[i].item(),Canberra_counts[i].item(),Geneva_counts[i].item(),Ottawa_counts[i].item()])\n",
    "\n",
    "\n",
    "if (False):\n",
    "    with open(\"featureVisualiser.csv\", mode='w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # Write a header (optional, if you want)\n",
    "        writer.writerow(header)\n",
    "        # Write the features from the array\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "    print(table)\n",
    "    \n",
    "# Plot a grouped bar chart\n",
    "# y_pos = np.arange(len(feature_names))*2 # Word indices\n",
    "# width = 0.4  # Bar width\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# ax.barh(y_pos + 3*width/2, Boston_counts, width, label=\"Boston\", color='red')\n",
    "# ax.barh(y_pos + width/2, Canberra_counts, width, label=\"Canberra\", color='orange')\n",
    "# ax.barh(y_pos - width/2, Geneva_counts, width, label=\"Geneva\", color='blue')\n",
    "# ax.barh(y_pos - 3*width/2, Ottawa_counts, width, label=\"Ottawa\", color='green')\n",
    "# \n",
    "# # Formatting\n",
    "# ax.set_yticks(y_pos, labels=feature_names)\n",
    "# \n",
    "# ax.set_xlabel(\"Word Count\")\n",
    "# ax.set_title(\"Feature Appearance in Each Class\")\n",
    "# ax.legend()\n",
    "# \n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Helper Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# This function does all the tunning for each model\n",
    "def hyperparamaterTunning(X, param, folds, model, verbose_val=1):\n",
    "    \n",
    "    model_gridSearch = GridSearchCV(model, param_grid=param,cv=folds, verbose=verbose_val) # According to doc the data will be split the same way accross all calls\n",
    "\n",
    "    model_best_clf = model_gridSearch.fit(X,y)\n",
    "\n",
    "    cv_results = model_gridSearch.cv_results_\n",
    "\n",
    "    \n",
    "    print()\n",
    "\n",
    "    print(f\"Best Parameters: {model_best_clf.best_params_}\")\n",
    "    try:\n",
    "\n",
    "        best_index = model_gridSearch.best_index_\n",
    "\n",
    "        score = []\n",
    "        for fold in range(folds):\n",
    "            score.append(model_gridSearch.cv_results_[f\"split{fold}_test_score\"][best_index].item())\n",
    "\n",
    "        print(f\"Cross-validation Accuracies: {score}\")\n",
    "    except:\n",
    "        pass\n",
    "    print(f\"Mean Accuracy: {model_best_clf.best_score_:.4f}\")\n",
    "\n",
    "    return model_best_clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Naive Bayes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, x_all, y_all, feature_vectoriser):\n",
    "        self.x_all = self.clean_text_data(x_all) # Make lists of strings\n",
    "        self.y_all = y_all\n",
    "        self.feature_vectoriser = feature_vectoriser\n",
    "\n",
    "        self.folds_features_probability = 0 # array of dict\n",
    "        self.folds_accuracy = 0\n",
    "        self.avg_accuracy = 0\n",
    "\n",
    "    \n",
    "    def calc_probability(self, x, y): # Train/Fit # Mathieu\n",
    "        # Create an empty dictionnary with the 3000 most common words for each subreddit.\n",
    "        features_probability_boston = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "        features_probability_canberra = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "        features_probability_geneva = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "        features_probability_ottawa = {word: 0 for word in self.feature_vectoriser.get_feature_names_out()}\n",
    "\n",
    "        # Initialize the count for the total number of text from each subreddit\n",
    "        count_boston = 0\n",
    "        count_canberra = 0\n",
    "        count_geneva = 0\n",
    "        count_ottowa = 0\n",
    "\n",
    "        # Add 1 to the word in the dictionnary when the word is present in the text\n",
    "        for i in range(y.shape[0]):\n",
    "            if y[i] == 0:\n",
    "                count_boston += 1\n",
    "                self.add_probability(features_probability_boston, x[i])\n",
    "            if y[i] == 1:\n",
    "                count_canberra += 1\n",
    "                self.add_probability(features_probability_canberra, x[i])\n",
    "            if y[i] == 2:\n",
    "                count_geneva += 1\n",
    "                self.add_probability(features_probability_geneva, x[i])\n",
    "            else:\n",
    "                count_ottowa += 1\n",
    "                self.add_probability(features_probability_ottawa, x[i])\n",
    "\n",
    "        # Add the total count of each city to a variable called \"city_count\" and the probability of each city in a variable called \"city_probability\" in each one of the dictionary\n",
    "        features_probability_boston[\"city_count\"] = count_boston\n",
    "        features_probability_boston[\"city_probability\"] = count_boston / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "        features_probability_canberra[\"city_count\"] = count_canberra\n",
    "        features_probability_canberra[\"city_probability\"] = count_canberra / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "        features_probability_geneva[\"city_count\"] = count_geneva\n",
    "        features_probability_geneva[\"city_probability\"] = count_geneva / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "        features_probability_ottawa[\"city_count\"] = count_ottowa\n",
    "        features_probability_ottawa[\"city_probability\"] = count_ottowa / (count_boston + count_canberra + count_geneva + count_ottowa)\n",
    "                \n",
    "        return features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa # return all dictionaries\n",
    "\n",
    "    def clean_text_data(self, x): # Helper function to make a list of lists of words \n",
    "        # Take text remove all capitalized letters, removed special characters and make an array of words.\n",
    "        cleaned_data = [\n",
    "            re.sub(r'[^a-z0-9\\s]', '', text.lower()).split()\n",
    "            for text in x\n",
    "        ]\n",
    "        print(\"This is the cleaned data\", cleaned_data[0])\n",
    "        return cleaned_data # return a list of lists of words (better to use lists for this since numpy is mostly for numerical values)\n",
    "    \n",
    "    def add_probability(self, city_dict, x): # Helper function to update probabilities given a dict and a list of words\n",
    "        for word in set(x): # Creates a set from words(unique elements)\n",
    "            if word in city_dict:\n",
    "                city_dict[word] += 1\n",
    "                \n",
    "    \n",
    "    def predict(self, features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa, x_i): # (Is x_i in formula equal to 1?)\n",
    "        # Initialize probabilities for each subreddit\n",
    "        prob_boston = features_probability_boston[\"city_probability\"]\n",
    "        prob_canberra = features_probability_canberra[\"city_probability\"]\n",
    "        prob_geneva = features_probability_geneva[\"city_probability\"]\n",
    "        prob_ottowa = features_probability_ottawa[\"city_probability\"]\n",
    "\n",
    "        for word in x_i:\n",
    "            if word in features_probability_boston: # All have the same most common words\n",
    "                # Laplace smoothing\n",
    "                prob_boston = prob_boston * ((features_probability_boston[word] + 1) / (features_probability_boston[\"city_count\"] + 2))\n",
    "                prob_canberra = prob_canberra * ((features_probability_canberra[word] + 1) / (features_probability_canberra[\"city_count\"] + 2))\n",
    "                prob_geneva = prob_geneva * ((features_probability_geneva[word] + 1) / (features_probability_geneva[\"city_count\"] + 2))\n",
    "                prob_ottowa = prob_ottowa * ((features_probability_ottawa[word] + 1) / (features_probability_ottawa[\"city_count\"] + 2))\n",
    "        \n",
    "        probabilities = np.array([prob_boston, prob_canberra, prob_geneva, prob_ottowa])\n",
    "        \n",
    "        return np.argmax(probabilities)\n",
    "\n",
    "    def accu_eval(self, x, y): # Issy\n",
    "        pass\n",
    "\n",
    "    def crossValidation(self, k): # Issy (PS: I think we are allowed to use the method from sklearn)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the cleaned data ['i', 'had', 'to', 'put', 'in', 'a', 'drain', 'wellfrench', 'drain', 'and', 'the', 'ground', 'about', '6', 'inches', 'down', 'was', 'all', 'mud', 'and', 'clay', 'i', 'was', 'ass', 'over', 'end', 'in', 'this', 'hole', 'scooping', 'clay', 'mud', 'and', 'was', 'joined', 'by', 'probably', 'ten', 'of', 'these', 'mud', 'daubers', 'for', 'a', 'couple', 'hours', 'they', 'never', 'bothered', 'me', 'at', 'all', 'in', 'their', 'own', 'little', 'way', 'they', 'were', 'kinda', 'helping', 'out', 'i', 'suppose', 'theyd', 'build', 'nests', 'in', 'the', 'garage', 'where', 'i', 'workout', 'and', 'aside', 'from', 'almost', 'smacking', 'into', 'each', 'other', 'they', 'never', 'bothered', 'me', 'theyd', 'just', 'go', 'back', 'and', 'forth', 'building', 'their', 'mud', 'tubes', 'and', 'filling', 'them', 'with', 'paralyzed', 'spiders', 'i', 'think', 'they', 'helped', 'with', 'garden', 'pests', 'and', 'we', 'had', 'so', 'many', 'spiders', 'i', 'didnt', 'mind', 'them', 'culling', 'that', 'herd', 'either']\n",
      "{'00': 0, '000': 0, '01': 0, '02': 0, '03': 0, '04': 0, '10': 9, '100': 6, '1000': 0, '11': 2, '1170': 0, '12': 2, '13': 1, '14': 2, '15': 9, '16': 1, '17': 0, '18': 3, '180': 2, '19': 1, '1hm3rjz': 0, '20': 9, '200': 1, '2000': 0, '2007': 1, '2011': 2, '2019': 2, '2020': 4, '2021': 2, '2022': 2, '2023': 4, '2024': 3, '2025': 2, '2026': 1, '21': 1, '22': 2, '24': 1, '25': 1, '250': 0, '28': 0, '2ftourism': 0, '30': 2, '300': 0, '30am': 0, '33': 1, '3avisiting': 0, '3d': 0, '3k': 0, '3rd': 2, '40': 1, '400': 1, '45': 1, '50': 6, '500': 0, '569ireqj06431': 0, '60': 4, '600': 0, '70': 1, '700': 0, '7am': 2, '80': 1, '80k': 1, '8am': 0, '90': 1, '90s': 0, '911': 0, 'abc': 0, 'ability': 2, 'able': 14, 'abroad': 1, 'absolute': 2, 'absolutely': 7, 'academic': 0, 'academically': 1, 'accept': 1, 'access': 5, 'accessible': 4, 'accommodation': 0, 'according': 2, 'account': 3, 'achieve': 2, 'aclu': 3, 'act': 4, 'action': 15, 'active': 2, 'actively': 3, 'activities': 3, 'activity': 1, 'acton': 1, 'actual': 4, 'actually': 15, 'add': 7, 'added': 1, 'adding': 1, 'addition': 4, 'additional': 2, 'additionally': 3, 'address': 2, 'addressed': 0, 'adds': 1, 'adjacent': 2, 'admin': 3, 'administration': 5, 'administrative': 3, 'admission': 1, 'admitted': 2, 'adults': 2, 'advance': 1, 'advantage': 0, 'advice': 4, 'advise': 1, 'affirming': 0, 'afford': 1, 'affordable': 4, 'afin': 0, 'afraid': 0, 'afternoon': 2, 'age': 3, 'agencies': 1, 'agency': 2, 'agenda': 1, 'agent': 0, 'agents': 1, 'ages': 3, 'aggressive': 3, 'aging': 3, 'ago': 13, 'agree': 7, 'agreement': 1, 'ahead': 2, 'ai': 1, 'ainslie': 0, 'air': 1, 'airbnb': 0, 'airport': 2, 'al': 0, 'alewife': 4, 'allow': 2, 'allowed': 4, 'allowing': 3, 'allston': 5, 'alors': 0, 'alternative': 1, 'alternatively': 2, 'amazing': 2, 'amazon': 1, 'america': 5, 'american': 4, 'americans': 0, 'anglais': 0, 'angry': 5, 'animal': 0, 'animals': 1, 'annemasse': 0, 'annes': 0, 'annoyed': 0, 'annoying': 1, 'annual': 1, 'ans': 0, 'answer': 1, 'answers': 1, 'anthem': 0, 'anti': 0, 'anu': 0, 'anymore': 2, 'anyways': 0, 'apart': 2, 'apartment': 7, 'apartments': 1, 'app': 0, 'apparently': 1, 'appears': 8, 'apple': 0, 'appliances': 0, 'applicants': 0, 'application': 1, 'applications': 0, 'apply': 0, 'applying': 0, 'appointment': 1, 'appreciate': 0, 'approach': 1, 'appropriate': 1, 'approved': 1, 'approximately': 2, 'apps': 1, 'appts': 0, 'april': 3, 'arboretum': 2, 'archaeological': 0, 'area': 20, 'areas': 8, 'aren': 0, 'arlington': 3, 'arm': 4, 'arrested': 4, 'arrive': 0, 'arrow': 0, 'arrows': 0, 'art': 7, 'artefacts': 0, 'article': 5, 'article_inline_text_link': 0, 'artist': 2, 'arts': 2, 'asap': 1, 'asian': 0, 'aside': 2, 'ask': 1, 'asked': 12, 'asking': 8, 'ass': 2, 'assault': 1, 'assets': 0, 'asshole': 2, 'assist': 0, 'assistant': 1, 'assume': 2, 'assuming': 0, 'ate': 1, 'attached': 4, 'attack': 2, 'attempt': 2, 'attend': 1, 'attended': 1, 'attention': 2, 'attorney': 3, 'au': 0, 'aussi': 0, 'australia': 0, 'australian': 0, 'authentic': 2, 'authority': 2, 'auto': 0, 'automatically': 15, 'automod': 0, 'automodrateur': 0, 'autre': 0, 'aux': 0, 'availability': 0, 'available': 6, 'avant': 0, 'ave': 5, 'avec': 0, 'avenue': 0, 'average': 3, 'avoid': 4, 'avoir': 0, 'aware': 1, 'away': 18, 'awesome': 1, 'awful': 2, 'awhile': 1, 'babies': 1, 'baby': 3, 'background': 1, 'bad': 13, 'bag': 0, 'bags': 2, 'bain': 0, 'bains': 0, 'bakery': 1, 'bank': 5, 'banks': 0, 'bar': 6, 'barely': 2, 'bars': 3, 'base': 1, 'based': 6, 'basic': 3, 'basically': 7, 'basics': 0, 'bay': 3, 'bbq': 1, 'bc': 2, 'beach': 1, 'beacon': 2, 'bear': 1, 'beautiful': 2, 'bed': 3, 'bedroom': 0, 'beef': 0, 'beer': 0, 'begin': 1, 'beginners': 2, 'beginning': 1, 'behaviour': 0, 'belco': 0, 'belconnen': 0, 'believe': 3, 'believed': 1, 'bell': 0, 'belmont': 5, 'benefit': 3, 'benefits': 5, 'best': 24, 'bet': 3, 'better': 26, 'bgsearch_overlay_results': 0, 'bia': 0, 'bicycle': 0, 'bien': 0, 'big': 12, 'bigger': 4, 'biggest': 5, 'bike': 8, 'bikes': 5, 'bilingual': 1, 'billionaires': 3, 'bills': 2, 'bin': 0, 'bird': 2, 'birds': 2, 'birthday': 0, 'bishop': 0, 'bit': 15, 'black': 3, 'blame': 2, 'blanc': 0, 'blend': 0, 'block': 2, 'blocked': 0, 'blocking': 0, 'blocks': 2, 'blue': 4, 'board': 0, 'boarding': 2, 'boat': 2, 'body': 3, 'bon': 0, 'bonne': 0, 'book': 2, 'booking': 1, 'books': 0, 'border': 3, 'born': 0, 'boston': 56, 'bostonglobe': 0, 'bot': 13, 'bothered': 0, 'bought': 3, 'bowl': 5, 'bowling': 4, 'box': 2, 'boxes': 1, 'boy': 1, 'boylston': 4, 'bpl': 2, 'braddon': 0, 'brain': 0, 'branch': 4, 'brand': 0, 'brands': 0, 'brasserie': 0, 'bread': 0, 'break': 1, 'breakfast': 1, 'breaking': 5, 'breaks': 0, 'breath': 2, 'brewing': 2, 'bridge': 8, 'bridges': 2, 'bright': 2, 'bring': 3, 'bringing': 4, 'brings': 1, 'brisbane': 0, 'broad': 1, 'broke': 1, 'broken': 1, 'brookline': 6, 'brother': 0, 'brought': 1, 'brutal': 1, 'bs': 2, 'btiments': 0, 'bubble': 0, 'buddy': 1, 'budget': 6, 'build': 4, 'building': 7, 'buildings': 4, 'builds': 1, 'built': 6, 'bunch': 2, 'buns': 0, 'bureau': 0, 'burger': 1, 'burgers': 0, 'bus': 11, 'buses': 2, 'bushes': 0, 'business': 7, 'businesses': 3, 'busses': 0, 'busy': 0, 'buy': 6, 'buying': 2, 'byward': 0, 'ca': 0, 'caf': 0, 'cafe': 3, 'cake': 1, 'called': 4, 'calling': 0, 'calls': 0, 'calm': 0, 'cambridge': 17, 'came': 4, 'camera': 1, 'cameras': 1, 'campbell': 1, 'camping': 0, 'campus': 0, 'canada': 2, 'canadian': 0, 'canal': 2, 'canberra': 0, 'canberrans': 0, 'cancel': 1, 'cancer': 4, 'candlepin': 3, 'canton': 1, 'canturf': 0, 'canvas': 0, 'capital': 0, 'car': 15, 'card': 0, 'cards': 1, 'care': 12, 'career': 0, 'cares': 4, 'carleton': 0, 'carling': 0, 'carouge': 0, 'carry': 1, 'carrying': 2, 'cars': 6, 'cas': 0, 'case': 7, 'cases': 4, 'cash': 5, 'castle': 2, 'casual': 0, 'cat': 1, 'catch': 1, 'categories': 1, 'cathedral': 0, 'catholic': 4, 'cats': 1, 'cause': 4, 'caused': 3, 'causes': 2, 'cave': 0, 'cbc': 0, 'cbd': 0, 'cbp': 3, 'cd': 0, 'ce': 0, 'ceci': 0, 'ceiling': 1, 'cela': 0, 'cent': 0, 'center': 10, 'centers': 4, 'central': 5, 'centre': 0, 'centres': 0, 'centretown': 0, 'cern': 0, 'certain': 3, 'certainly': 1, 'ces': 0, 'cette': 0, 'ch': 0, 'chair': 0, 'chance': 3, 'chancellor': 0, 'chances': 2, 'change': 9, 'changed': 2, 'changes': 3, 'changing': 2, 'chaque': 0, 'charge': 5, 'charged': 2, 'charitynavigator': 0, 'charles': 7, 'chart': 1, 'chat': 2, 'cheap': 1, 'cheaper': 4, 'cheapest': 0, 'check': 17, 'checked': 1, 'checking': 1, 'cheese': 0, 'chef': 1, 'cher': 0, 'chestnut': 2, 'chf': 0, 'chicken': 2, 'child': 4, 'children': 3, 'china': 1, 'chinese': 4, 'chips': 0, 'chocolate': 1, 'choice': 3, 'choices': 0, 'choose': 0, 'chose': 2, 'christmas': 0, 'church': 3, 'churches': 1, 'cinema': 0, 'circle': 2, 'cities': 6, 'citizen': 1, 'citizens': 2, 'citizenship': 2, 'city': 27, 'civic': 0, 'civil': 3, 'claim': 1, 'class': 5, 'classes': 2, 'classic': 2, 'clean': 3, 'cleaner': 1, 'cleaning': 0, 'clear': 2, 'clearly': 3, 'climate': 3, 'clinic': 1, 'clinics': 1, 'close': 8, 'closed': 3, 'closer': 5, 'closest': 1, 'closing': 0, 'clothes': 2, 'clothing': 1, 'club': 3, 'clubs': 0, 'coalition': 3, 'coast': 3, 'coat': 3, 'cocktails': 1, 'coffee': 6, 'coin': 0, 'cold': 7, 'coles': 0, 'collection': 2, 'college': 3, 'colleges': 2, 'colours': 0, 'com': 0, 'combined': 2, 'come': 16, 'comedians': 1, 'comes': 3, 'comfortable': 1, 'coming': 11, 'comm': 3, 'comme': 0, 'comment': 0, 'comments': 1, 'commercial': 2, 'commission': 0, 'commit': 1, 'common': 10, 'commonly': 6, 'commonwealth': 2, 'communication': 2, 'communities': 5, 'community': 13, 'commute': 6, 'commuter': 4, 'commuting': 1, 'companies': 4, 'company': 6, 'comparable': 1, 'compared': 4, 'comparison': 2, 'competition': 1, 'competitive': 0, 'complain': 0, 'complaining': 2, 'complaint': 2, 'complete': 3, 'completely': 1, 'complex': 0, 'compose': 0, 'concerned': 4, 'concerns': 14, 'concerts': 0, 'concrete': 1, 'condition': 1, 'conditions': 1, 'condo': 2, 'condos': 2, 'confidence': 1, 'confirmed': 0, 'conflict': 2, 'confused': 2, 'congestion': 1, 'congrats': 2, 'connect': 0, 'connected': 0, 'connecting': 2, 'connection': 1, 'connections': 0, 'connects': 3, 'conseils': 0, 'consider': 11, 'consideration': 0, 'considered': 5, 'considering': 1, 'constantly': 1, 'constitution': 2, 'construction': 5, 'consultation': 0, 'contact': 25, 'contacted': 0, 'contemporary': 1, 'content': 0, 'context': 2, 'continue': 6, 'contract': 1, 'contracts': 2, 'contrat': 0, 'contre': 0, 'control': 1, 'convenient': 2, 'conversation': 2, 'convert': 0, 'convince': 1, 'cook': 0, 'cooked': 1, 'cool': 8, 'cop': 1, 'cops': 1, 'copy': 0, 'cornavin': 0, 'corner': 2, 'corporate': 4, 'correctly': 1, 'cortge': 0, 'cos': 0, 'cost': 9, 'costs': 6, 'couch': 0, 'cough': 1, 'coughing': 3, 'couldn': 0, 'council': 2, 'councilor': 0, 'count': 2, 'counter': 3, 'countries': 2, 'country': 8, 'countryside': 0, 'couple': 4, 'course': 6, 'courses': 0, 'court': 6, 'cover': 3, 'coverage': 0, 'covered': 3, 'covers': 1, 'covid': 4, 'coworkers': 4, 'coyotes': 0, 'cp': 0, 'crace': 0, 'crack': 1, 'crap': 2, 'crazy': 7, 'cream': 3, 'create': 4, 'created': 1, 'creating': 3, 'creative': 1, 'credit': 1, 'criminals': 4, 'crisis': 1, 'croissant': 0, 'cross': 3, 'crossing': 1, 'crossings': 0, 'crowded': 3, 'crying': 1, 'ct': 0, 'ctvnews': 0, 'cultural': 1, 'culture': 0, 'cummings': 0, 'cup': 3, 'curious': 4, 'curly': 1, 'currency': 1, 'current': 4, 'currently': 5, 'curriculum': 0, 'cursus': 0, 'custom': 1, 'customer': 0, 'customers': 0, 'cut': 5, 'cute': 1, 'cuts': 4, 'cutting': 1, 'cve': 0, 'cycle': 2, 'cycling': 0, 'cyclist': 0, 'cyclists': 2, 'da': 0, 'dad': 3, 'daily': 3, 'damn': 4, 'dangerous': 5, 'dans': 0, 'dark': 1, 'data': 2, 'date': 4, 'dates': 0, 'dating': 1, 'davis': 3, 'day': 25, 'daycare': 0, 'daycares': 0, 'days': 9, 'dead': 2, 'deal': 10, 'dealership': 1, 'dealing': 4, 'deals': 0, 'death': 2, 'debt': 1, 'decade': 2, 'decades': 2, 'december': 0, 'decent': 2, 'decide': 1, 'decided': 2, 'decision': 2, 'decisions': 1, 'decorating': 0, 'dedicated': 3, 'definitely': 6, 'degree': 1, 'degrees': 1, 'delicious': 1, 'deliver': 0, 'delivered': 1, 'delivery': 0, 'demand': 2, 'demande': 0, 'demarches': 0, 'democracy': 1, 'democratic': 3, 'democrats': 4, 'denied': 3, 'dental': 0, 'dentist': 0, 'dentists': 1, 'department': 4, 'departments': 0, 'depend': 0, 'depending': 1, 'depends': 3, 'deposit': 1, 'depuis': 0, 'des': 0, 'description': 2, 'design': 2, 'designed': 1, 'despite': 3, 'dessert': 0, 'detailed': 0, 'details': 10, 'detour': 2, 'develop': 0, 'developed': 0, 'developers': 1, 'development': 4, 'diablerets': 0, 'dickson': 0, 'did': 15, 'didn': 0, 'difference': 4, 'different': 12, 'differently': 1, 'difficult': 0, 'digital': 0, 'digitec': 0, 'dining': 1, 'dinner': 2, 'direct': 3, 'direction': 1, 'directions': 0, 'directly': 5, 'director': 1, 'dirt': 1, 'dirty': 0, 'discount': 0, 'discounted': 3, 'discrimination': 1, 'disgusting': 2, 'dish': 0, 'dishes': 0, 'display': 2, 'distance': 1, 'dj': 0, 'dm': 2, 'doctor': 3, 'doctors': 0, 'document': 1, 'documents': 0, 'does': 14, 'doesn': 0, 'dog': 1, 'dogs': 1, 'doing': 18, 'dollar': 1, 'dollars': 6, 'don': 0, 'donate': 4, 'donated': 3, 'donation': 2, 'donc': 0, 'dont': 64, 'donuts': 3, 'door': 7, 'doors': 1, 'dorchester': 4, 'douane': 0, 'double': 3, 'doubt': 2, 'downside': 0, 'downtown': 7, 'dozen': 2, 'dr': 1, 'drain': 0, 'dress': 1, 'drink': 3, 'drinking': 0, 'drinks': 1, 'drive': 11, 'driven': 3, 'driver': 0, 'drivers': 0, 'driveway': 2, 'driving': 12, 'drop': 2, 'dropped': 2, 'drove': 1, 'drug': 0, 'drugs': 0, 'dry': 2, 'du': 0, 'dude': 3, 'dunks': 3, 'duration': 1, 'ear': 1, 'earlier': 0, 'early': 4, 'easier': 3, 'easily': 5, 'east': 6, 'easy': 8, 'easyride': 0, 'eat': 1, 'eating': 1, 'eau': 0, 'eaux': 0, 'economic': 3, 'economy': 0, 'ed': 1, 'edge': 3, 'edit': 8, 'editor': 0, 'education': 3, 'educators': 1, 'effective': 1, 'effectively': 2, 'effects': 2, 'efficient': 2, 'effort': 2, 'eggs': 3, 'eh': 0, 'ein': 0, 'elaine': 0, 'election': 2, 'electric': 4, 'elevated': 1, 'elevator': 1, 'elon': 4, 'email': 1, 'embassy': 0, 'emergency': 0, 'employee': 2, 'employees': 3, 'employer': 2, 'employers': 0, 'employment': 0, 'en': 0, 'encourage': 2, 'end': 18, 'ended': 4, 'ends': 2, 'energy': 1, 'enforcement': 4, 'engagement': 1, 'england': 6, 'english': 1, 'enjoy': 16, 'enter': 0, 'entering': 2, 'entertainment': 1, 'entire': 11, 'entirely': 1, 'entre': 0, 'entry': 2, 'environ': 0, 'environment': 4, 'envoyez': 0, 'epic': 1, 'equal': 1, 'equipment': 2, 'equivalent': 3, 'es': 0, 'especially': 11, 'esplanade': 4, 'essential': 1, 'essentially': 2, 'est': 0, 'estate': 2, 'et': 0, 'eu': 0, 'euro': 0, 'europe': 2, 'european': 0, 'europeans': 0, 'evap': 0, 'eve': 0, 'evening': 1, 'event': 3, 'events': 3, 'everybody': 0, 'evidence': 2, 'ex': 0, 'exact': 1, 'exactly': 3, 'exam': 2, 'example': 6, 'examples': 1, 'excellent': 5, 'exception': 1, 'exchange': 0, 'excited': 1, 'exciting': 1, 'exemple': 0, 'exercise': 0, 'exhibit': 1, 'exist': 1, 'exit': 1, 'expats': 0, 'expect': 7, 'expected': 0, 'expenses': 3, 'expensive': 15, 'experience': 6, 'experienced': 2, 'experiences': 2, 'expertise': 1, 'explain': 3, 'explore': 5, 'express': 0, 'extended': 3, 'extra': 0, 'extreme': 1, 'extremely': 4, 'eye': 6, 'eyes': 1, 'fabric': 0, 'face': 8, 'facebook': 1, 'facilities': 3, 'fact': 6, 'factor': 2, 'factors': 0, 'factory': 1, 'failed': 2, 'failing': 2, 'fair': 3, 'fairly': 1, 'fais': 0, 'fait': 0, 'faith': 3, 'fake': 1, 'fall': 4, 'families': 4, 'family': 14, 'famous': 1, 'fan': 4, 'fancy': 1, 'faneuil': 2, 'fantastic': 1, 'faon': 0, 'far': 12, 'fare': 0, 'fares': 0, 'farm': 0, 'farmers': 1, 'fascism': 4, 'fast': 1, 'faster': 2, 'fault': 1, 'favourite': 0, 'fear': 2, 'february': 0, 'fed': 1, 'federal': 13, 'fee': 0, 'feel': 17, 'feeling': 3, 'feels': 3, 'fees': 3, 'feet': 0, 'fellow': 0, 'fells': 2, 'felt': 3, 'female': 0, 'femme': 0, 'fencing': 1, 'fens': 4, 'fenway': 6, 'ferney': 0, 'fewer': 2, 'field': 1, 'fields': 1, 'fight': 5, 'fighting': 4, 'figure': 2, 'file': 1, 'filled': 0, 'filling': 0, 'filter': 0, 'final': 0, 'finally': 2, 'finance': 0, 'financial': 1, 'finding': 2, 'finds': 0, 'fine': 10, 'finish': 2, 'finished': 0, 'fired': 1, 'fireworks': 0, 'firm': 1, 'fish': 0, 'fit': 4, 'fits': 1, 'fix': 1, 'fixed': 2, 'flair': 0, 'flat': 3, 'flexible': 1, 'flight': 6, 'flights': 1, 'floor': 2, 'flow': 3, 'flu': 6, 'fly': 3, 'flying': 0, 'focus': 4, 'focused': 0, 'folks': 6, 'follow': 1, 'followed': 3, 'following': 1, 'follows': 1, 'fondue': 0, 'food': 7, 'foods': 2, 'foot': 4, 'force': 2, 'forcing': 0, 'forecast': 1, 'foreign': 0, 'forest': 2, 'forever': 0, 'forget': 4, 'form': 1, 'format': 0, 'forms': 1, 'forward': 3, 'fr': 0, 'frais': 0, 'franais': 0, 'france': 0, 'francs': 0, 'frankly': 1, 'fraud': 0, 'free': 9, 'freedom': 3, 'freezing': 1, 'french': 3, 'frequency': 1, 'frequent': 1, 'frequently': 2, 'fresh': 5, 'friday': 3, 'fried': 1, 'friend': 5, 'friendly': 3, 'friends': 2, 'frontaliers': 0, 'frontire': 0, 'frozen': 3, 'fruit': 0, 'frustrated': 0, 'frustrating': 1, 'fuck': 5, 'fucking': 5, 'fuel': 1, 'fully': 2, 'fun': 8, 'function': 1, 'functioning': 1, 'fund': 2, 'funding': 6, 'funds': 3, 'funny': 0, 'furniture': 0, 'future': 0, 'fyi': 1, 'fyshwick': 0, 'gad_source': 0, 'game': 4, 'games': 2, 'gaming': 1, 'garage': 0, 'garant': 0, 'garbage': 0, 'garden': 6, 'gardens': 1, 'gare': 0, 'gas': 1, 'gatineau': 0, 'gave': 2, 'gclid': 0, 'ge': 0, 'gear': 2, 'gender': 1, 'general': 6, 'generally': 8, 'generation': 1, 'geneva': 0, 'geneve': 0, 'genevieve': 0, 'genevois': 0, 'gens': 0, 'genuinely': 4, 'genve': 0, 'george': 1, 'german': 0, 'gets': 6, 'getting': 17, 'gex': 0, 'giant': 2, 'gift': 0, 'giralang': 0, 'girl': 1, 'girlfriend': 1, 'girls': 0, 'given': 2, 'gives': 1, 'giving': 3, 'gl': 0, 'glad': 2, 'glass': 2, 'glebe': 0, 'globe': 1, 'gloves': 5, 'god': 3, 'goes': 3, 'going': 46, 'gold': 0, 'golden': 1, 'golf': 2, 'gone': 1, 'gonna': 4, 'goo': 0, 'good': 47, 'goods': 0, 'google': 8, 'gorgeous': 0, 'got': 17, 'gotten': 3, 'gov': 1, 'government': 16, 'gps': 1, 'grab': 5, 'grad': 3, 'graduate': 0, 'graduated': 2, 'graffiti': 0, 'grand': 1, 'granted': 1, 'grants': 3, 'grass': 0, 'grateful': 2, 'gravel': 1, 'great': 24, 'greater': 1, 'green': 5, 'grew': 5, 'groceries': 2, 'grocery': 1, 'ground': 4, 'grounds': 3, 'group': 12, 'groups': 9, 'grow': 0, 'growing': 1, 'grown': 1, 'growth': 2, 'guarantee': 0, 'guess': 1, 'guide': 1, 'gungahlin': 0, 'guy': 7, 'guys': 3, 'gva': 0, 'haha': 1, 'hair': 1, 'half': 4, 'hall': 5, 'hand': 1, 'handle': 1, 'hands': 5, 'hang': 1, 'happen': 6, 'happened': 6, 'happening': 3, 'happens': 6, 'happy': 7, 'hard': 12, 'harder': 3, 'hardware': 0, 'harvard': 9, 'hasn': 0, 'hassle': 1, 'hat': 5, 'hate': 8, 'haven': 1, 'having': 8, 'hayden': 1, 'head': 9, 'heading': 3, 'headlight': 0, 'health': 5, 'healthcare': 2, 'healthy': 1, 'heaps': 0, 'hear': 4, 'heard': 3, 'heart': 3, 'heat': 5, 'heating': 3, 'heavy': 1, 'heights': 0, 'held': 4, 'hell': 5, 'help': 24, 'helped': 1, 'helpful': 2, 'helping': 5, 'helps': 3, 'heritage': 0, 'hey': 3, 'hi': 1, 'hide': 2, 'high': 10, 'higher': 2, 'highest': 3, 'highly': 3, 'highschool': 0, 'highway': 1, 'hike': 1, 'hiking': 1, 'hill': 7, 'hills': 2, 'hired': 1, 'history': 7, 'hit': 6, 'hockey': 0, 'hold': 3, 'holding': 1, 'hole': 2, 'holiday': 0, 'holidays': 0, 'home': 16, 'homeless': 3, 'honest': 0, 'honestly': 5, 'hope': 7, 'hopefully': 1, 'hoping': 2, 'horrible': 0, 'horse': 1, 'hospital': 3, 'hot': 4, 'hotel': 3, 'hotels': 1, 'hour': 1, 'hours': 5, 'house': 12, 'household': 2, 'houses': 2, 'housesigma': 0, 'housing': 12, 'hrc': 0, 'hrs': 1, 'html': 0, 'http': 0, 'https': 0, 'hub': 1, 'hudson': 2, 'huge': 5, 'human': 6, 'humane': 0, 'humidity': 1, 'hundreds': 2, 'hunt': 0, 'hurt': 4, 'husband': 3, 'hut': 0, 'hybrid': 0, 'ice': 11, 'iced': 2, 'ici': 0, 'id': 18, 'idea': 7, 'ideas': 2, 'identity': 1, 'idk': 1, 'idp': 0, 'iga': 0, 'ignore': 1, 'ignored': 1, 'iirc': 0, 'il': 0, 'illegal': 6, 'illegals': 2, 'ils': 0, 'im': 53, 'image': 1, 'imagine': 5, 'immediate': 1, 'immediately': 6, 'immersion': 0, 'immigrant': 5, 'immigrants': 12, 'immigration': 5, 'imo': 1, 'impact': 4, 'impacted': 2, 'implemented': 1, 'important': 10, 'impossible': 1, 'impressed': 1, 'impression': 2, 'improved': 1, 'improvements': 2, 'improving': 0, 'impts': 0, 'inches': 3, 'incident': 2, 'include': 9, 'included': 0, 'including': 5, 'inclusive': 0, 'income': 2, 'increase': 2, 'increased': 3, 'increases': 1, 'increasing': 1, 'incredibly': 3, 'independent': 0, 'indirect': 5, 'individual': 1, 'individuals': 1, 'indonesian': 0, 'indoor': 0, 'industry': 0, 'inefficient': 1, 'inevitable': 1, 'info': 5, 'information': 8, 'infos': 0, 'infrastructure': 1, 'ingredients': 1, 'initial': 0, 'inner': 0, 'insane': 6, 'insanely': 1, 'inside': 5, 'insignificant': 0, 'inspect': 1, 'inspection': 1, 'inspired': 0, 'instagram': 2, 'install': 0, 'installation': 1, 'installed': 1, 'instead': 5, 'institute': 1, 'institution': 0, 'institutions': 2, 'insulation': 3, 'insurance': 3, 'integrated': 0, 'intel': 0, 'intended': 1, 'intentions': 2, 'interested': 1, 'interesting': 4, 'interests': 6, 'interior': 0, 'international': 3, 'internet': 0, 'internship': 0, 'intersection': 1, 'interview': 1, 'interviews': 0, 'introduce': 0, 'invasive': 1, 'inventory': 2, 'investment': 2, 'invite': 1, 'involved': 6, 'io': 0, 'ios': 0, 'iphone': 0, 'island': 5, 'isn': 0, 'isolation': 0, 'issue': 7, 'issues': 3, 'italian': 2, 'italy': 0, 'itch': 2, 'items': 1, 'jacket': 2, 'james': 1, 'jams': 0, 'jan': 2, 'january': 4, 'japanese': 0, 'jardin': 0, 'je': 0, 'jfk': 4, 'jimsdriving': 0, 'job': 8, 'jobs': 7, 'join': 0, 'joined': 1, 'joke': 1, 'jonction': 0, 'journey': 0, 'jpeg': 0, 'jpg': 0, 'july': 2, 'jump': 1, 'junction': 0, 'june': 1, 'just': 76, 'juste': 0, 'justice': 4, 'justify': 1, 'juveniles': 1, 'kanata': 0, 'keen': 1, 'keeping': 4, 'keeps': 2, 'kendall': 3, 'kenmore': 2, 'kept': 3, 'key': 2, 'kg': 0, 'kicked': 1, 'kid': 4, 'kids': 8, 'kill': 3, 'killed': 3, 'kind': 12, 'kinda': 3, 'kindergarten': 0, 'king': 1, 'kingston': 0, 'kippax': 0, 'kitchen': 1, 'knew': 1, 'know': 33, 'knowing': 3, 'knowledge': 3, 'knowledgeable': 0, 'known': 2, 'knows': 4, 'kraft': 7, 'la': 1, 'lab': 3, 'label': 1, 'labor': 0, 'lack': 2, 'lacking': 0, 'ladies': 1, 'lady': 0, 'laid': 1, 'lake': 1, 'lancy': 0, 'land': 2, 'landlord': 5, 'landsdowne': 0, 'lane': 4, 'lanes': 9, 'language': 2, 'languages': 1, 'lansdowne': 0, 'large': 1, 'largely': 1, 'larger': 3, 'late': 4, 'lately': 3, 'later': 8, 'latest': 2, 'laundry': 2, 'lausanne': 0, 'law': 10, 'lawn': 1, 'laws': 4, 'lawyer': 0, 'lazy': 0, 'le': 0, 'lead': 2, 'leadership': 1, 'leading': 3, 'leads': 1, 'learn': 4, 'learned': 1, 'learning': 0, 'lease': 2, 'leave': 7, 'leaves': 0, 'leaving': 3, 'led': 4, 'left': 8, 'legal': 8, 'les': 0, 'lesson': 0, 'lessons': 2, 'let': 10, 'letter': 1, 'letting': 1, 'level': 7, 'levels': 1, 'leverage': 1, 'lexington': 5, 'liberal': 3, 'library': 3, 'licence': 0, 'license': 0, 'life': 12, 'lifestyle': 0, 'light': 2, 'lights': 2, 'like': 88, 'liked': 1, 'likely': 12, 'likes': 0, 'limit': 1, 'limited': 3, 'line': 11, 'lines': 2, 'link': 3, 'lisez': 0, 'list': 5, 'listed': 2, 'listen': 3, 'lists': 0, 'literally': 6, 'little': 11, 'live': 19, 'lived': 7, 'lives': 9, 'living': 10, 'll': 0, 'lobster': 3, 'local': 14, 'locally': 2, 'locals': 1, 'located': 2, 'location': 5, 'locations': 2, 'logement': 0, 'logements': 0, 'lol': 7, 'london': 0, 'long': 12, 'longer': 3, 'lonsdale': 0, 'look': 25, 'looked': 3, 'looking': 16, 'looks': 9, 'loop': 1, 'lors': 0, 'lose': 5, 'losing': 4, 'loss': 2, 'lost': 2, 'lot': 22, 'lots': 8, 'loud': 0, 'louder': 1, 'loudly': 0, 'love': 7, 'lovely': 2, 'loves': 2, 'loving': 1, 'low': 5, 'lower': 4, 'loyer': 0, 'lrt': 0, 'luck': 2, 'lucky': 3, 'lunch': 0, 'luxury': 2, 'lyon': 0, 'ma': 11, 'machine': 0, 'madness': 1, 'maga': 2, 'mail': 1, 'main': 1, 'mainly': 1, 'maintain': 3, 'maintenance': 3, 'mais': 0, 'major': 6, 'majority': 1, 'majura': 0, 'make': 43, 'maker': 1, 'makes': 12, 'making': 7, 'malden': 6, 'mall': 2, 'man': 5, 'manage': 1, 'managed': 1, 'management': 2, 'manager': 1, 'managers': 0, 'manchester': 3, 'map': 4, 'maps': 6, 'march': 3, 'mark': 2, 'marked': 0, 'market': 1, 'markets': 0, 'mass': 10, 'massachusetts': 13, 'masses': 2, 'massive': 5, 'master': 1, 'maternelle': 0, 'matter': 2, 'matters': 2, 'maybe': 16, 'mayor': 1, 'mbta': 5, 'meal': 1, 'mean': 8, 'meaning': 0, 'means': 7, 'meant': 1, 'measure': 1, 'meat': 1, 'media': 5, 'median': 0, 'medical': 3, 'medicare': 0, 'mediocre': 1, 'meet': 2, 'meeting': 2, 'meetings': 2, 'meetup': 0, 'meh': 0, 'melbourne': 0, 'member': 3, 'members': 3, 'membership': 2, 'memory': 1, 'men': 1, 'mental': 4, 'mentality': 0, 'mention': 3, 'mentioned': 3, 'menu': 3, 'mess': 1, 'message': 2, 'met': 2, 'metal': 1, 'meter': 2, 'metro': 0, 'metrolinx': 0, 'mexican': 1, 'meyrin': 0, 'mfa': 3, 'mid': 0, 'middle': 10, 'midnight': 2, 'migros': 0, 'miles': 3, 'million': 5, 'millions': 3, 'min': 2, 'mind': 11, 'mini': 1, 'minimize': 1, 'minimum': 5, 'mins': 0, 'minute': 1, 'minutes': 6, 'mira': 4, 'miss': 2, 'missed': 0, 'missing': 2, 'mistake': 2, 'mit': 6, 'mix': 3, 'mixed': 0, 'mme': 0, 'mobile': 0, 'model': 1, 'models': 1, 'moderators': 13, 'modern': 1, 'modrateur': 0, 'mods': 0, 'module': 0, 'moi': 0, 'moins': 0, 'mois': 0, 'moment': 6, 'mon': 0, 'monday': 2, 'monde': 0, 'money': 16, 'monster': 1, 'mont': 0, 'montant': 0, 'month': 7, 'monthly': 0, 'months': 6, 'montreal': 0, 'montreux': 0, 'mormon': 0, 'morning': 2, 'mornings': 0, 'mountain': 3, 'mountains': 1, 'moved': 7, 'movie': 2, 'moving': 6, 'mowing': 0, 'ms': 0, 'mt': 3, 'mud': 0, 'multi': 1, 'multiple': 5, 'museum': 7, 'music': 1, 'musk': 3, 'names': 1, 'narrow': 2, 'nation': 2, 'national': 1, 'nationals': 1, 'nations': 2, 'native': 2, 'natural': 2, 'nature': 1, 'nazi': 5, 'ne': 1, 'near': 10, 'nearby': 4, 'nearly': 1, 'necessarily': 1, 'necessary': 0, 'need': 44, 'needed': 2, 'needing': 1, 'needs': 4, 'negative': 1, 'negotiated': 3, 'neighborhood': 5, 'neighborhoods': 3, 'neighbors': 8, 'neighbour': 0, 'neighbourhood': 0, 'neighbourhoods': 0, 'neighbours': 0, 'net': 3, 'network': 3, 'new': 26, 'newbury': 4, 'news': 5, 'nhl': 0, 'nice': 6, 'nicer': 1, 'nick': 0, 'night': 13, 'nightmare': 0, 'nights': 0, 'noise': 1, 'non': 1, 'noodles': 0, 'noon': 2, 'normal': 6, 'normally': 0, 'north': 13, 'note': 4, 'notice': 2, 'noticed': 1, 'noting': 0, 'nsw': 0, 'number': 11, 'numbers': 0, 'nutshells': 1, 'nyc': 3, 'obvious': 2, 'obviously': 1, 'oc': 0, 'occasional': 2, 'occasionally': 1, 'ocdsb': 0, 'odd': 1, 'offer': 5, 'offered': 0, 'offering': 2, 'offers': 1, 'office': 6, 'officer': 2, 'officers': 1, 'official': 1, 'officials': 5, 'oh': 7, 'ok': 7, 'okay': 2, 'old': 11, 'older': 2, 'ones': 7, 'ongoing': 1, 'online': 5, 'ont': 0, 'ontario': 0, 'op': 7, 'open': 15, 'opened': 1, 'opening': 0, 'operations': 3, 'opinion': 0, 'opportunities': 2, 'opportunity': 1, 'opposite': 0, 'option': 2, 'options': 5, 'orange': 3, 'order': 5, 'ordered': 0, 'org': 2, 'organisation': 0, 'organisations': 0, 'organization': 4, 'organizations': 3, 'orgs': 3, 'original': 2, 'originally': 1, 'orleans': 1, 'ottawa': 0, 'ottawacitizen': 0, 'otto': 0, 'ou': 0, 'outdoor': 1, 'outdoors': 2, 'outright': 1, 'outside': 9, 'overall': 3, 'overhead': 2, 'owned': 3, 'owner': 1, 'owners': 2, 'p1': 0, 'pa': 2, 'package': 1, 'page': 1, 'pages': 2, 'paid': 7, 'pain': 1, 'paint': 2, 'pandemic': 1, 'paper': 1, 'paperwork': 0, 'paquis': 0, 'par': 1, 'parc': 0, 'parent': 1, 'parents': 7, 'parfois': 0, 'park': 10, 'parked': 0, 'parking': 5, 'parks': 1, 'parkway': 0, 'parliament': 0, 'participants': 3, 'participate': 3, 'particular': 1, 'particularly': 2, 'partner': 2, 'partnership': 0, 'parts': 4, 'party': 4, 'pas': 0, 'pass': 1, 'passed': 1, 'passes': 1, 'passing': 1, 'passport': 2, 'past': 19, 'patch': 2, 'patek': 0, 'path': 4, 'pathology': 0, 'paths': 0, 'patient': 2, 'patients': 0, 'patrol': 0, 'pay': 11, 'payer': 0, 'paying': 5, 'payment': 2, 'pays': 1, 'pc': 0, 'pdf': 0, 'peak': 0, 'pedestrian': 0, 'people': 73, 'pepper': 1, 'peppers': 0, 'perception': 0, 'perfect': 1, 'perfectly': 1, 'performance': 1, 'performed': 13, 'performing': 1, 'period': 4, 'periods': 0, 'permits': 1, 'person': 12, 'personal': 6, 'personally': 0, 'personne': 0, 'personnel': 2, 'pets': 0, 'peut': 0, 'peux': 0, 'phone': 2, 'phones': 2, 'photo': 3, 'photos': 2, 'physical': 2, 'physically': 0, 'physio': 0, 'pic': 3, 'pick': 2, 'picked': 0, 'picking': 2, 'pickup': 1, 'picture': 2, 'pie': 0, 'piece': 1, 'pieces': 1, 'pies': 0, 'pipe': 0, 'pitch': 2, 'pizza': 2, 'pizze': 0, 'pjpg': 0, 'place': 17, 'places': 15, 'plainpalais': 0, 'plan': 3, 'plane': 3, 'planning': 4, 'plans': 4, 'plant': 1, 'plastic': 0, 'plate': 0, 'plates': 0, 'platform': 1, 'play': 3, 'playing': 2, 'pleasant': 2, 'please_note_veuillez_noter': 0, 'plenty': 5, 'plus': 4, 'png': 0, 'pocket': 1, 'point': 8, 'points': 3, 'police': 5, 'policies': 2, 'political': 3, 'politics': 2, 'pond': 5, 'pool': 3, 'poor': 3, 'poorly': 2, 'pop': 1, 'population': 3, 'pork': 1, 'port': 7, 'porter': 3, 'portion': 2, 'position': 2, 'positions': 2, 'positive': 2, 'possible': 8, 'possibly': 3, 'post': 14, 'posted': 1, 'posting': 7, 'posts': 9, 'potential': 3, 'potentially': 0, 'pour': 0, 'power': 7, 'ppl': 1, 'pquis': 0, 'practice': 0, 'pre': 0, 'premium': 2, 'prepare': 0, 'prepared': 1, 'present': 1, 'president': 6, 'pressure': 1, 'presto': 0, 'pretend': 1, 'pretty': 18, 'preview': 0, 'previous': 1, 'price': 3, 'priced': 0, 'prices': 6, 'pricing': 1, 'primary': 2, 'prior': 1, 'priority': 0, 'private': 4, 'pro': 1, 'probably': 21, 'problem': 4, 'problems': 2, 'process': 1, 'produce': 1, 'product': 1, 'products': 2, 'professional': 1, 'professionals': 0, 'professor': 2, 'profit': 0, 'program': 5, 'programs': 5, 'progress': 1, 'project': 5, 'projects': 6, 'promotion': 0, 'proper': 2, 'properly': 1, 'properties': 1, 'property': 3, 'prosecutors': 2, 'protected': 2, 'protection': 4, 'protest': 7, 'protesting': 3, 'protests': 7, 'proud': 3, 'prove': 2, 'provide': 5, 'provided': 0, 'providence': 1, 'provider': 1, 'providing': 2, 'province': 0, 'pt': 1, 'pub': 1, 'public': 19, 'publication': 0, 'pull': 1, 'pulled': 3, 'pump': 0, 'purchase': 0, 'purchasing': 1, 'purpose': 0, 'purposes': 2, 'push': 3, 'pushed': 1, 'pushing': 0, 'putting': 3, 'qu': 0, 'quality': 3, 'quand': 0, 'que': 0, 'queanbeyan': 0, 'queer': 1, 'question': 10, 'questions': 15, 'qui': 0, 'quick': 1, 'quickly': 5, 'quiet': 1, 'quincy': 3, 'quite': 3, 'quote': 1, 'radon': 2, 'rail': 4, 'rain': 1, 'rainy': 0, 'raised': 2, 'raku': 0, 'ramen': 0, 'ran': 4, 'random': 2, 'range': 0, 'rare': 0, 'rarely': 1, 'rat': 1, 'rate': 4, 'rates': 3, 'rating': 1, 'rats': 2, 'raw': 1, 'rd': 1, 'reach': 4, 'reaction': 2, 'read': 2, 'reader': 0, 'reading': 3, 'ready': 1, 'real': 11, 'realise': 0, 'realistic': 1, 'realistically': 2, 'reality': 4, 'realize': 1, 'really': 23, 'reason': 5, 'reasonable': 0, 'reasons': 2, 'rebel': 1, 'receive': 4, 'received': 2, 'recent': 3, 'recently': 3, 'recherche': 0, 'recognise': 0, 'recognition': 2, 'recommend': 10, 'recommendation': 0, 'recommendations': 0, 'recommended': 2, 'record': 2, 'records': 3, 'recurring': 0, 'recycling': 2, 'red': 3, 'redd': 0, 'reddit': 2, 'reduce': 1, 'reduced': 0, 'reducing': 1, 'reduction': 0, 'refer': 1, 'reference': 0, 'reflect': 1, 'refuse': 1, 'regarde': 0, 'regarding': 1, 'regardless': 2, 'regina': 0, 'region': 1, 'regional': 0, 'register': 0, 'registered': 0, 'registration': 0, 'regular': 3, 'regularly': 3, 'related': 4, 'relation': 0, 'relationship': 1, 'relative': 1, 'relatively': 1, 'relevant': 1, 'reliable': 1, 'relief': 0, 'rely': 1, 'remain': 1, 'remaining': 1, 'remains': 1, 'remember': 4, 'removal': 1, 'remove': 1, 'removed': 2, 'rent': 5, 'rental': 2, 'rentals': 2, 'renting': 2, 'rents': 2, 'repair': 2, 'repairs': 3, 'replace': 1, 'report': 1, 'reported': 3, 'reporting': 2, 'republican': 3, 'request': 1, 'require': 5, 'required': 2, 'requirements': 0, 'research': 14, 'reservation': 3, 'residential': 2, 'residents': 2, 'resolve': 0, 'resolved': 1, 'resources': 8, 'respect': 2, 'respond': 1, 'response': 3, 'rest': 3, 'restaurant': 4, 'restaurants': 7, 'restrict_sr': 0, 'restrictions': 0, 'result': 1, 'results': 2, 'resume': 1, 'resumes': 0, 'retail': 0, 'retire': 0, 'return': 4, 'returned': 3, 'reverse': 1, 'review': 1, 'reviews': 1, 'rgie': 0, 'rgies': 0, 'rich': 4, 'ride': 4, 'rideau': 0, 'ridiculous': 3, 'right': 35, 'rights': 10, 'riotact': 0, 'risk': 5, 'river': 5, 'road': 7, 'roads': 3, 'rock': 0, 'role': 2, 'roles': 0, 'roll': 3, 'roof': 0, 'room': 4, 'roommates': 4, 'rooms': 1, 'rough': 1, 'roughly': 3, 'round': 2, 'route': 3, 'routes': 1, 'row': 0, 'rue': 0, 'rugby': 1, 'rule': 2, 'rules': 3, 'run': 8, 'running': 7, 'runs': 3, 'rural': 1, 'rush': 0, 'rvision': 0, 'saconnex': 0, 'sad': 3, 'safe': 9, 'safer': 2, 'safety': 1, 'sahady': 1, 'said': 17, 'saint': 0, 'salaire': 0, 'salaries': 0, 'salary': 2, 'sales': 2, 'salesperson': 0, 'salt': 1, 'sans': 0, 'sat': 3, 'saturday': 1, 'save': 3, 'savings': 3, 'saw': 5, 'say': 14, 'saying': 4, 'says': 5, 'sbb': 0, 'scammed': 0, 'scene': 0, 'schedule': 2, 'scheduled': 2, 'scheme': 0, 'school': 16, 'schooling': 1, 'schools': 9, 'science': 2, 'scratch': 1, 'screaming': 1, 'screen': 0, 'screwed': 2, 'se': 0, 'search': 9, 'season': 2, 'seat': 0, 'seating': 0, 'seats': 1, 'second': 3, 'seconds': 0, 'section': 3, 'sections': 0, 'sector': 1, 'security': 3, 'seeing': 8, 'seemingly': 1, 'seen': 10, 'select': 1, 'selection': 1, 'self': 1, 'sell': 2, 'selling': 1, 'sells': 0, 'senator': 3, 'send': 5, 'sending': 1, 'senior': 2, 'sens': 0, 'sense': 7, 'sent': 2, 'separate': 2, 'serait': 0, 'seriously': 4, 'seront': 0, 'serve': 1, 'servette': 0, 'service': 3, 'services': 3, 'sessions': 1, 'set': 5, 'setting': 1, 'seul': 0, 'severe': 1, 'shadows': 1, 'shake': 1, 'shame': 4, 'share': 2, 'shared': 0, 'sharing': 0, 'shelter': 0, 'shift': 0, 'shifts': 1, 'ship': 1, 'shipping': 0, 'shit': 8, 'shock': 1, 'shocked': 1, 'shoes': 1, 'shoot': 4, 'shop': 4, 'shopping': 0, 'shops': 1, 'shore': 3, 'short': 2, 'shot': 3, 'shouldn': 0, 'showed': 1, 'showing': 5, 'shows': 2, 'si': 0, 'sick': 7, 'sidebar': 6, 'sides': 2, 'sidewalk': 3, 'sign': 4, 'signed': 3, 'significant': 4, 'significantly': 2, 'signs': 2, 'similar': 12, 'simple': 2, 'simply': 3, 'single': 6, 'sir': 0, 'sit': 3, 'site': 2, 'sites': 0, 'sitting': 1, 'situation': 4, 'situations': 3, 'size': 1, 'sized': 1, 'sizes': 0, 'skate': 2, 'skates': 1, 'skating': 1, 'ski': 1, 'skiing': 1, 'skill': 1, 'skills': 2, 'skin': 0, 'sleep': 2, 'sleepwalk': 0, 'slide': 1, 'slightly': 2, 'slip': 0, 'slow': 2, 'small': 4, 'smaller': 4, 'smart': 0, 'smell': 0, 'smithsonian': 1, 'smoke': 2, 'smoking': 2, 'snap': 1, 'snow': 8, 'social': 5, 'society': 1, 'soft': 0, 'soit': 0, 'sold': 1, 'soleil': 0, 'solid': 2, 'solution': 3, 'solutions': 1, 'solve': 2, 'son': 0, 'sono': 0, 'sont': 0, 'soon': 1, 'sorry': 1, 'sort': 1, 'sought': 2, 'sound': 0, 'sounds': 5, 'soup': 0, 'source': 2, 'sources': 1, 'south': 12, 'southern': 2, 'southie': 2, 'souvent': 0, 'space': 9, 'spaces': 3, 'speak': 6, 'speaker': 0, 'speaking': 2, 'special': 4, 'specially': 0, 'species': 3, 'specific': 13, 'specifically': 3, 'spectacular': 2, 'speed': 0, 'spend': 4, 'spending': 2, 'spent': 3, 'spices': 0, 'spiders': 0, 'splitting': 1, 'spoke': 2, 'sports': 4, 'spot': 7, 'spots': 7, 'spray': 1, 'spring': 6, 'sq': 5, 'square': 9, 'sr': 0, 'srsltid': 0, 'st': 9, 'stadium': 1, 'staff': 3, 'stage': 2, 'stairs': 0, 'stand': 4, 'standard': 1, 'standards': 2, 'standing': 2, 'stands': 0, 'start': 11, 'started': 7, 'starting': 1, 'starts': 1, 'state': 28, 'statement': 3, 'states': 5, 'station': 7, 'stations': 2, 'statue': 3, 'statues': 3, 'status': 1, 'stay': 6, 'staying': 8, 'steak': 0, 'steel': 0, 'step': 4, 'stick': 0, 'stock': 2, 'stop': 19, 'stopped': 1, 'stops': 1, 'store': 6, 'stores': 4, 'stories': 2, 'storm': 1, 'story': 2, 'straight': 4, 'strawberries': 0, 'street': 14, 'streets': 2, 'stress': 0, 'stretch': 0, 'string': 1, 'stroller': 0, 'strong': 3, 'struggle': 1, 'stuck': 0, 'student': 2, 'students': 4, 'studio': 1, 'study': 1, 'stuff': 14, 'stupid': 3, 'style': 1, 'sub': 0, 'subaru': 0, 'submissions': 0, 'subreddit': 3, 'suburb': 3, 'suburbs': 1, 'subway': 4, 'succeed': 7, 'success': 1, 'successful': 0, 'sucks': 3, 'suddenly': 1, 'suggest': 3, 'suggested': 1, 'suggesting': 4, 'suggestion': 0, 'suggestions': 0, 'suis': 0, 'suisse': 0, 'sujet': 0, 'summer': 1, 'summernats': 0, 'sun': 5, 'sunday': 3, 'sunny': 0, 'super': 5, 'superior': 2, 'supermarket': 0, 'supply': 1, 'support': 14, 'supported': 1, 'supporting': 4, 'supportive': 1, 'suppose': 2, 'supposed': 1, 'supreme': 2, 'sur': 0, 'sure': 21, 'surely': 1, 'surface': 0, 'surgery': 0, 'surprise': 1, 'surprised': 6, 'surrounding': 2, 'surroundings': 0, 'surtout': 0, 'survive': 0, 'sushi': 0, 'swap': 1, 'sweet': 2, 'swimming': 1, 'swiss': 0, 'switched': 0, 'switzerland': 0, 'sydney': 0, 'symptoms': 3, 'systems': 3, 'ta': 0, 'table': 4, 'tables': 1, 'taken': 2, 'takes': 3, 'taking': 7, 'talk': 9, 'talking': 4, 'tall': 0, 'tamiflu': 4, 'tanks': 1, 'tant': 0, 'tap': 1, 'target': 1, 'targets': 1, 'tariffs': 1, 'taste': 4, 'tax': 1, 'taxes': 2, 'taxi': 0, 'tbh': 1, 'tcs': 0, 'td': 4, 'te': 0, 'tea': 2, 'teach': 2, 'teacher': 2, 'teachers': 1, 'teaching': 3, 'team': 2, 'tech': 2, 'technical': 1, 'technically': 0, 'teen': 2, 'tell': 12, 'telling': 2, 'tells': 1, 'temperature': 2, 'temperatures': 1, 'temporary': 2, 'temps': 0, 'tenant': 1, 'tenants': 1, 'tend': 2, 'term': 2, 'terminal': 0, 'terms': 2, 'terrible': 4, 'territory': 0, 'tes': 0, 'tesla': 4, 'test': 3, 'tests': 3, 'text': 1, 'textile': 2, 'textiles': 1, 'thank': 4, 'thanks': 3, 'therapists': 0, 'thing': 26, 'things': 16, 'think': 37, 'thinking': 5, 'thinks': 0, 'thought': 6, 'thousands': 2, 'thread': 3, 'threat': 2, 'threw': 2, 'thrift': 2, 'throw': 4, 'thursday': 1, 'ticket': 2, 'tickets': 2, 'tight': 1, 'tiktok': 1, 'till': 1, 'time': 47, 'times': 6, 'timing': 2, 'tiny': 1, 'tip': 2, 'tips': 1, 'tired': 1, 'tires': 1, 'today': 6, 'told': 7, 'tomatoes': 0, 'tomorrow': 1, 'ton': 3, 'took': 4, 'tool': 1, 'tools': 1, 'toronto': 1, 'total': 4, 'totally': 4, 'touch': 2, 'tough': 2, 'tour': 2, 'tourism': 6, 'tourists': 0, 'tours': 2, 'tout': 0, 'toute': 0, 'town': 14, 'townhouse': 0, 'towns': 5, 'tpg': 0, 'track': 1, 'tracks': 2, 'trade': 1, 'traditional': 0, 'traffic': 10, 'trail': 4, 'trails': 1, 'train': 3, 'trained': 0, 'training': 0, 'trains': 0, 'tram': 0, 'transfer': 0, 'transfers': 0, 'transgender': 1, 'transit': 4, 'translate': 0, 'transparency': 2, 'transpo': 0, 'transport': 4, 'transportation': 2, 'trash': 3, 'travaux': 0, 'travel': 2, 'traveling': 1, 'tre': 0, 'treat': 2, 'treated': 2, 'treatment': 2, 'treats': 1, 'tree': 3, 'trees': 0, 'trick': 0, 'tricky': 0, 'tried': 1, 'trip': 4, 'trips': 1, 'trouble': 1, 'trs': 0, 'truck': 1, 'true': 3, 'truly': 2, 'trump': 12, 'trust': 3, 'try': 10, 'trying': 10, 'tu': 0, 'tub': 2, 'tuesday': 0, 'tuggers': 0, 'tuition': 1, 'tunnel': 0, 'turn': 6, 'turned': 1, 'turning': 4, 'turns': 2, 'tv': 0, 'twice': 1, 'twin': 2, 'twitter': 2, 'type': 2, 'types': 2, 'typical': 0, 'uber': 2, 'uk': 0, 'underground': 0, 'understand': 7, 'understanding': 2, 'understood': 2, 'une': 0, 'unfortunately': 1, 'uni': 0, 'unige': 0, 'union': 5, 'unique': 1, 'unit': 2, 'units': 1, 'universities': 4, 'university': 3, 'unless': 3, 'unlike': 3, 'unsure': 1, 'ups': 0, 'upstairs': 0, 'ur': 0, 'urban': 4, 'use': 15, 'used': 11, 'useful': 0, 'useless': 0, 'users': 1, 'uses': 1, 'using': 14, 'usual': 4, 'usually': 7, 'va': 0, 'vacuum': 0, 'valid': 0, 'valley': 1, 'value': 2, 'van': 0, 'various': 2, 'vat': 0, 'vaud': 0, 've': 0, 'vegan': 0, 'veggie': 1, 'vehicle': 2, 'vehicles': 3, 'venue': 0, 'version': 0, 'vet': 0, 'vice': 1, 'victim': 2, 'video': 3, 'videos': 0, 'vie': 0, 'view': 0, 'views': 3, 'village': 0, 'ville': 0, 'vintage': 0, 'visible': 4, 'visit': 2, 'visited': 1, 'visiting': 0, 'visitor': 7, 'vives': 0, 'volume': 0, 'volunteer': 3, 'volunteering': 2, 'vote': 1, 'voted': 3, 'votre': 0, 'vous': 0, 'vraiment': 0, 'vs': 1, 'wage': 2, 'wages': 4, 'wait': 6, 'waiting': 2, 'waitlist': 0, 'walk': 14, 'walkable': 3, 'walked': 3, 'walking': 12, 'walks': 1, 'wall': 3, 'walls': 1, 'walter': 0, 'wander': 3, 'wanna': 2, 'want': 45, 'wanted': 7, 'wanting': 1, 'wants': 3, 'war': 0, 'warm': 3, 'wash': 1, 'washington': 2, 'wasn': 0, 'waste': 5, 'watch': 4, 'watching': 4, 'water': 5, 'waterfront': 0, 'watson': 0, 'waves': 0, 'way': 41, 'ways': 7, 'weak': 1, 'wealthy': 1, 'wear': 2, 'weather': 2, 'webp': 0, 'website': 6, 'wedding': 1, 'wednesday': 1, 'week': 15, 'weekday': 1, 'weekend': 5, 'weeks': 9, 'weight': 1, 'weird': 2, 'welcome': 4, 'went': 8, 'weren': 0, 'west': 7, 'western': 5, 'wetlands': 0, 'whatsapp': 0, 'white': 9, 'width': 0, 'wife': 6, 'wiki': 0, 'wikipedia': 0, 'wild': 2, 'willing': 4, 'win': 3, 'wind': 4, 'window': 2, 'windows': 0, 'windy': 2, 'wine': 1, 'wing': 3, 'winner': 1, 'winter': 13, 'wiring': 1, 'wise': 0, 'wish': 1, 'woden': 0, 'wolves': 0, 'woman': 5, 'women': 2, 'won': 3, 'wonder': 2, 'wonderful': 4, 'woods': 0, 'wool': 0, 'woolworths': 0, 'word': 2, 'words': 3, 'work': 22, 'worked': 3, 'worker': 2, 'workers': 6, 'working': 8, 'works': 2, 'world': 10, 'worried': 3, 'worry': 3, 'worse': 10, 'worst': 3, 'worth': 8, 'wouldn': 0, 'wow': 2, 'write': 4, 'writing': 1, 'written': 1, 'wrong': 5, 'wu': 7, 'www': 0, 'yard': 1, 'yarn': 0, 'yarns': 0, 'yeah': 6, 'year': 17, 'yearly': 0, 'years': 33, 'yelling': 2, 'yellow': 2, 'yes': 4, 'yesterday': 4, 'young': 2, 'younger': 1, 'youtu': 0, 'youtube': 1, 'zero': 4, 'zone': 2, 'zoo': 0, 'zurich': 0, 'city_count': 350, 'city_probability': 0.16666666666666666}\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes set up\n",
    "naiveBayes = NaiveBayes(X.to_numpy().flatten(), y.to_numpy(), feature_vectoriser=naiveBayes_uni)\n",
    "\n",
    "##### THIS IS JUST FOR TESTING #####\n",
    "features_probability_boston, features_probability_canberra, features_probability_geneva, features_probability_ottawa = naiveBayes.calc_probability(naiveBayes.x_all, naiveBayes.y_all)\n",
    "print(features_probability_boston)\n",
    "##### THIS IS JUST FOR TESTING #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 80 folds for each of 11 candidates, totalling 880 fits\n",
      "\n",
      "Best Parameters: {'alpha': np.float64(0.7100000000000001)}\n",
      "Cross-validation Accuracies: [0.8333333333333334, 0.6666666666666666, 0.6111111111111112, 0.6111111111111112, 0.7222222222222222, 0.6111111111111112, 0.6666666666666666, 0.6666666666666666, 0.8888888888888888, 0.6111111111111112, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.7222222222222222, 0.7777777777777778, 0.6111111111111112, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.6111111111111112, 0.7222222222222222, 0.6666666666666666, 0.6111111111111112, 0.8333333333333334, 0.8333333333333334, 0.6666666666666666, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.6666666666666666, 0.8888888888888888, 0.8333333333333334, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.5882352941176471, 0.7647058823529411, 0.5882352941176471, 0.5294117647058824, 0.8235294117647058, 0.7058823529411765, 0.7058823529411765, 0.8823529411764706, 0.6470588235294118, 0.7058823529411765, 0.7058823529411765, 0.8235294117647058, 0.7647058823529411, 0.47058823529411764, 0.6470588235294118, 0.47058823529411764, 0.8235294117647058, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.7647058823529411, 0.6470588235294118, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.7058823529411765, 0.7058823529411765, 0.7647058823529411, 0.7058823529411765, 0.7058823529411765, 0.5294117647058824, 0.7058823529411765, 0.7647058823529411, 0.8823529411764706, 0.7058823529411765, 0.5882352941176471, 0.7058823529411765]\n",
      "Mean Accuracy: 0.7217\n"
     ]
    }
   ],
   "source": [
    "# Compare with Naive Bayes model from sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "param_grid_NB_1 = {'alpha': np.arange(0.01, 1.11, 0.1)}\n",
    "\n",
    "NB = hyperparamaterTunning(X_uni, param_grid_NB_1 ,folds, MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Logistic Regression Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 80 folds for each of 6 candidates, totalling 480 fits\n",
      "\n",
      "Best Parameters: {'l1_ratio': np.float64(0.0), 'max_iter': 1000, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "Cross-validation Accuracies: [0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.7222222222222222, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.8333333333333334, 0.6111111111111112, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.6111111111111112, 0.8888888888888888, 0.8888888888888888, 0.6111111111111112, 0.5555555555555556, 0.8888888888888888, 0.7777777777777778, 0.7058823529411765, 0.7647058823529411, 0.7058823529411765, 0.5294117647058824, 0.7058823529411765, 0.5882352941176471, 0.6470588235294118, 0.9411764705882353, 0.7058823529411765, 0.7647058823529411, 0.8823529411764706, 0.6470588235294118, 0.6470588235294118, 0.7058823529411765, 0.7647058823529411, 0.7647058823529411, 0.7058823529411765, 0.6470588235294118, 0.5294117647058824, 0.8235294117647058, 0.6470588235294118, 0.7058823529411765, 0.8235294117647058, 0.7647058823529411, 0.5882352941176471, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.6470588235294118, 0.6470588235294118, 0.7647058823529411, 0.5882352941176471, 0.7058823529411765, 0.7058823529411765, 0.7647058823529411, 0.8235294117647058, 0.8235294117647058, 0.6470588235294118, 0.7647058823529411, 0.7647058823529411]\n",
      "Mean Accuracy: 0.7283\n",
      "Fitting 80 folds for each of 6 candidates, totalling 480 fits\n",
      "\n",
      "Best Parameters: {'l1_ratio': np.float64(0.0), 'max_iter': 1000, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "Cross-validation Accuracies: [0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.7777777777777778, 0.6666666666666666, 0.6111111111111112, 0.7222222222222222, 0.8333333333333334, 0.6111111111111112, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.6666666666666666, 0.6111111111111112, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.8888888888888888, 0.9444444444444444, 0.6111111111111112, 0.6111111111111112, 0.9444444444444444, 0.7222222222222222, 0.5882352941176471, 0.7647058823529411, 0.7058823529411765, 0.5294117647058824, 0.6470588235294118, 0.5882352941176471, 0.6470588235294118, 0.9411764705882353, 0.7058823529411765, 0.7647058823529411, 0.8823529411764706, 0.6470588235294118, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.7647058823529411, 0.6470588235294118, 0.7058823529411765, 0.4117647058823529, 0.7647058823529411, 0.6470588235294118, 0.8235294117647058, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.8235294117647058, 0.5882352941176471, 0.7058823529411765, 0.7647058823529411, 0.7647058823529411, 0.8235294117647058, 0.9411764705882353, 0.6470588235294118, 0.7647058823529411, 0.5882352941176471]\n",
      "Mean Accuracy: 0.7304\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_logModel_1 = [\n",
    "    {\"penalty\":[\"elasticnet\"],\n",
    "     \"l1_ratio\": np.arange(0, 1.2, 0.2), # 0 is only l2 penalty, 1 is only l1 penalty\n",
    "     \"solver\":[\"saga\"],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "logModel_tunned_1a = hyperparamaterTunning(X_uni, param_grid_logModel_1, folds, LogisticRegression(fit_intercept=True))\n",
    "logModel_tunned_1b = hyperparamaterTunning(X_uni_bi, param_grid_logModel_1, folds, LogisticRegression(fit_intercept=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 80 folds for each of 12 candidates, totalling 960 fits\n",
      "\n",
      "Best Parameters: {'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs', 'tol': 0.0001}\n",
      "Cross-validation Accuracies: [0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.7777777777777778, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.8333333333333334, 0.6111111111111112, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.6111111111111112, 0.8888888888888888, 0.8888888888888888, 0.6111111111111112, 0.5555555555555556, 0.8888888888888888, 0.7777777777777778, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.5294117647058824, 0.7058823529411765, 0.5882352941176471, 0.6470588235294118, 0.9411764705882353, 0.7058823529411765, 0.7647058823529411, 0.8823529411764706, 0.6470588235294118, 0.6470588235294118, 0.7058823529411765, 0.7647058823529411, 0.7647058823529411, 0.7058823529411765, 0.6470588235294118, 0.5294117647058824, 0.8235294117647058, 0.6470588235294118, 0.7058823529411765, 0.8823529411764706, 0.7647058823529411, 0.5882352941176471, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.6470588235294118, 0.6470588235294118, 0.7647058823529411, 0.5882352941176471, 0.7058823529411765, 0.7058823529411765, 0.7647058823529411, 0.8235294117647058, 0.8235294117647058, 0.6470588235294118, 0.7647058823529411, 0.7647058823529411]\n",
      "Mean Accuracy: 0.7290\n",
      "Fitting 80 folds for each of 12 candidates, totalling 960 fits\n",
      "\n",
      "Best Parameters: {'max_iter': 1000, 'penalty': 'l2', 'solver': 'sag', 'tol': 0.0001}\n",
      "Cross-validation Accuracies: [0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.6111111111111112, 0.7777777777777778, 0.6666666666666666, 0.6111111111111112, 0.7222222222222222, 0.8333333333333334, 0.6111111111111112, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.6666666666666666, 0.6111111111111112, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.8888888888888888, 0.9444444444444444, 0.6111111111111112, 0.6111111111111112, 0.9444444444444444, 0.7222222222222222, 0.5882352941176471, 0.7647058823529411, 0.7058823529411765, 0.5294117647058824, 0.6470588235294118, 0.5882352941176471, 0.6470588235294118, 0.9411764705882353, 0.7058823529411765, 0.7647058823529411, 0.8823529411764706, 0.6470588235294118, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.7647058823529411, 0.6470588235294118, 0.7058823529411765, 0.4117647058823529, 0.7647058823529411, 0.6470588235294118, 0.8235294117647058, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.8235294117647058, 0.5882352941176471, 0.7058823529411765, 0.7647058823529411, 0.7647058823529411, 0.8235294117647058, 0.9411764705882353, 0.6470588235294118, 0.7647058823529411, 0.5882352941176471]\n",
      "Mean Accuracy: 0.7304\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_logModel_2 = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"solver\":[\"sag\",\"lbfgs\",\"newton-cg\"],\n",
    "     \"tol\":[1e-4,1e-5],\n",
    "     \"max_iter\": [1000,2000]\n",
    "     }]\n",
    "logModel_tunned_2a = hyperparamaterTunning(X_uni, param_grid_logModel_2, folds, LogisticRegression(fit_intercept=True))\n",
    "\n",
    "logModel_tunned_2b = hyperparamaterTunning(X_uni_bi, param_grid_logModel_2, folds, LogisticRegression(fit_intercept=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SVM Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "param_grid_SVC_1 = [\n",
    "    {\"penalty\":[\"l1\",\"l2\"],\n",
    "     \"C\": np.arange(0.1,1.1,0.1).tolist(),\n",
    "     \"loss\": [\"squared_hinge\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "SVMModel_tunned_1a = hyperparamaterTunning(X_uni, param_grid_SVC_1, folds, LinearSVC(fit_intercept=True))\n",
    "\n",
    "SVMModel_tunned_1b = hyperparamaterTunning(X_uni_bi, param_grid_SVC_1, folds, LinearSVC(fit_intercept=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "param_grid_SVC_2 = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"C\": np.arange(0.1,1.1,0.1).tolist(),\n",
    "     \"loss\": [\"hinge\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "# X_uni --> only discarded term that are smaller than 2\n",
    "\n",
    "# MAX VALUE: 0.7443 -> Best Parameters: {'C': 0.6, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.0001} using X1_uni and 70 folds\n",
    "\n",
    "SVMModel_tunned_2a = hyperparamaterTunning(X_uni, param_grid_SVC_2, folds, LinearSVC(fit_intercept=True)) # Best one so far\n",
    "SVMModel_tunned_2b = hyperparamaterTunning(X_uni_bi, param_grid_SVC_2, folds, LinearSVC(fit_intercept=True)) # Best one so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# LOOK INTO THIS OR ELSE DELETE\n",
    "param_grid_SDG = [\n",
    "    {\"penalty\":[\"l1\",\"l2\"],\n",
    "     \"loss\": [\"hinge\", \"log_loss\", \"modified_huber\", \"squared_hinge\", \"perceptron\", \"squared_error\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "param_grid_SDG = [\n",
    "    {\"penalty\":[\"l2\"],\n",
    "     \"alpha\":[1e-3],\n",
    "     \"loss\": [\"hinge\"],\n",
    "     \"tol\":[1e-4],\n",
    "     \"max_iter\": [1000]\n",
    "     }]\n",
    "\n",
    "# SDGModel = hyperparamaterTunning(X_uni_bi, param_grid_SDG, folds, SGDClassifier(fit_intercept=True),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forest Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 80 folds for each of 6 candidates, totalling 480 fits\n",
      "\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 18, 'max_features': 'sqrt'}\n",
      "Cross-validation Accuracies: [0.7777777777777778, 0.5, 0.6666666666666666, 0.5555555555555556, 0.5, 0.6666666666666666, 0.5555555555555556, 0.6666666666666666, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.5, 0.5, 0.7222222222222222, 0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.6666666666666666, 0.8888888888888888, 0.8333333333333334, 0.5555555555555556, 0.8333333333333334, 0.6111111111111112, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.7222222222222222, 0.5882352941176471, 0.5294117647058824, 0.47058823529411764, 0.5882352941176471, 0.6470588235294118, 0.5882352941176471, 0.6470588235294118, 0.8235294117647058, 0.6470588235294118, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.5882352941176471, 0.5882352941176471, 0.5882352941176471, 0.47058823529411764, 0.4117647058823529, 0.5882352941176471, 0.4117647058823529, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.5882352941176471, 0.7647058823529411, 0.6470588235294118, 0.5882352941176471, 0.5882352941176471, 0.8235294117647058, 0.5882352941176471, 0.7058823529411765, 0.4117647058823529, 0.7058823529411765, 0.5882352941176471, 0.6470588235294118, 0.7647058823529411, 0.5294117647058824, 0.8235294117647058]\n",
      "Mean Accuracy: 0.6683\n",
      "Fitting 80 folds for each of 6 candidates, totalling 480 fits\n",
      "\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 18, 'max_features': 'sqrt'}\n",
      "Cross-validation Accuracies: [0.6666666666666666, 0.5555555555555556, 0.5555555555555556, 0.6666666666666666, 0.5, 0.6666666666666666, 0.5555555555555556, 0.7222222222222222, 0.7777777777777778, 0.6111111111111112, 0.6666666666666666, 0.5555555555555556, 0.5555555555555556, 0.6111111111111112, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.6111111111111112, 0.7222222222222222, 0.6111111111111112, 0.6666666666666666, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.6111111111111112, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.5555555555555556, 0.7777777777777778, 0.7777777777777778, 0.6470588235294118, 0.5882352941176471, 0.6470588235294118, 0.5882352941176471, 0.6470588235294118, 0.5882352941176471, 0.5882352941176471, 0.7647058823529411, 0.7058823529411765, 0.8235294117647058, 0.7058823529411765, 0.5294117647058824, 0.5294117647058824, 0.6470588235294118, 0.5294117647058824, 0.8235294117647058, 0.47058823529411764, 0.5882352941176471, 0.5882352941176471, 0.6470588235294118, 0.6470588235294118, 0.5882352941176471, 0.5882352941176471, 0.5882352941176471, 0.5882352941176471, 0.6470588235294118, 0.7647058823529411, 0.7058823529411765, 0.6470588235294118, 0.5882352941176471, 0.7647058823529411, 0.6470588235294118, 0.6470588235294118, 0.47058823529411764, 0.8235294117647058, 0.5294117647058824, 0.5294117647058824, 0.5882352941176471, 0.6470588235294118, 0.8823529411764706]\n",
      "Mean Accuracy: 0.6656\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid_rf = [{\n",
    "\"criterion\":[\"gini\", \"entropy\", \"log_loss\"],\n",
    "\"max_features\":[\"sqrt\", \"log2\"],\n",
    "\"max_depth\": [18] # Need to look into what values to use here\n",
    "}]\n",
    "rF = hyperparamaterTunning(X_uni, param_grid_rf, folds, RandomForestClassifier())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
